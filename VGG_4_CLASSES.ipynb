{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG - 4 CLASSES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPW/iqi45bAyLuB1ZwQTtO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunitavni/Speech-Emotion-Recognition/blob/master/VGG_4_CLASSES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3PT8T-iKqm3"
      },
      "source": [
        "#%% Imports\n",
        "import numpy as np;\n",
        "import torch;\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Model Configuration\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,device):\n",
        "        self.device = device;\n",
        "        # super Constructor\n",
        "        super(Net, self).__init__()\n",
        "        # Input size is (dim = 1, h = 161, w = 101)\n",
        "        # Conv layer output is [(Width - Kernel +2*Padding) / (Stride) ] + 1\n",
        "\n",
        "        conv1 = [nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(64),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.MaxPool2d(kernel_size=2,stride=2)];\n",
        "\n",
        "        self.conv1 = nn.Sequential(*conv1)\n",
        "\n",
        "        conv2 = [nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(128),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.MaxPool2d(kernel_size=2,stride=2)];\n",
        "\n",
        "        self.conv2 = nn.Sequential(*conv2)\n",
        "\n",
        "        conv3 = [nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(256),\n",
        "                 nn.ReLU(inplace=True)];\n",
        "\n",
        "        self.conv3 = nn.Sequential(*conv3)\n",
        "\n",
        "        conv4 = [nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(256),\n",
        "                 nn.ReLU(inplace=True)];\n",
        "\n",
        "        self.conv4 = nn.Sequential(*conv4)\n",
        "\n",
        "        conv5 = [nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(256),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.MaxPool2d(kernel_size=2,stride=2)];\n",
        "\n",
        "        self.conv5 = nn.Sequential(*conv5)\n",
        "\n",
        "        conv6 = [nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.ReLU(inplace=True)];\n",
        "\n",
        "        self.conv6 = nn.Sequential(*conv6)\n",
        "\n",
        "        conv7 = [nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.MaxPool2d(kernel_size=2,stride=2)];\n",
        "\n",
        "        self.conv7 = nn.Sequential(*conv7)\n",
        "\n",
        "        conv8 = [nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.ReLU(inplace=True)];\n",
        "\n",
        "        self.conv8 = nn.Sequential(*conv8)\n",
        "\n",
        "        conv9 = [nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.MaxPool2d(kernel_size=2,stride=2)];\n",
        "\n",
        "        self.conv9 = nn.Sequential(*conv9)\n",
        "\n",
        "\n",
        "        conv10 = [nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size=3,stride = 1,padding = 1),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.Dropout2d(),\n",
        "                  nn.AvgPool2d(kernel_size=1,stride=1)];\n",
        "\n",
        "        self.conv10 = nn.Sequential(*conv10)\n",
        "\n",
        "\n",
        "        fc = [nn.Linear(7680,512),\n",
        "              nn.ReLU(inplace=True),\n",
        "              nn.Linear(512,4),\n",
        "              nn.LogSoftmax()\n",
        "              ];\n",
        "\n",
        "        self.fc = nn.Sequential(*fc);\n",
        "\n",
        "\n",
        "    def setCriterion(self,criterion):\n",
        "        self.criterion = criterion;\n",
        "\n",
        "    def setOptimizer(self,optimizer):\n",
        "        self.optimizer = optimizer;\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x);\n",
        "        x = self.conv2(x);\n",
        "        x = self.conv3(x);\n",
        "        x = self.conv4(x);\n",
        "        x = self.conv5(x);\n",
        "        x = self.conv6(x);\n",
        "        x = self.conv7(x);\n",
        "        x = self.conv8(x);\n",
        "        x = self.conv9(x);\n",
        "        x = self.conv10(x);\n",
        "        x = x.view(x.size(0),-1)\n",
        "        return self.fc(x);\n",
        "\n",
        "    def fit(self,loader,epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, (data, labels) in enumerate(loader):\n",
        "                # Zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward propogation\n",
        "                inputs, targets = data.to(self.device),labels.to(self.device)\n",
        "\n",
        "                # Calculate network's output\n",
        "                output = self.forward(inputs)\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.criterion(output, targets.long())\n",
        "\n",
        "                # Calculate the derivatives\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the weights\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Print the batch loss\n",
        "                if (batch_idx % 20 == 0):\n",
        "                  print(f\"Epoch: {epoch} Batch: {batch_idx} Loss: {loss.item()}\")\n",
        "\n",
        "        print(\"Training completed\")\n",
        "\n",
        "    def predict(self,data):\n",
        "        predictions = [];\n",
        "        with torch.no_grad():\n",
        "            for d in data:\n",
        "                d = d.to(self.device);\n",
        "                val = self.forward(d);\n",
        "                unused,val = val.max(1)\n",
        "                predictions.append(val.item());\n",
        "        return np.array(predictions);\n",
        "\n",
        "\n",
        "    def calculateLoss(self,Loader):\n",
        "        Loss = [];\n",
        "        with torch.no_grad():\n",
        "            for index, data in enumerate(Loader):\n",
        "                # Get the images and the labels\n",
        "                images, labels = data[0].to(self.device),data[1].to(self.device);\n",
        "\n",
        "                # Forward propogation\n",
        "                outputs = self.forward(images);\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.criterion(outputs, labels);\n",
        "\n",
        "                # Add the batch loss to the list\n",
        "                Loss.append(loss.item());\n",
        "\n",
        "        # Return the average batch loss\n",
        "        return np.mean(Loss);\n",
        "\n",
        "    def calculateAccuracy(self,Loader):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in Loader:\n",
        "                images, labels = data[0].to(self.device),data[1].to(self.device)\n",
        "                outputs = self.forward(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = (100 * correct / total);\n",
        "        print('Accuracy of the network %d %%' %acc)\n",
        "        return acc;\n",
        "\n",
        "    def fitWithMetrics(self,trainLoader,testLoader,epochs):\n",
        "        trainLoss = [];\n",
        "        trainAccuracy = [];\n",
        "        testLoss = [];\n",
        "        testAccuracy = [];\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, (data, labels) in enumerate(trainLoader):\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.forward(data.to(self.device))\n",
        "                loss = self.criterion(output, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                print(f\"Epoch: {epoch} Batch: {batch_idx} Loss: {loss.item()}\")\n",
        "\n",
        "            # Calculate loss on train\n",
        "            trainLoss.append(self.calculateLoss(trainLoader));\n",
        "\n",
        "            # Calculate Accuracy on train\n",
        "            trainAccuracy.append(self.calculateAccuracy(trainLoader));\n",
        "\n",
        "            # Calculate loss on test\n",
        "            testLoss.append(self.calculateLoss(testLoader));\n",
        "\n",
        "            # Calculate Accuracy on test\n",
        "            testAccuracy.append(self.calculateAccuracy(testLoader));\n",
        "\n",
        "\n",
        "\n",
        "            print(f\"Epoch: {epoch} train loss: {trainLoss} validation loss: {testLoss}\")\n",
        "        print(\"Training completed\")\n",
        "        return trainLoss,trainAccuracy,testLoss,testAccuracy;\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hAAk4z1NT1W",
        "outputId": "3568d0cf-46cd-4629-96bc-7e3e26ec8688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "pip install conda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/4e/c533c3136427be62c38cc0e038cabf167bb54489c2ced2f6df903c456861/conda-4.3.16.tar.gz (299kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 3.5MB/s \n",
            "\u001b[?25hCollecting pycosat>=0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/fd/e38d68774c0a345b0090d608a90f1fbf423970d812f7ec7aef9ac024e648/pycosat-0.6.3.zip (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.12.4 in /usr/local/lib/python3.6/dist-packages (from conda) (2.23.0)\n",
            "Collecting ruamel.yaml>=0.11.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/92/59af3e38227b9cc14520bf1e59516d99ceca53e3b8448094248171e9432b/ruamel.yaml-0.16.10-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.12.4->conda) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.12.4->conda) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.12.4->conda) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.12.4->conda) (1.24.3)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/77/4bcd63f362bcb6c8f4f06253c11f9772f64189bf08cf3f40c5ccbda9e561/ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 11.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: conda, pycosat\n",
            "  Building wheel for conda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for conda: filename=conda-4.3.16-cp36-none-any.whl size=336938 sha256=0655fa8dcd9c11a273457881283b8ad9c6501537251a6cc749d0235de78579b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/50/79/302742d53e2231ec545cb3791abfdd24de234021ed8e0588a0\n",
            "  Building wheel for pycosat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycosat: filename=pycosat-0.6.3-cp36-cp36m-linux_x86_64.whl size=142847 sha256=ec4b55165b248cda3318aab4c50c5fd0e96633605889d19b8f4a1fe6964a6ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/67/ff/5570304e45814eccef48a3c69c3af25d0456ed3a34eddbbe38\n",
            "Successfully built conda pycosat\n",
            "Installing collected packages: pycosat, ruamel.yaml.clib, ruamel.yaml, conda\n",
            "Successfully installed conda-4.3.16 pycosat-0.6.3 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRRFZlduNZ_a",
        "outputId": "26d0f288-492b-4bee-bee2-1c6caa51f7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "pip install pysoundfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysoundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile) (2.20)\n",
            "Installing collected packages: pysoundfile\n",
            "Successfully installed pysoundfile-0.9.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjhIkzj6ODLd"
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "AUDIO_EXTENSIONS = [\n",
        "    '.wav', '.WAV',\n",
        "]\n",
        "\n",
        "\n",
        "def is_audio_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in AUDIO_EXTENSIONS)\n",
        "\n",
        "\n",
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "def make_dataset(dir, class_to_idx):\n",
        "    spects = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    for target in sorted(os.listdir(dir)):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "\n",
        "        for root, _, fnames in sorted(os.walk(d)):\n",
        "            for fname in sorted(fnames):\n",
        "                if is_audio_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    item = (path, class_to_idx[target])\n",
        "                    spects.append(item)\n",
        "    return spects\n",
        "\n",
        "\n",
        "def spect_loader(path, window_size, window_stride, window, normalize, max_len=101):\n",
        "    y, sr = sf.read(path)\n",
        "    # n_fft = 4096\n",
        "    n_fft = int(sr * window_size)\n",
        "    win_length = n_fft\n",
        "    hop_length = int(sr * window_stride)\n",
        "\n",
        "    # STFT\n",
        "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
        "                     win_length=win_length, window=window)\n",
        "    spect, phase = librosa.magphase(D)\n",
        "\n",
        "    # S = log(S+1)\n",
        "    spect = np.log1p(spect)\n",
        "\n",
        "    # make all spects with the same dims\n",
        "    # TODO: change that in the future\n",
        "    if spect.shape[1] < max_len:\n",
        "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n",
        "        spect = np.hstack((spect, pad))\n",
        "    elif spect.shape[1] > max_len:\n",
        "        spect = spect[:, :max_len]\n",
        "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n",
        "    spect = torch.FloatTensor(spect)\n",
        "\n",
        "    # z-score normalization\n",
        "    if normalize:\n",
        "        mean = spect.mean()\n",
        "        std = spect.std()\n",
        "        if std != 0:\n",
        "            spect.add_(-mean)\n",
        "            spect.div_(std)\n",
        "\n",
        "    return spect\n",
        "\n",
        "\n",
        "class GCommandLoader(data.Dataset):\n",
        "    \"\"\"A google command data set loader where the wavs are arranged in this way: ::\n",
        "        root/one/xxx.wav\n",
        "        root/one/xxy.wav\n",
        "        root/one/xxz.wav\n",
        "        root/head/123.wav\n",
        "        root/head/nsdf3.wav\n",
        "        root/head/asd932_.wav\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        window_size: window size for the stft, default value is .02\n",
        "        window_stride: window stride for the stft, default value is .01\n",
        "        window_type: typye of window to extract the stft, default value is 'hamming'\n",
        "        normalize: boolean, whether or not to normalize the spect to have zero mean and one std\n",
        "        max_len: the maximum length of frames to use\n",
        "     Attributes:\n",
        "        classes (list): List of the class names.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        spects (list): List of (spects path, class_index) tuples\n",
        "        STFT parameter: window_size, window_stride, window_type, normalize\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None, window_size=.02,\n",
        "                 window_stride=.01, window_type='hamming', normalize=True, max_len=101):\n",
        "        classes, class_to_idx = find_classes(root)\n",
        "        spects = make_dataset(root, class_to_idx)\n",
        "        if len(spects) == 0:\n",
        "            raise (RuntimeError(\"Found 0 sound files in subfolders of: \" + root + \"Supported audio file extensions are: \" + \",\".join(AUDIO_EXTENSIONS)))\n",
        "\n",
        "        self.root = root\n",
        "        self.spects = spects\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.loader = spect_loader\n",
        "        self.window_size = window_size\n",
        "        self.window_stride = window_stride\n",
        "        self.window_type = window_type\n",
        "        self.normalize = normalize\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (spect, target) where target is class_index of the target class.\n",
        "        \"\"\"\n",
        "        path, target = self.spects[index]\n",
        "        spect = self.loader(path, self.window_size, self.window_stride, self.window_type, self.normalize, self.max_len)\n",
        "        if self.transform is not None:\n",
        "            spect = self.transform(spect)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return spect, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spects)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWB5tf-rLPHa"
      },
      "source": [
        "import librosa    #Python Library for analysing audio\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F;\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import os.path\n",
        "import pdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO3HW1ejLTnS"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------- #\n",
        "                                        # Data Section\n",
        "                        # Prepare all the data for training and prediction\n",
        "# --------------------------------------------------------------------------------------------- #\n",
        "AUDIO_EXTENSIONS = [\n",
        "    '.wav', '.WAV',\n",
        "]\n",
        "\n",
        "def is_audio_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in AUDIO_EXTENSIONS)\n",
        "\n",
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "def make_dataset(dir, class_to_idx):\n",
        "    spects = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    for target in sorted(os.listdir(dir)):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "\n",
        "        for root, _, fnames in sorted(os.walk(d)):\n",
        "            for fname in sorted(fnames):\n",
        "                if is_audio_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    item = (path, class_to_idx[target])\n",
        "                    spects.append(item)\n",
        "    return spects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3-aXvXKMK41",
        "outputId": "7b3abd9c-7ca9-44ea-c1a3-c80d83639a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8vQTSmLYsq"
      },
      "source": [
        "# Get the directories which represent the classes\n",
        "dirPath = \"/content/drive/My Drive/Data4\"\n",
        "\n",
        "classes,class_to_idx = find_classes(dirPath)\n",
        "spects = make_dataset(dirPath,class_to_idx);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5P5xVSLboj",
        "outputId": "fcab9d24-bb4f-40de-9720-db218bac7927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "print(\"Loading the data..\")\n",
        "# Use the sound itself\n",
        "X = [];\n",
        "y = [];\n",
        "\n",
        "i = 0;\n",
        "myLen = len(spects);\n",
        "for spect in spects:\n",
        "    file, samplerate = librosa.load(spect[0], sr=4000)\n",
        "    X.append(file);\n",
        "    y.append(spect[1]);\n",
        "    i = i + 1;\n",
        "    if(i % 500 == 0):\n",
        "        print(f\"Loading in process: {i/myLen}\")\n",
        "\n",
        "# for i in range(10):\n",
        "#     file, samplerate = librosa.load(spects[i][0], sr=4000)\n",
        "#     X.append(file);\n",
        "#     y.append(spects[i][1]);\n",
        "\n",
        "print(\"Finished loading!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the data..\n",
            "Loading in process: 0.0669882100750268\n",
            "Loading in process: 0.1339764201500536\n",
            "Loading in process: 0.20096463022508038\n",
            "Loading in process: 0.2679528403001072\n",
            "Loading in process: 0.334941050375134\n",
            "Loading in process: 0.40192926045016075\n",
            "Loading in process: 0.46891747052518756\n",
            "Loading in process: 0.5359056806002144\n",
            "Loading in process: 0.6028938906752411\n",
            "Loading in process: 0.669882100750268\n",
            "Loading in process: 0.7368703108252947\n",
            "Loading in process: 0.8038585209003215\n",
            "Loading in process: 0.8708467309753484\n",
            "Loading in process: 0.9378349410503751\n",
            "Finished loading!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCjePBCezPye"
      },
      "source": [
        "X_new = []\n",
        "\n",
        "max_len = 16261;\n",
        "for i in range(len(X)):\n",
        "    x_len = len(X[i]);\n",
        "    if(x_len <= max_len):\n",
        "        X_new.append(np.concatenate((X[i],np.zeros(16261 - len(X[i])))).reshape(1,161,101))\n",
        "    else:\n",
        "        X_new.append( X[i][0:max_len].reshape(1,161,101) );"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKwypUALgeH"
      },
      "source": [
        "# Shuffle\n",
        "from sklearn.utils import shuffle\n",
        "X_new,y = shuffle(X_new,y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vivxbbGyLm1A"
      },
      "source": [
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_new, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZrYPbUWLnYW"
      },
      "source": [
        "X_train = torch.FloatTensor(np.array(X_train));\n",
        "y_train = torch.from_numpy(np.array(y_train));\n",
        "X_test = torch.FloatTensor(np.array(X_test));\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=5, shuffle=True,\n",
        "        num_workers=0, pin_memory=True, sampler=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHIkES1pLpSj",
        "outputId": "bb0fc08f-d0f5-40ca-adcc-8bbb9ba20774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "#%%\n",
        "# Initialize the network\n",
        "clf = Net(device)\n",
        "\n",
        "\n",
        "#%%\n",
        "# Print the network\n",
        "print(clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Net(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv6): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv7): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv8): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv9): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv10): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout2d(p=0.5, inplace=False)\n",
            "    (4): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=7680, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "    (3): LogSoftmax()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2oXDd01LuDi",
        "outputId": "58702a58-f9e9-4ca1-d000-4050acecdd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set the loss criterion\n",
        "criterion = F.nll_loss;\n",
        "clf.setCriterion(criterion);\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = optim.Adam(params=clf.parameters(), lr=0.0001);\n",
        "clf.setOptimizer(optimizer);\n",
        "\n",
        "# Set the network to training mode\n",
        "clf.train();\n",
        "\n",
        "# Set the network to CUDA\n",
        "clf.to(device);\n",
        "\n",
        "# Train the network\n",
        "clf.fit(train_loader,80);\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Batch: 0 Loss: 0.04143838956952095\n",
            "Epoch: 0 Batch: 20 Loss: 0.0022295950911939144\n",
            "Epoch: 0 Batch: 40 Loss: 0.010278129950165749\n",
            "Epoch: 0 Batch: 60 Loss: 0.8752123117446899\n",
            "Epoch: 0 Batch: 80 Loss: 0.15833711624145508\n",
            "Epoch: 0 Batch: 100 Loss: 0.09964542090892792\n",
            "Epoch: 0 Batch: 120 Loss: 0.6296706199645996\n",
            "Epoch: 0 Batch: 140 Loss: 0.0006031036609783769\n",
            "Epoch: 0 Batch: 160 Loss: 0.2985903322696686\n",
            "Epoch: 0 Batch: 180 Loss: 0.013796234503388405\n",
            "Epoch: 0 Batch: 200 Loss: 0.1819661557674408\n",
            "Epoch: 0 Batch: 220 Loss: 0.3442099690437317\n",
            "Epoch: 0 Batch: 240 Loss: 0.0659143477678299\n",
            "Epoch: 0 Batch: 260 Loss: 0.002065372420474887\n",
            "Epoch: 0 Batch: 280 Loss: 0.02356576919555664\n",
            "Epoch: 0 Batch: 300 Loss: 0.21269121766090393\n",
            "Epoch: 0 Batch: 320 Loss: 0.026526832953095436\n",
            "Epoch: 0 Batch: 340 Loss: 0.04737968370318413\n",
            "Epoch: 0 Batch: 360 Loss: 0.013088035397231579\n",
            "Epoch: 0 Batch: 380 Loss: 0.0686769038438797\n",
            "Epoch: 0 Batch: 400 Loss: 0.040851593017578125\n",
            "Epoch: 0 Batch: 420 Loss: 0.35827770829200745\n",
            "Epoch: 0 Batch: 440 Loss: 0.1695113182067871\n",
            "Epoch: 0 Batch: 460 Loss: 0.004663562867790461\n",
            "Epoch: 0 Batch: 480 Loss: 0.00021476745314430445\n",
            "Epoch: 0 Batch: 500 Loss: 0.04809980466961861\n",
            "Epoch: 0 Batch: 520 Loss: 0.21420244872570038\n",
            "Epoch: 0 Batch: 540 Loss: 0.019403647631406784\n",
            "Epoch: 0 Batch: 560 Loss: 0.08016786724328995\n",
            "Epoch: 0 Batch: 580 Loss: 0.007847309112548828\n",
            "Epoch: 0 Batch: 600 Loss: 0.0012733459006994963\n",
            "Epoch: 0 Batch: 620 Loss: 0.023116016760468483\n",
            "Epoch: 0 Batch: 640 Loss: 0.014642143622040749\n",
            "Epoch: 0 Batch: 660 Loss: 0.2996043264865875\n",
            "Epoch: 0 Batch: 680 Loss: 0.000622653984464705\n",
            "Epoch: 0 Batch: 700 Loss: 0.00957021676003933\n",
            "Epoch: 0 Batch: 720 Loss: 0.12250914424657822\n",
            "Epoch: 0 Batch: 740 Loss: 0.03361396864056587\n",
            "Epoch: 0 Batch: 760 Loss: 0.024196719750761986\n",
            "Epoch: 0 Batch: 780 Loss: 0.3658583462238312\n",
            "Epoch: 0 Batch: 800 Loss: 0.0010156631469726562\n",
            "Epoch: 0 Batch: 820 Loss: 0.18804731965065002\n",
            "Epoch: 0 Batch: 840 Loss: 0.38021039962768555\n",
            "Epoch: 0 Batch: 860 Loss: 0.05052337795495987\n",
            "Epoch: 0 Batch: 880 Loss: 0.011801719665527344\n",
            "Epoch: 0 Batch: 900 Loss: 0.35440748929977417\n",
            "Epoch: 0 Batch: 920 Loss: 0.044956207275390625\n",
            "Epoch: 0 Batch: 940 Loss: 0.20598307251930237\n",
            "Epoch: 0 Batch: 960 Loss: 0.04234800487756729\n",
            "Epoch: 0 Batch: 980 Loss: 0.1223750114440918\n",
            "Epoch: 1 Batch: 0 Loss: 0.015052080154418945\n",
            "Epoch: 1 Batch: 20 Loss: 0.38348546624183655\n",
            "Epoch: 1 Batch: 40 Loss: 0.16093139350414276\n",
            "Epoch: 1 Batch: 60 Loss: 0.013013171963393688\n",
            "Epoch: 1 Batch: 80 Loss: 0.008308696560561657\n",
            "Epoch: 1 Batch: 100 Loss: 0.4258669912815094\n",
            "Epoch: 1 Batch: 120 Loss: 0.20819321274757385\n",
            "Epoch: 1 Batch: 140 Loss: 0.00149450299795717\n",
            "Epoch: 1 Batch: 160 Loss: 0.013067913241684437\n",
            "Epoch: 1 Batch: 180 Loss: 0.0012896538246423006\n",
            "Epoch: 1 Batch: 200 Loss: 0.0028683661948889494\n",
            "Epoch: 1 Batch: 220 Loss: 0.019022464752197266\n",
            "Epoch: 1 Batch: 240 Loss: 0.01058273296803236\n",
            "Epoch: 1 Batch: 260 Loss: 0.0018414497608318925\n",
            "Epoch: 1 Batch: 280 Loss: 0.03400053828954697\n",
            "Epoch: 1 Batch: 300 Loss: 0.13427190482616425\n",
            "Epoch: 1 Batch: 320 Loss: 0.1310877799987793\n",
            "Epoch: 1 Batch: 340 Loss: 0.14033660292625427\n",
            "Epoch: 1 Batch: 360 Loss: 0.7014126777648926\n",
            "Epoch: 1 Batch: 380 Loss: 0.003753089811652899\n",
            "Epoch: 1 Batch: 400 Loss: 0.045822761952877045\n",
            "Epoch: 1 Batch: 420 Loss: 0.21391335129737854\n",
            "Epoch: 1 Batch: 440 Loss: 0.004549884703010321\n",
            "Epoch: 1 Batch: 460 Loss: 0.4474739134311676\n",
            "Epoch: 1 Batch: 480 Loss: 0.1949397623538971\n",
            "Epoch: 1 Batch: 500 Loss: 0.030347442254424095\n",
            "Epoch: 1 Batch: 520 Loss: 0.001072788261808455\n",
            "Epoch: 1 Batch: 540 Loss: 0.008129596710205078\n",
            "Epoch: 1 Batch: 560 Loss: 0.03592996671795845\n",
            "Epoch: 1 Batch: 580 Loss: 0.07621259987354279\n",
            "Epoch: 1 Batch: 600 Loss: 0.024275779724121094\n",
            "Epoch: 1 Batch: 620 Loss: 0.017646407708525658\n",
            "Epoch: 1 Batch: 640 Loss: 0.0015480041038244963\n",
            "Epoch: 1 Batch: 660 Loss: 0.15081605315208435\n",
            "Epoch: 1 Batch: 680 Loss: 0.2368588000535965\n",
            "Epoch: 1 Batch: 700 Loss: 0.07786273956298828\n",
            "Epoch: 1 Batch: 720 Loss: 0.007949638180434704\n",
            "Epoch: 1 Batch: 740 Loss: 0.006342887878417969\n",
            "Epoch: 1 Batch: 760 Loss: 0.08829405158758163\n",
            "Epoch: 1 Batch: 780 Loss: 0.25379571318626404\n",
            "Epoch: 1 Batch: 800 Loss: 0.08822803199291229\n",
            "Epoch: 1 Batch: 820 Loss: 0.26443806290626526\n",
            "Epoch: 1 Batch: 840 Loss: 0.15182438492774963\n",
            "Epoch: 1 Batch: 860 Loss: 0.0061168670654296875\n",
            "Epoch: 1 Batch: 880 Loss: 0.3397909998893738\n",
            "Epoch: 1 Batch: 900 Loss: 0.04195690155029297\n",
            "Epoch: 1 Batch: 920 Loss: 0.0752447098493576\n",
            "Epoch: 1 Batch: 940 Loss: 0.0962430015206337\n",
            "Epoch: 1 Batch: 960 Loss: 0.0908133015036583\n",
            "Epoch: 1 Batch: 980 Loss: 0.35887113213539124\n",
            "Epoch: 2 Batch: 0 Loss: 0.011542702093720436\n",
            "Epoch: 2 Batch: 20 Loss: 0.04185886308550835\n",
            "Epoch: 2 Batch: 40 Loss: 0.07961807399988174\n",
            "Epoch: 2 Batch: 60 Loss: 0.0026458739303052425\n",
            "Epoch: 2 Batch: 80 Loss: 0.010485649108886719\n",
            "Epoch: 2 Batch: 100 Loss: 1.5702228546142578\n",
            "Epoch: 2 Batch: 120 Loss: 0.48396891355514526\n",
            "Epoch: 2 Batch: 140 Loss: 0.01261892355978489\n",
            "Epoch: 2 Batch: 160 Loss: 0.5629804730415344\n",
            "Epoch: 2 Batch: 180 Loss: 0.17087984085083008\n",
            "Epoch: 2 Batch: 200 Loss: 0.006393623538315296\n",
            "Epoch: 2 Batch: 220 Loss: 0.05421428754925728\n",
            "Epoch: 2 Batch: 240 Loss: 0.04418821260333061\n",
            "Epoch: 2 Batch: 260 Loss: 0.013404893688857555\n",
            "Epoch: 2 Batch: 280 Loss: 0.07065634429454803\n",
            "Epoch: 2 Batch: 300 Loss: 0.009899521246552467\n",
            "Epoch: 2 Batch: 320 Loss: 0.008467579260468483\n",
            "Epoch: 2 Batch: 340 Loss: 0.09898991882801056\n",
            "Epoch: 2 Batch: 360 Loss: 0.16713885962963104\n",
            "Epoch: 2 Batch: 380 Loss: 0.05106472969055176\n",
            "Epoch: 2 Batch: 400 Loss: 0.026159953325986862\n",
            "Epoch: 2 Batch: 420 Loss: 0.03736291080713272\n",
            "Epoch: 2 Batch: 440 Loss: 0.028476333245635033\n",
            "Epoch: 2 Batch: 460 Loss: 0.3178284168243408\n",
            "Epoch: 2 Batch: 480 Loss: 0.002034473465755582\n",
            "Epoch: 2 Batch: 500 Loss: 0.12384605407714844\n",
            "Epoch: 2 Batch: 520 Loss: 0.044669248163700104\n",
            "Epoch: 2 Batch: 540 Loss: 0.3784102499485016\n",
            "Epoch: 2 Batch: 560 Loss: 0.19941286742687225\n",
            "Epoch: 2 Batch: 580 Loss: 0.16196981072425842\n",
            "Epoch: 2 Batch: 600 Loss: 0.03429098054766655\n",
            "Epoch: 2 Batch: 620 Loss: 0.4951915740966797\n",
            "Epoch: 2 Batch: 640 Loss: 0.02109518088400364\n",
            "Epoch: 2 Batch: 660 Loss: 0.04565482214093208\n",
            "Epoch: 2 Batch: 680 Loss: 0.0028502463828772306\n",
            "Epoch: 2 Batch: 700 Loss: 0.036385633051395416\n",
            "Epoch: 2 Batch: 720 Loss: 0.046282194554805756\n",
            "Epoch: 2 Batch: 740 Loss: 0.025895118713378906\n",
            "Epoch: 2 Batch: 760 Loss: 0.017629623413085938\n",
            "Epoch: 2 Batch: 780 Loss: 0.44861525297164917\n",
            "Epoch: 2 Batch: 800 Loss: 0.035207271575927734\n",
            "Epoch: 2 Batch: 820 Loss: 0.010267352685332298\n",
            "Epoch: 2 Batch: 840 Loss: 0.005774307064712048\n",
            "Epoch: 2 Batch: 860 Loss: 0.00042972565279342234\n",
            "Epoch: 2 Batch: 880 Loss: 0.0002405166596872732\n",
            "Epoch: 2 Batch: 900 Loss: 0.020261097699403763\n",
            "Epoch: 2 Batch: 920 Loss: 0.0022745132446289062\n",
            "Epoch: 2 Batch: 940 Loss: 0.015863895416259766\n",
            "Epoch: 2 Batch: 960 Loss: 0.007115173153579235\n",
            "Epoch: 2 Batch: 980 Loss: 0.0004616737423930317\n",
            "Epoch: 3 Batch: 0 Loss: 0.007126999087631702\n",
            "Epoch: 3 Batch: 20 Loss: 1.283202052116394\n",
            "Epoch: 3 Batch: 40 Loss: 0.3007742762565613\n",
            "Epoch: 3 Batch: 60 Loss: 0.2133834809064865\n",
            "Epoch: 3 Batch: 80 Loss: 0.18035082519054413\n",
            "Epoch: 3 Batch: 100 Loss: 0.4097718596458435\n",
            "Epoch: 3 Batch: 120 Loss: 0.2967631220817566\n",
            "Epoch: 3 Batch: 140 Loss: 0.016953181475400925\n",
            "Epoch: 3 Batch: 160 Loss: 0.04337787628173828\n",
            "Epoch: 3 Batch: 180 Loss: 0.005077838897705078\n",
            "Epoch: 3 Batch: 200 Loss: 0.20995482802391052\n",
            "Epoch: 3 Batch: 220 Loss: 0.04507646709680557\n",
            "Epoch: 3 Batch: 240 Loss: 0.03481721878051758\n",
            "Epoch: 3 Batch: 260 Loss: 0.6022859811782837\n",
            "Epoch: 3 Batch: 280 Loss: 0.09179077297449112\n",
            "Epoch: 3 Batch: 300 Loss: 0.12046118080615997\n",
            "Epoch: 3 Batch: 320 Loss: 0.6988121867179871\n",
            "Epoch: 3 Batch: 340 Loss: 0.015835952013731003\n",
            "Epoch: 3 Batch: 360 Loss: 0.5785045623779297\n",
            "Epoch: 3 Batch: 380 Loss: 0.02307257615029812\n",
            "Epoch: 3 Batch: 400 Loss: 0.005515384487807751\n",
            "Epoch: 3 Batch: 420 Loss: 0.003511142684146762\n",
            "Epoch: 3 Batch: 440 Loss: 0.24059972167015076\n",
            "Epoch: 3 Batch: 460 Loss: 0.019864369183778763\n",
            "Epoch: 3 Batch: 480 Loss: 0.02890310250222683\n",
            "Epoch: 3 Batch: 500 Loss: 0.2594276964664459\n",
            "Epoch: 3 Batch: 520 Loss: 0.33071058988571167\n",
            "Epoch: 3 Batch: 540 Loss: 0.19976797699928284\n",
            "Epoch: 3 Batch: 560 Loss: 0.7431730031967163\n",
            "Epoch: 3 Batch: 580 Loss: 0.11766533553600311\n",
            "Epoch: 3 Batch: 600 Loss: 0.0026666640769690275\n",
            "Epoch: 3 Batch: 620 Loss: 0.0173371322453022\n",
            "Epoch: 3 Batch: 640 Loss: 0.41265982389450073\n",
            "Epoch: 3 Batch: 660 Loss: 0.06461472809314728\n",
            "Epoch: 3 Batch: 680 Loss: 0.010431766510009766\n",
            "Epoch: 3 Batch: 700 Loss: 0.4204515516757965\n",
            "Epoch: 3 Batch: 720 Loss: 0.4071942865848541\n",
            "Epoch: 3 Batch: 740 Loss: 0.02565307542681694\n",
            "Epoch: 3 Batch: 760 Loss: 0.0028967857360839844\n",
            "Epoch: 3 Batch: 780 Loss: 0.07026024162769318\n",
            "Epoch: 3 Batch: 800 Loss: 0.45731478929519653\n",
            "Epoch: 3 Batch: 820 Loss: 0.012766075320541859\n",
            "Epoch: 3 Batch: 840 Loss: 0.1211579293012619\n",
            "Epoch: 3 Batch: 860 Loss: 0.1380077302455902\n",
            "Epoch: 3 Batch: 880 Loss: 0.004636192228645086\n",
            "Epoch: 3 Batch: 900 Loss: 0.01562786102294922\n",
            "Epoch: 3 Batch: 920 Loss: 0.0960237979888916\n",
            "Epoch: 3 Batch: 940 Loss: 0.18232610821723938\n",
            "Epoch: 3 Batch: 960 Loss: 0.003833293914794922\n",
            "Epoch: 3 Batch: 980 Loss: 0.001112270401790738\n",
            "Epoch: 4 Batch: 0 Loss: 0.0002951622009277344\n",
            "Epoch: 4 Batch: 20 Loss: 0.0031949044205248356\n",
            "Epoch: 4 Batch: 40 Loss: 0.36109933257102966\n",
            "Epoch: 4 Batch: 60 Loss: 0.050069332122802734\n",
            "Epoch: 4 Batch: 80 Loss: 0.03917407989501953\n",
            "Epoch: 4 Batch: 100 Loss: 0.10334863513708115\n",
            "Epoch: 4 Batch: 120 Loss: 0.14237017929553986\n",
            "Epoch: 4 Batch: 140 Loss: 0.04773864895105362\n",
            "Epoch: 4 Batch: 160 Loss: 0.05815448611974716\n",
            "Epoch: 4 Batch: 180 Loss: 0.00909128226339817\n",
            "Epoch: 4 Batch: 200 Loss: 0.14882059395313263\n",
            "Epoch: 4 Batch: 220 Loss: 0.5267859697341919\n",
            "Epoch: 4 Batch: 240 Loss: 0.0008626937633380294\n",
            "Epoch: 4 Batch: 260 Loss: 0.0058119772002100945\n",
            "Epoch: 4 Batch: 280 Loss: 0.019832085818052292\n",
            "Epoch: 4 Batch: 300 Loss: 0.0901368111371994\n",
            "Epoch: 4 Batch: 320 Loss: 0.017232799902558327\n",
            "Epoch: 4 Batch: 340 Loss: 0.08809304237365723\n",
            "Epoch: 4 Batch: 360 Loss: 0.1640985757112503\n",
            "Epoch: 4 Batch: 380 Loss: 0.014852046966552734\n",
            "Epoch: 4 Batch: 400 Loss: 0.002694797469303012\n",
            "Epoch: 4 Batch: 420 Loss: 0.004722881130874157\n",
            "Epoch: 4 Batch: 440 Loss: 0.48773813247680664\n",
            "Epoch: 4 Batch: 460 Loss: 0.002147197723388672\n",
            "Epoch: 4 Batch: 480 Loss: 0.629501461982727\n",
            "Epoch: 4 Batch: 500 Loss: 0.02385115623474121\n",
            "Epoch: 4 Batch: 520 Loss: 0.09398112446069717\n",
            "Epoch: 4 Batch: 540 Loss: 0.06995868682861328\n",
            "Epoch: 4 Batch: 560 Loss: 0.26796990633010864\n",
            "Epoch: 4 Batch: 580 Loss: 0.05378847196698189\n",
            "Epoch: 4 Batch: 600 Loss: 0.009890747256577015\n",
            "Epoch: 4 Batch: 620 Loss: 0.11316470801830292\n",
            "Epoch: 4 Batch: 640 Loss: 0.27822667360305786\n",
            "Epoch: 4 Batch: 660 Loss: 0.001741695450618863\n",
            "Epoch: 4 Batch: 680 Loss: 0.03525667265057564\n",
            "Epoch: 4 Batch: 700 Loss: 0.0356903076171875\n",
            "Epoch: 4 Batch: 720 Loss: 0.00923452340066433\n",
            "Epoch: 4 Batch: 740 Loss: 0.047138310968875885\n",
            "Epoch: 4 Batch: 760 Loss: 0.003704071044921875\n",
            "Epoch: 4 Batch: 780 Loss: 0.08681969344615936\n",
            "Epoch: 4 Batch: 800 Loss: 0.14073476195335388\n",
            "Epoch: 4 Batch: 820 Loss: 0.2293073683977127\n",
            "Epoch: 4 Batch: 840 Loss: 0.0023725510109215975\n",
            "Epoch: 4 Batch: 860 Loss: 0.13734130561351776\n",
            "Epoch: 4 Batch: 880 Loss: 0.16873407363891602\n",
            "Epoch: 4 Batch: 900 Loss: 0.007266902830451727\n",
            "Epoch: 4 Batch: 920 Loss: 0.06364736706018448\n",
            "Epoch: 4 Batch: 940 Loss: 0.028169870376586914\n",
            "Epoch: 4 Batch: 960 Loss: 0.07688836753368378\n",
            "Epoch: 4 Batch: 980 Loss: 0.018025875091552734\n",
            "Epoch: 5 Batch: 0 Loss: 0.10093574225902557\n",
            "Epoch: 5 Batch: 20 Loss: 0.0018812179332599044\n",
            "Epoch: 5 Batch: 40 Loss: 0.0009696960332803428\n",
            "Epoch: 5 Batch: 60 Loss: 0.00866851769387722\n",
            "Epoch: 5 Batch: 80 Loss: 0.1706503927707672\n",
            "Epoch: 5 Batch: 100 Loss: 0.9069780111312866\n",
            "Epoch: 5 Batch: 120 Loss: 0.016582727432250977\n",
            "Epoch: 5 Batch: 140 Loss: 0.012312126345932484\n",
            "Epoch: 5 Batch: 160 Loss: 0.006917476654052734\n",
            "Epoch: 5 Batch: 180 Loss: 0.00010242462303722277\n",
            "Epoch: 5 Batch: 200 Loss: 0.03130302578210831\n",
            "Epoch: 5 Batch: 220 Loss: 0.07030310481786728\n",
            "Epoch: 5 Batch: 240 Loss: 0.004296016879379749\n",
            "Epoch: 5 Batch: 260 Loss: 0.01719813421368599\n",
            "Epoch: 5 Batch: 280 Loss: 0.0028243064880371094\n",
            "Epoch: 5 Batch: 300 Loss: 0.21615581214427948\n",
            "Epoch: 5 Batch: 320 Loss: 0.02559490129351616\n",
            "Epoch: 5 Batch: 340 Loss: 0.06083808094263077\n",
            "Epoch: 5 Batch: 360 Loss: 0.10603924095630646\n",
            "Epoch: 5 Batch: 380 Loss: 0.12078003585338593\n",
            "Epoch: 5 Batch: 400 Loss: 0.0004493713495321572\n",
            "Epoch: 5 Batch: 420 Loss: 0.3420735001564026\n",
            "Epoch: 5 Batch: 440 Loss: 0.003375434782356024\n",
            "Epoch: 5 Batch: 460 Loss: 0.005058002658188343\n",
            "Epoch: 5 Batch: 480 Loss: 0.0050598145462572575\n",
            "Epoch: 5 Batch: 500 Loss: 0.1095825657248497\n",
            "Epoch: 5 Batch: 520 Loss: 0.11864366382360458\n",
            "Epoch: 5 Batch: 540 Loss: 0.04706287384033203\n",
            "Epoch: 5 Batch: 560 Loss: 0.08236093819141388\n",
            "Epoch: 5 Batch: 580 Loss: 0.06646642833948135\n",
            "Epoch: 5 Batch: 600 Loss: 0.0043090819381177425\n",
            "Epoch: 5 Batch: 620 Loss: 0.041131019592285156\n",
            "Epoch: 5 Batch: 640 Loss: 0.001203346299007535\n",
            "Epoch: 5 Batch: 660 Loss: 0.007869338616728783\n",
            "Epoch: 5 Batch: 680 Loss: 0.006975746247917414\n",
            "Epoch: 5 Batch: 700 Loss: 0.910079836845398\n",
            "Epoch: 5 Batch: 720 Loss: 0.2602745592594147\n",
            "Epoch: 5 Batch: 740 Loss: 0.03829803317785263\n",
            "Epoch: 5 Batch: 760 Loss: 0.011379432864487171\n",
            "Epoch: 5 Batch: 780 Loss: 0.015818119049072266\n",
            "Epoch: 5 Batch: 800 Loss: 0.22549085319042206\n",
            "Epoch: 5 Batch: 820 Loss: 0.0027522086165845394\n",
            "Epoch: 5 Batch: 840 Loss: 0.2239680290222168\n",
            "Epoch: 5 Batch: 860 Loss: 0.10890092700719833\n",
            "Epoch: 5 Batch: 880 Loss: 0.0021093368995934725\n",
            "Epoch: 5 Batch: 900 Loss: 0.10727095603942871\n",
            "Epoch: 5 Batch: 920 Loss: 0.05266733095049858\n",
            "Epoch: 5 Batch: 940 Loss: 0.0002589225769042969\n",
            "Epoch: 5 Batch: 960 Loss: 0.02438335493206978\n",
            "Epoch: 5 Batch: 980 Loss: 0.10259179770946503\n",
            "Epoch: 6 Batch: 0 Loss: 0.03866863250732422\n",
            "Epoch: 6 Batch: 20 Loss: 0.01911783218383789\n",
            "Epoch: 6 Batch: 40 Loss: 0.007871150970458984\n",
            "Epoch: 6 Batch: 60 Loss: 0.0004638671816792339\n",
            "Epoch: 6 Batch: 80 Loss: 0.018436813727021217\n",
            "Epoch: 6 Batch: 100 Loss: 0.03745918348431587\n",
            "Epoch: 6 Batch: 120 Loss: 0.011354446411132812\n",
            "Epoch: 6 Batch: 140 Loss: 0.0020406723488122225\n",
            "Epoch: 6 Batch: 160 Loss: 0.35774746537208557\n",
            "Epoch: 6 Batch: 180 Loss: 0.008343887515366077\n",
            "Epoch: 6 Batch: 200 Loss: 0.04942502826452255\n",
            "Epoch: 6 Batch: 220 Loss: 0.019366741180419922\n",
            "Epoch: 6 Batch: 240 Loss: 0.08390426635742188\n",
            "Epoch: 6 Batch: 260 Loss: 0.005284404847770929\n",
            "Epoch: 6 Batch: 280 Loss: 0.00782232265919447\n",
            "Epoch: 6 Batch: 300 Loss: 0.3561208248138428\n",
            "Epoch: 6 Batch: 320 Loss: 0.0519196018576622\n",
            "Epoch: 6 Batch: 340 Loss: 0.20381999015808105\n",
            "Epoch: 6 Batch: 360 Loss: 0.009554481133818626\n",
            "Epoch: 6 Batch: 380 Loss: 0.019030189141631126\n",
            "Epoch: 6 Batch: 400 Loss: 0.006734657101333141\n",
            "Epoch: 6 Batch: 420 Loss: 0.008975028991699219\n",
            "Epoch: 6 Batch: 440 Loss: 0.06853795051574707\n",
            "Epoch: 6 Batch: 460 Loss: 0.7765477895736694\n",
            "Epoch: 6 Batch: 480 Loss: 0.08262958377599716\n",
            "Epoch: 6 Batch: 500 Loss: 0.07954631000757217\n",
            "Epoch: 6 Batch: 520 Loss: 0.029953574761748314\n",
            "Epoch: 6 Batch: 540 Loss: 0.034844301640987396\n",
            "Epoch: 6 Batch: 560 Loss: 0.0024972916580736637\n",
            "Epoch: 6 Batch: 580 Loss: 0.0034262656699866056\n",
            "Epoch: 6 Batch: 600 Loss: 0.24558019638061523\n",
            "Epoch: 6 Batch: 620 Loss: 0.0003621101495809853\n",
            "Epoch: 6 Batch: 640 Loss: 0.07953371852636337\n",
            "Epoch: 6 Batch: 660 Loss: 0.05249672010540962\n",
            "Epoch: 6 Batch: 680 Loss: 0.026898670941591263\n",
            "Epoch: 6 Batch: 700 Loss: 0.02173643186688423\n",
            "Epoch: 6 Batch: 720 Loss: 0.03224945068359375\n",
            "Epoch: 6 Batch: 740 Loss: 0.032772064208984375\n",
            "Epoch: 6 Batch: 760 Loss: 0.3099845051765442\n",
            "Epoch: 6 Batch: 780 Loss: 0.4489101469516754\n",
            "Epoch: 6 Batch: 800 Loss: 0.10419473797082901\n",
            "Epoch: 6 Batch: 820 Loss: 0.13640384376049042\n",
            "Epoch: 6 Batch: 840 Loss: 0.0032931328751146793\n",
            "Epoch: 6 Batch: 860 Loss: 0.28949156403541565\n",
            "Epoch: 6 Batch: 880 Loss: 0.05114622041583061\n",
            "Epoch: 6 Batch: 900 Loss: 0.0020973205100744963\n",
            "Epoch: 6 Batch: 920 Loss: 0.04079017788171768\n",
            "Epoch: 6 Batch: 940 Loss: 0.030344676226377487\n",
            "Epoch: 6 Batch: 960 Loss: 0.29942387342453003\n",
            "Epoch: 6 Batch: 980 Loss: 0.15561382472515106\n",
            "Epoch: 7 Batch: 0 Loss: 0.0020265579223632812\n",
            "Epoch: 7 Batch: 20 Loss: 0.0018057823181152344\n",
            "Epoch: 7 Batch: 40 Loss: 0.049640558660030365\n",
            "Epoch: 7 Batch: 60 Loss: 0.004836463835090399\n",
            "Epoch: 7 Batch: 80 Loss: 0.03361453860998154\n",
            "Epoch: 7 Batch: 100 Loss: 0.0010877609020099044\n",
            "Epoch: 7 Batch: 120 Loss: 0.027634620666503906\n",
            "Epoch: 7 Batch: 140 Loss: 0.8692710995674133\n",
            "Epoch: 7 Batch: 160 Loss: 0.5445078015327454\n",
            "Epoch: 7 Batch: 180 Loss: 0.017515182495117188\n",
            "Epoch: 7 Batch: 200 Loss: 0.003415393875911832\n",
            "Epoch: 7 Batch: 220 Loss: 0.006903648376464844\n",
            "Epoch: 7 Batch: 240 Loss: 0.001004123711027205\n",
            "Epoch: 7 Batch: 260 Loss: 0.07773275673389435\n",
            "Epoch: 7 Batch: 280 Loss: 0.00113763811532408\n",
            "Epoch: 7 Batch: 300 Loss: 0.003929710481315851\n",
            "Epoch: 7 Batch: 320 Loss: 0.013847922906279564\n",
            "Epoch: 7 Batch: 340 Loss: 0.0016224861610680819\n",
            "Epoch: 7 Batch: 360 Loss: 0.016835976392030716\n",
            "Epoch: 7 Batch: 380 Loss: 0.3972321152687073\n",
            "Epoch: 7 Batch: 400 Loss: 0.0682135596871376\n",
            "Epoch: 7 Batch: 420 Loss: 0.04296550899744034\n",
            "Epoch: 7 Batch: 440 Loss: 0.0009630203130654991\n",
            "Epoch: 7 Batch: 460 Loss: 0.549307644367218\n",
            "Epoch: 7 Batch: 480 Loss: 0.08256702125072479\n",
            "Epoch: 7 Batch: 500 Loss: 0.034241579473018646\n",
            "Epoch: 7 Batch: 520 Loss: 0.6409395337104797\n",
            "Epoch: 7 Batch: 540 Loss: 0.004137611482292414\n",
            "Epoch: 7 Batch: 560 Loss: 0.6869093179702759\n",
            "Epoch: 7 Batch: 580 Loss: 0.33366385102272034\n",
            "Epoch: 7 Batch: 600 Loss: 0.049344539642333984\n",
            "Epoch: 7 Batch: 620 Loss: 0.11492779105901718\n",
            "Epoch: 7 Batch: 640 Loss: 0.12299065291881561\n",
            "Epoch: 7 Batch: 660 Loss: 0.08957891166210175\n",
            "Epoch: 7 Batch: 680 Loss: 0.005314636044204235\n",
            "Epoch: 7 Batch: 700 Loss: 0.003456497099250555\n",
            "Epoch: 7 Batch: 720 Loss: 0.04100313037633896\n",
            "Epoch: 7 Batch: 740 Loss: 0.0017750740516930819\n",
            "Epoch: 7 Batch: 760 Loss: 0.2376668006181717\n",
            "Epoch: 7 Batch: 780 Loss: 0.02614288404583931\n",
            "Epoch: 7 Batch: 800 Loss: 0.005290412809699774\n",
            "Epoch: 7 Batch: 820 Loss: 0.0216235164552927\n",
            "Epoch: 7 Batch: 840 Loss: 0.00993270892649889\n",
            "Epoch: 7 Batch: 860 Loss: 0.12729111313819885\n",
            "Epoch: 7 Batch: 880 Loss: 0.004637432284653187\n",
            "Epoch: 7 Batch: 900 Loss: 0.006283760070800781\n",
            "Epoch: 7 Batch: 920 Loss: 0.025513553991913795\n",
            "Epoch: 7 Batch: 940 Loss: 0.41673946380615234\n",
            "Epoch: 7 Batch: 960 Loss: 0.16823336482048035\n",
            "Epoch: 7 Batch: 980 Loss: 0.012073993682861328\n",
            "Epoch: 8 Batch: 0 Loss: 0.06556300818920135\n",
            "Epoch: 8 Batch: 20 Loss: 0.012961959466338158\n",
            "Epoch: 8 Batch: 40 Loss: 0.010592078790068626\n",
            "Epoch: 8 Batch: 60 Loss: 0.0009792328346520662\n",
            "Epoch: 8 Batch: 80 Loss: 0.013561057858169079\n",
            "Epoch: 8 Batch: 100 Loss: 0.0066239358857274055\n",
            "Epoch: 8 Batch: 120 Loss: 0.035251714289188385\n",
            "Epoch: 8 Batch: 140 Loss: 0.1392243653535843\n",
            "Epoch: 8 Batch: 160 Loss: 0.00055780413094908\n",
            "Epoch: 8 Batch: 180 Loss: 0.02454090118408203\n",
            "Epoch: 8 Batch: 200 Loss: 0.03490405157208443\n",
            "Epoch: 8 Batch: 220 Loss: 0.005040359683334827\n",
            "Epoch: 8 Batch: 240 Loss: 0.01082544308155775\n",
            "Epoch: 8 Batch: 260 Loss: 0.5051432847976685\n",
            "Epoch: 8 Batch: 280 Loss: 0.0011433601612225175\n",
            "Epoch: 8 Batch: 300 Loss: 0.017946291714906693\n",
            "Epoch: 8 Batch: 320 Loss: 0.062014199793338776\n",
            "Epoch: 8 Batch: 340 Loss: 0.021058272570371628\n",
            "Epoch: 8 Batch: 360 Loss: 0.12684956192970276\n",
            "Epoch: 8 Batch: 380 Loss: 0.0008707046508789062\n",
            "Epoch: 8 Batch: 400 Loss: 0.005041980650275946\n",
            "Epoch: 8 Batch: 420 Loss: 0.4485761523246765\n",
            "Epoch: 8 Batch: 440 Loss: 0.005319499876350164\n",
            "Epoch: 8 Batch: 460 Loss: 0.03643770143389702\n",
            "Epoch: 8 Batch: 480 Loss: 0.005413723178207874\n",
            "Epoch: 8 Batch: 500 Loss: 0.6785147190093994\n",
            "Epoch: 8 Batch: 520 Loss: 0.1560080498456955\n",
            "Epoch: 8 Batch: 540 Loss: 0.07198347896337509\n",
            "Epoch: 8 Batch: 560 Loss: 0.05540213733911514\n",
            "Epoch: 8 Batch: 580 Loss: 0.24060606956481934\n",
            "Epoch: 8 Batch: 600 Loss: 0.01688241958618164\n",
            "Epoch: 8 Batch: 620 Loss: 0.05311641842126846\n",
            "Epoch: 8 Batch: 640 Loss: 0.007053470704704523\n",
            "Epoch: 8 Batch: 660 Loss: 0.013766909018158913\n",
            "Epoch: 8 Batch: 680 Loss: 0.0008489608881063759\n",
            "Epoch: 8 Batch: 700 Loss: 0.009853172115981579\n",
            "Epoch: 8 Batch: 720 Loss: 0.03247880935668945\n",
            "Epoch: 8 Batch: 740 Loss: 0.12643161416053772\n",
            "Epoch: 8 Batch: 760 Loss: 0.0024253844749182463\n",
            "Epoch: 8 Batch: 780 Loss: 0.02660655975341797\n",
            "Epoch: 8 Batch: 800 Loss: 0.0020520209800451994\n",
            "Epoch: 8 Batch: 820 Loss: 0.0017156600952148438\n",
            "Epoch: 8 Batch: 840 Loss: 0.014364242553710938\n",
            "Epoch: 8 Batch: 860 Loss: 0.005228138063102961\n",
            "Epoch: 8 Batch: 880 Loss: 0.0045563699677586555\n",
            "Epoch: 8 Batch: 900 Loss: 0.09179182350635529\n",
            "Epoch: 8 Batch: 920 Loss: 0.0195770263671875\n",
            "Epoch: 8 Batch: 940 Loss: 0.08371920883655548\n",
            "Epoch: 8 Batch: 960 Loss: 0.08592300117015839\n",
            "Epoch: 8 Batch: 980 Loss: 0.023259639739990234\n",
            "Epoch: 9 Batch: 0 Loss: 0.08803100883960724\n",
            "Epoch: 9 Batch: 20 Loss: 0.0006559371831826866\n",
            "Epoch: 9 Batch: 40 Loss: 0.00039844511775299907\n",
            "Epoch: 9 Batch: 60 Loss: 0.00029687880305573344\n",
            "Epoch: 9 Batch: 80 Loss: 0.03056344948709011\n",
            "Epoch: 9 Batch: 100 Loss: 0.005683613009750843\n",
            "Epoch: 9 Batch: 120 Loss: 0.06615543365478516\n",
            "Epoch: 9 Batch: 140 Loss: 0.02875337563455105\n",
            "Epoch: 9 Batch: 160 Loss: 0.12901297211647034\n",
            "Epoch: 9 Batch: 180 Loss: 0.19230632483959198\n",
            "Epoch: 9 Batch: 200 Loss: 0.0498136505484581\n",
            "Epoch: 9 Batch: 220 Loss: 0.0021788596641272306\n",
            "Epoch: 9 Batch: 240 Loss: 0.01033792458474636\n",
            "Epoch: 9 Batch: 260 Loss: 0.15998630225658417\n",
            "Epoch: 9 Batch: 280 Loss: 0.34535637497901917\n",
            "Epoch: 9 Batch: 300 Loss: 0.004455947782844305\n",
            "Epoch: 9 Batch: 320 Loss: 0.0004383087216410786\n",
            "Epoch: 9 Batch: 340 Loss: 0.005050182342529297\n",
            "Epoch: 9 Batch: 360 Loss: 0.06496407836675644\n",
            "Epoch: 9 Batch: 380 Loss: 0.11203889548778534\n",
            "Epoch: 9 Batch: 400 Loss: 0.0078582763671875\n",
            "Epoch: 9 Batch: 420 Loss: 0.01618328131735325\n",
            "Epoch: 9 Batch: 440 Loss: 0.9550865888595581\n",
            "Epoch: 9 Batch: 460 Loss: 0.4995788037776947\n",
            "Epoch: 9 Batch: 480 Loss: 0.11094536632299423\n",
            "Epoch: 9 Batch: 500 Loss: 0.03032512590289116\n",
            "Epoch: 9 Batch: 520 Loss: 0.30060940980911255\n",
            "Epoch: 9 Batch: 540 Loss: 0.01154479943215847\n",
            "Epoch: 9 Batch: 560 Loss: 0.5561380386352539\n",
            "Epoch: 9 Batch: 580 Loss: 0.019646357744932175\n",
            "Epoch: 9 Batch: 600 Loss: 0.13448219001293182\n",
            "Epoch: 9 Batch: 620 Loss: 0.23865079879760742\n",
            "Epoch: 9 Batch: 640 Loss: 0.0004814147832803428\n",
            "Epoch: 9 Batch: 660 Loss: 0.08368901908397675\n",
            "Epoch: 9 Batch: 680 Loss: 0.012712669558823109\n",
            "Epoch: 9 Batch: 700 Loss: 0.010669231414794922\n",
            "Epoch: 9 Batch: 720 Loss: 0.005040264222770929\n",
            "Epoch: 9 Batch: 740 Loss: 0.008450984954833984\n",
            "Epoch: 9 Batch: 760 Loss: 0.7813252210617065\n",
            "Epoch: 9 Batch: 780 Loss: 0.003877353621646762\n",
            "Epoch: 9 Batch: 800 Loss: 0.13167047500610352\n",
            "Epoch: 9 Batch: 820 Loss: 0.11767911911010742\n",
            "Epoch: 9 Batch: 840 Loss: 0.023816680535674095\n",
            "Epoch: 9 Batch: 860 Loss: 0.026372050866484642\n",
            "Epoch: 9 Batch: 880 Loss: 0.35314589738845825\n",
            "Epoch: 9 Batch: 900 Loss: 0.006544685456901789\n",
            "Epoch: 9 Batch: 920 Loss: 0.0033823014236986637\n",
            "Epoch: 9 Batch: 940 Loss: 0.45622411370277405\n",
            "Epoch: 9 Batch: 960 Loss: 0.0014205932384356856\n",
            "Epoch: 9 Batch: 980 Loss: 0.02443370781838894\n",
            "Epoch: 10 Batch: 0 Loss: 0.0048539163544774055\n",
            "Epoch: 10 Batch: 20 Loss: 0.006152438931167126\n",
            "Epoch: 10 Batch: 40 Loss: 0.0426173210144043\n",
            "Epoch: 10 Batch: 60 Loss: 0.00021495818509720266\n",
            "Epoch: 10 Batch: 80 Loss: 0.2315184623003006\n",
            "Epoch: 10 Batch: 100 Loss: 0.03349809721112251\n",
            "Epoch: 10 Batch: 120 Loss: 0.057602882385253906\n",
            "Epoch: 10 Batch: 140 Loss: 0.0007118225330486894\n",
            "Epoch: 10 Batch: 160 Loss: 0.004702377133071423\n",
            "Epoch: 10 Batch: 180 Loss: 0.000592136406339705\n",
            "Epoch: 10 Batch: 200 Loss: 0.044881224632263184\n",
            "Epoch: 10 Batch: 220 Loss: 0.22299686074256897\n",
            "Epoch: 10 Batch: 240 Loss: 0.003593921661376953\n",
            "Epoch: 10 Batch: 260 Loss: 0.03920402377843857\n",
            "Epoch: 10 Batch: 280 Loss: 0.0023128509055823088\n",
            "Epoch: 10 Batch: 300 Loss: 0.036958884447813034\n",
            "Epoch: 10 Batch: 320 Loss: 0.0021435737144201994\n",
            "Epoch: 10 Batch: 340 Loss: 0.04349327087402344\n",
            "Epoch: 10 Batch: 360 Loss: 0.061061859130859375\n",
            "Epoch: 10 Batch: 380 Loss: 0.002340126084163785\n",
            "Epoch: 10 Batch: 400 Loss: 0.0018161773914471269\n",
            "Epoch: 10 Batch: 420 Loss: 0.004833793733268976\n",
            "Epoch: 10 Batch: 440 Loss: 0.03194008022546768\n",
            "Epoch: 10 Batch: 460 Loss: 0.011010264977812767\n",
            "Epoch: 10 Batch: 480 Loss: 0.19827871024608612\n",
            "Epoch: 10 Batch: 500 Loss: 0.0002330779971089214\n",
            "Epoch: 10 Batch: 520 Loss: 0.02095765992999077\n",
            "Epoch: 10 Batch: 540 Loss: 0.08044900745153427\n",
            "Epoch: 10 Batch: 560 Loss: 0.009457111358642578\n",
            "Epoch: 10 Batch: 580 Loss: 0.031001757830381393\n",
            "Epoch: 10 Batch: 600 Loss: 0.015758132562041283\n",
            "Epoch: 10 Batch: 620 Loss: 0.016808319836854935\n",
            "Epoch: 10 Batch: 640 Loss: 0.006066703703254461\n",
            "Epoch: 10 Batch: 660 Loss: 0.0027268410194665194\n",
            "Epoch: 10 Batch: 680 Loss: 0.002283763838931918\n",
            "Epoch: 10 Batch: 700 Loss: 0.21946534514427185\n",
            "Epoch: 10 Batch: 720 Loss: 0.03641185909509659\n",
            "Epoch: 10 Batch: 740 Loss: 0.21341891586780548\n",
            "Epoch: 10 Batch: 760 Loss: 0.03795113414525986\n",
            "Epoch: 10 Batch: 780 Loss: 0.014178085140883923\n",
            "Epoch: 10 Batch: 800 Loss: 0.017523670569062233\n",
            "Epoch: 10 Batch: 820 Loss: 0.04052753373980522\n",
            "Epoch: 10 Batch: 840 Loss: 0.13661566376686096\n",
            "Epoch: 10 Batch: 860 Loss: 0.0007630347972735763\n",
            "Epoch: 10 Batch: 880 Loss: 0.004808998201042414\n",
            "Epoch: 10 Batch: 900 Loss: 0.08088264614343643\n",
            "Epoch: 10 Batch: 920 Loss: 0.24716052412986755\n",
            "Epoch: 10 Batch: 940 Loss: 0.013049602508544922\n",
            "Epoch: 10 Batch: 960 Loss: 0.017456436529755592\n",
            "Epoch: 10 Batch: 980 Loss: 0.005943012423813343\n",
            "Epoch: 11 Batch: 0 Loss: 0.0012880325084552169\n",
            "Epoch: 11 Batch: 20 Loss: 0.02706127241253853\n",
            "Epoch: 11 Batch: 40 Loss: 0.027799224480986595\n",
            "Epoch: 11 Batch: 60 Loss: 0.010319137945771217\n",
            "Epoch: 11 Batch: 80 Loss: 0.010438919067382812\n",
            "Epoch: 11 Batch: 100 Loss: 0.007029819302260876\n",
            "Epoch: 11 Batch: 120 Loss: 0.06862592697143555\n",
            "Epoch: 11 Batch: 140 Loss: 0.04027276113629341\n",
            "Epoch: 11 Batch: 160 Loss: 0.03470215946435928\n",
            "Epoch: 11 Batch: 180 Loss: 0.009049415588378906\n",
            "Epoch: 11 Batch: 200 Loss: 0.017265701666474342\n",
            "Epoch: 11 Batch: 220 Loss: 0.013428211212158203\n",
            "Epoch: 11 Batch: 240 Loss: 0.005084895994514227\n",
            "Epoch: 11 Batch: 260 Loss: 0.004836940672248602\n",
            "Epoch: 11 Batch: 280 Loss: 0.014542674645781517\n",
            "Epoch: 11 Batch: 300 Loss: 0.021908093243837357\n",
            "Epoch: 11 Batch: 320 Loss: 0.005782890133559704\n",
            "Epoch: 11 Batch: 340 Loss: 0.8094472885131836\n",
            "Epoch: 11 Batch: 360 Loss: 0.007405757904052734\n",
            "Epoch: 11 Batch: 380 Loss: 0.0415802001953125\n",
            "Epoch: 11 Batch: 400 Loss: 0.00072565081063658\n",
            "Epoch: 11 Batch: 420 Loss: 0.021906280890107155\n",
            "Epoch: 11 Batch: 440 Loss: 0.011134910397231579\n",
            "Epoch: 11 Batch: 460 Loss: 0.0034521103370934725\n",
            "Epoch: 11 Batch: 480 Loss: 0.0011383056407794356\n",
            "Epoch: 11 Batch: 500 Loss: 0.41308489441871643\n",
            "Epoch: 11 Batch: 520 Loss: 0.22071358561515808\n",
            "Epoch: 11 Batch: 540 Loss: 0.034665919840335846\n",
            "Epoch: 11 Batch: 560 Loss: 0.2749699652194977\n",
            "Epoch: 11 Batch: 580 Loss: 0.03504514694213867\n",
            "Epoch: 11 Batch: 600 Loss: 0.018290232867002487\n",
            "Epoch: 11 Batch: 620 Loss: 0.0025493621360510588\n",
            "Epoch: 11 Batch: 640 Loss: 0.04795894771814346\n",
            "Epoch: 11 Batch: 660 Loss: 0.055295657366514206\n",
            "Epoch: 11 Batch: 680 Loss: 0.00135631556622684\n",
            "Epoch: 11 Batch: 700 Loss: 0.028653239831328392\n",
            "Epoch: 11 Batch: 720 Loss: 0.0014469146262854338\n",
            "Epoch: 11 Batch: 740 Loss: 0.21856579184532166\n",
            "Epoch: 11 Batch: 760 Loss: 0.00043544769869185984\n",
            "Epoch: 11 Batch: 780 Loss: 0.0006935119745321572\n",
            "Epoch: 11 Batch: 800 Loss: 0.00456924457103014\n",
            "Epoch: 11 Batch: 820 Loss: 0.050965022295713425\n",
            "Epoch: 11 Batch: 840 Loss: 0.011541938409209251\n",
            "Epoch: 11 Batch: 860 Loss: 0.003117179963737726\n",
            "Epoch: 11 Batch: 880 Loss: 0.42310723662376404\n",
            "Epoch: 11 Batch: 900 Loss: 0.009533405303955078\n",
            "Epoch: 11 Batch: 920 Loss: 0.0033309936989098787\n",
            "Epoch: 11 Batch: 940 Loss: 0.03174591064453125\n",
            "Epoch: 11 Batch: 960 Loss: 0.00031871796818450093\n",
            "Epoch: 11 Batch: 980 Loss: 0.0067733763717114925\n",
            "Epoch: 12 Batch: 0 Loss: 0.0007727622869424522\n",
            "Epoch: 12 Batch: 20 Loss: 0.0004561424138955772\n",
            "Epoch: 12 Batch: 40 Loss: 0.010235404595732689\n",
            "Epoch: 12 Batch: 60 Loss: 0.07457809150218964\n",
            "Epoch: 12 Batch: 80 Loss: 0.0004077911435160786\n",
            "Epoch: 12 Batch: 100 Loss: 0.005701827816665173\n",
            "Epoch: 12 Batch: 120 Loss: 0.028612708672881126\n",
            "Epoch: 12 Batch: 140 Loss: 0.0031587600242346525\n",
            "Epoch: 12 Batch: 160 Loss: 0.006970024202018976\n",
            "Epoch: 12 Batch: 180 Loss: 0.3652006983757019\n",
            "Epoch: 12 Batch: 200 Loss: 0.0034982680808752775\n",
            "Epoch: 12 Batch: 220 Loss: 0.008378791622817516\n",
            "Epoch: 12 Batch: 240 Loss: 0.0004161834658589214\n",
            "Epoch: 12 Batch: 260 Loss: 0.014144515618681908\n",
            "Epoch: 12 Batch: 280 Loss: 0.012199783697724342\n",
            "Epoch: 12 Batch: 300 Loss: 0.06748799979686737\n",
            "Epoch: 12 Batch: 320 Loss: 0.0010131836170330644\n",
            "Epoch: 12 Batch: 340 Loss: 1.983642505365424e-05\n",
            "Epoch: 12 Batch: 360 Loss: 0.45074567198753357\n",
            "Epoch: 12 Batch: 380 Loss: 0.003702354384586215\n",
            "Epoch: 12 Batch: 400 Loss: 0.03542022779583931\n",
            "Epoch: 12 Batch: 420 Loss: 7.629394644936838e-07\n",
            "Epoch: 12 Batch: 440 Loss: 0.01279439963400364\n",
            "Epoch: 12 Batch: 460 Loss: 0.05417671054601669\n",
            "Epoch: 12 Batch: 480 Loss: 0.0008306503295898438\n",
            "Epoch: 12 Batch: 500 Loss: 0.012686347588896751\n",
            "Epoch: 12 Batch: 520 Loss: 0.015075111761689186\n",
            "Epoch: 12 Batch: 540 Loss: 4.558563159662299e-05\n",
            "Epoch: 12 Batch: 560 Loss: 0.015168905258178711\n",
            "Epoch: 12 Batch: 580 Loss: 0.0019060134654864669\n",
            "Epoch: 12 Batch: 600 Loss: 1.0071009397506714\n",
            "Epoch: 12 Batch: 620 Loss: 0.17149333655834198\n",
            "Epoch: 12 Batch: 640 Loss: 0.13071146607398987\n",
            "Epoch: 12 Batch: 660 Loss: 0.00275249476544559\n",
            "Epoch: 12 Batch: 680 Loss: 0.23190298676490784\n",
            "Epoch: 12 Batch: 700 Loss: 0.023528099060058594\n",
            "Epoch: 12 Batch: 720 Loss: 0.0058845519088208675\n",
            "Epoch: 12 Batch: 740 Loss: 0.9633368253707886\n",
            "Epoch: 12 Batch: 760 Loss: 0.33451154828071594\n",
            "Epoch: 12 Batch: 780 Loss: 0.07037954032421112\n",
            "Epoch: 12 Batch: 800 Loss: 0.038755226880311966\n",
            "Epoch: 12 Batch: 820 Loss: 0.03224658966064453\n",
            "Epoch: 12 Batch: 840 Loss: 0.27003297209739685\n",
            "Epoch: 12 Batch: 860 Loss: 0.022986317053437233\n",
            "Epoch: 12 Batch: 880 Loss: 0.012940406799316406\n",
            "Epoch: 12 Batch: 900 Loss: 0.030569935217499733\n",
            "Epoch: 12 Batch: 920 Loss: 0.0701010674238205\n",
            "Epoch: 12 Batch: 940 Loss: 0.009799957275390625\n",
            "Epoch: 12 Batch: 960 Loss: 0.0245190616697073\n",
            "Epoch: 12 Batch: 980 Loss: 0.00019741058349609375\n",
            "Epoch: 13 Batch: 0 Loss: 0.037561751902103424\n",
            "Epoch: 13 Batch: 20 Loss: 0.0008241653558798134\n",
            "Epoch: 13 Batch: 40 Loss: 0.013475227169692516\n",
            "Epoch: 13 Batch: 60 Loss: 0.003570175264030695\n",
            "Epoch: 13 Batch: 80 Loss: 0.007932186126708984\n",
            "Epoch: 13 Batch: 100 Loss: 0.00367393484339118\n",
            "Epoch: 13 Batch: 120 Loss: 0.0016226768493652344\n",
            "Epoch: 13 Batch: 140 Loss: 0.09084224700927734\n",
            "Epoch: 13 Batch: 160 Loss: 0.004265403840690851\n",
            "Epoch: 13 Batch: 180 Loss: 0.0008481025579385459\n",
            "Epoch: 13 Batch: 200 Loss: 0.07806143909692764\n",
            "Epoch: 13 Batch: 220 Loss: 0.08374452590942383\n",
            "Epoch: 13 Batch: 240 Loss: 0.011188983917236328\n",
            "Epoch: 13 Batch: 260 Loss: 0.001698398613370955\n",
            "Epoch: 13 Batch: 280 Loss: 0.02790689468383789\n",
            "Epoch: 13 Batch: 300 Loss: 0.029781054705381393\n",
            "Epoch: 13 Batch: 320 Loss: 0.0031997680198401213\n",
            "Epoch: 13 Batch: 340 Loss: 0.10363225638866425\n",
            "Epoch: 13 Batch: 360 Loss: 0.12883205711841583\n",
            "Epoch: 13 Batch: 380 Loss: 0.0012928008800372481\n",
            "Epoch: 13 Batch: 400 Loss: 0.002288436982780695\n",
            "Epoch: 13 Batch: 420 Loss: 0.010582828894257545\n",
            "Epoch: 13 Batch: 440 Loss: 0.000335693359375\n",
            "Epoch: 13 Batch: 460 Loss: 0.17911548912525177\n",
            "Epoch: 13 Batch: 480 Loss: 0.0018080711597576737\n",
            "Epoch: 13 Batch: 500 Loss: 0.008739566430449486\n",
            "Epoch: 13 Batch: 520 Loss: 0.01785869523882866\n",
            "Epoch: 13 Batch: 540 Loss: 0.0009441375732421875\n",
            "Epoch: 13 Batch: 560 Loss: 0.05755043029785156\n",
            "Epoch: 13 Batch: 580 Loss: 0.13553933799266815\n",
            "Epoch: 13 Batch: 600 Loss: 0.02639751508831978\n",
            "Epoch: 13 Batch: 620 Loss: 0.1372164785861969\n",
            "Epoch: 13 Batch: 640 Loss: 0.08472461998462677\n",
            "Epoch: 13 Batch: 660 Loss: 0.00040073395939543843\n",
            "Epoch: 13 Batch: 680 Loss: 0.27304238080978394\n",
            "Epoch: 13 Batch: 700 Loss: 0.46724605560302734\n",
            "Epoch: 13 Batch: 720 Loss: 0.009310531429946423\n",
            "Epoch: 13 Batch: 740 Loss: 0.025255870074033737\n",
            "Epoch: 13 Batch: 760 Loss: 0.012796306982636452\n",
            "Epoch: 13 Batch: 780 Loss: 0.3331630229949951\n",
            "Epoch: 13 Batch: 800 Loss: 0.3333702087402344\n",
            "Epoch: 13 Batch: 820 Loss: 0.09113750606775284\n",
            "Epoch: 13 Batch: 840 Loss: 0.00033550261287018657\n",
            "Epoch: 13 Batch: 860 Loss: 0.06023550033569336\n",
            "Epoch: 13 Batch: 880 Loss: 0.015017414465546608\n",
            "Epoch: 13 Batch: 900 Loss: 0.0057430267333984375\n",
            "Epoch: 13 Batch: 920 Loss: 0.37797579169273376\n",
            "Epoch: 13 Batch: 940 Loss: 0.001621246337890625\n",
            "Epoch: 13 Batch: 960 Loss: 0.0005270958063192666\n",
            "Epoch: 13 Batch: 980 Loss: 0.005262183956801891\n",
            "Epoch: 14 Batch: 0 Loss: 0.0015218735206872225\n",
            "Epoch: 14 Batch: 20 Loss: 0.00551261892542243\n",
            "Epoch: 14 Batch: 40 Loss: 0.06436385959386826\n",
            "Epoch: 14 Batch: 60 Loss: 0.02698345109820366\n",
            "Epoch: 14 Batch: 80 Loss: 0.030719613656401634\n",
            "Epoch: 14 Batch: 100 Loss: 0.0005384444957599044\n",
            "Epoch: 14 Batch: 120 Loss: 0.038803815841674805\n",
            "Epoch: 14 Batch: 140 Loss: 0.008236503228545189\n",
            "Epoch: 14 Batch: 160 Loss: 0.03692145273089409\n",
            "Epoch: 14 Batch: 180 Loss: 0.0010063170921057463\n",
            "Epoch: 14 Batch: 200 Loss: 0.025240134447813034\n",
            "Epoch: 14 Batch: 220 Loss: 0.006895733065903187\n",
            "Epoch: 14 Batch: 240 Loss: 0.08680887520313263\n",
            "Epoch: 14 Batch: 260 Loss: 0.0004207611200399697\n",
            "Epoch: 14 Batch: 280 Loss: 0.0024862289428710938\n",
            "Epoch: 14 Batch: 300 Loss: 0.018782520666718483\n",
            "Epoch: 14 Batch: 320 Loss: 0.015830134972929955\n",
            "Epoch: 14 Batch: 340 Loss: 0.03326015546917915\n",
            "Epoch: 14 Batch: 360 Loss: 0.0067649842239916325\n",
            "Epoch: 14 Batch: 380 Loss: 0.028145408257842064\n",
            "Epoch: 14 Batch: 400 Loss: 0.008489131927490234\n",
            "Epoch: 14 Batch: 420 Loss: 0.0001047134428517893\n",
            "Epoch: 14 Batch: 440 Loss: 0.007252979092299938\n",
            "Epoch: 14 Batch: 460 Loss: 0.005563163664191961\n",
            "Epoch: 14 Batch: 480 Loss: 0.006838989444077015\n",
            "Epoch: 14 Batch: 500 Loss: 0.09143886715173721\n",
            "Epoch: 14 Batch: 520 Loss: 0.017525291070342064\n",
            "Epoch: 14 Batch: 540 Loss: 0.04472465440630913\n",
            "Epoch: 14 Batch: 560 Loss: 0.3009633421897888\n",
            "Epoch: 14 Batch: 580 Loss: 0.008302879519760609\n",
            "Epoch: 14 Batch: 600 Loss: 0.026372432708740234\n",
            "Epoch: 14 Batch: 620 Loss: 0.23139190673828125\n",
            "Epoch: 14 Batch: 640 Loss: 0.058228492736816406\n",
            "Epoch: 14 Batch: 660 Loss: 0.003196144010871649\n",
            "Epoch: 14 Batch: 680 Loss: 0.00030765534029342234\n",
            "Epoch: 14 Batch: 700 Loss: 0.0049836160615086555\n",
            "Epoch: 14 Batch: 720 Loss: 0.0013821602333337069\n",
            "Epoch: 14 Batch: 740 Loss: 0.0005512237548828125\n",
            "Epoch: 14 Batch: 760 Loss: 0.001058387802913785\n",
            "Epoch: 14 Batch: 780 Loss: 0.014465046115219593\n",
            "Epoch: 14 Batch: 800 Loss: 0.002625751541927457\n",
            "Epoch: 14 Batch: 820 Loss: 0.021419143304228783\n",
            "Epoch: 14 Batch: 840 Loss: 0.15906858444213867\n",
            "Epoch: 14 Batch: 860 Loss: 0.17293529212474823\n",
            "Epoch: 14 Batch: 880 Loss: 0.18799524009227753\n",
            "Epoch: 14 Batch: 900 Loss: 0.0030858039390295744\n",
            "Epoch: 14 Batch: 920 Loss: 0.09426073729991913\n",
            "Epoch: 14 Batch: 940 Loss: 0.21084241569042206\n",
            "Epoch: 14 Batch: 960 Loss: 0.02457256242632866\n",
            "Epoch: 14 Batch: 980 Loss: 0.0025166510604321957\n",
            "Epoch: 15 Batch: 0 Loss: 0.00361976632848382\n",
            "Epoch: 15 Batch: 20 Loss: 0.0061318399384617805\n",
            "Epoch: 15 Batch: 40 Loss: 0.03811082988977432\n",
            "Epoch: 15 Batch: 60 Loss: 0.23798684775829315\n",
            "Epoch: 15 Batch: 80 Loss: 0.00015525818162132055\n",
            "Epoch: 15 Batch: 100 Loss: 0.24159936606884003\n",
            "Epoch: 15 Batch: 120 Loss: 0.31113681197166443\n",
            "Epoch: 15 Batch: 140 Loss: 0.6095081567764282\n",
            "Epoch: 15 Batch: 160 Loss: 0.00103588099591434\n",
            "Epoch: 15 Batch: 180 Loss: 0.00015382767014671117\n",
            "Epoch: 15 Batch: 200 Loss: 0.0018114090198650956\n",
            "Epoch: 15 Batch: 220 Loss: 0.6867350339889526\n",
            "Epoch: 15 Batch: 240 Loss: 0.02759704552590847\n",
            "Epoch: 15 Batch: 260 Loss: 0.0015253067249432206\n",
            "Epoch: 15 Batch: 280 Loss: 0.027488280087709427\n",
            "Epoch: 15 Batch: 300 Loss: 0.012992572970688343\n",
            "Epoch: 15 Batch: 320 Loss: 0.13853678107261658\n",
            "Epoch: 15 Batch: 340 Loss: 0.022096823900938034\n",
            "Epoch: 15 Batch: 360 Loss: 0.0019060134654864669\n",
            "Epoch: 15 Batch: 380 Loss: 0.3833393454551697\n",
            "Epoch: 15 Batch: 400 Loss: 0.014611244201660156\n",
            "Epoch: 15 Batch: 420 Loss: 0.9034975171089172\n",
            "Epoch: 15 Batch: 440 Loss: 0.01744070090353489\n",
            "Epoch: 15 Batch: 460 Loss: 0.04552745819091797\n",
            "Epoch: 15 Batch: 480 Loss: 0.34330469369888306\n",
            "Epoch: 15 Batch: 500 Loss: 0.011571121402084827\n",
            "Epoch: 15 Batch: 520 Loss: 0.02460947073996067\n",
            "Epoch: 15 Batch: 540 Loss: 0.3405640125274658\n",
            "Epoch: 15 Batch: 560 Loss: 0.00196495046839118\n",
            "Epoch: 15 Batch: 580 Loss: 0.002318477723747492\n",
            "Epoch: 15 Batch: 600 Loss: 0.1866706907749176\n",
            "Epoch: 15 Batch: 620 Loss: 0.0012778282398357987\n",
            "Epoch: 15 Batch: 640 Loss: 0.01187820453196764\n",
            "Epoch: 15 Batch: 660 Loss: 0.0008518219110555947\n",
            "Epoch: 15 Batch: 680 Loss: 0.0007452011341229081\n",
            "Epoch: 15 Batch: 700 Loss: 0.0010395050048828125\n",
            "Epoch: 15 Batch: 720 Loss: 0.016680097207427025\n",
            "Epoch: 15 Batch: 740 Loss: 0.0006392478826455772\n",
            "Epoch: 15 Batch: 760 Loss: 4.76837158203125e-05\n",
            "Epoch: 15 Batch: 780 Loss: 0.00361213693395257\n",
            "Epoch: 15 Batch: 800 Loss: 0.0033932686783373356\n",
            "Epoch: 15 Batch: 820 Loss: 0.016063595190644264\n",
            "Epoch: 15 Batch: 840 Loss: 0.00039882661076262593\n",
            "Epoch: 15 Batch: 860 Loss: 0.046327970921993256\n",
            "Epoch: 15 Batch: 880 Loss: 0.09120893478393555\n",
            "Epoch: 15 Batch: 900 Loss: 0.7583364248275757\n",
            "Epoch: 15 Batch: 920 Loss: 0.0036109923385083675\n",
            "Epoch: 15 Batch: 940 Loss: 0.029023360460996628\n",
            "Epoch: 15 Batch: 960 Loss: 0.0530911460518837\n",
            "Epoch: 15 Batch: 980 Loss: 0.05205383151769638\n",
            "Epoch: 16 Batch: 0 Loss: 0.001217746757902205\n",
            "Epoch: 16 Batch: 20 Loss: 0.015108585357666016\n",
            "Epoch: 16 Batch: 40 Loss: 0.0015019417041912675\n",
            "Epoch: 16 Batch: 60 Loss: 0.0009639739873819053\n",
            "Epoch: 16 Batch: 80 Loss: 0.821154773235321\n",
            "Epoch: 16 Batch: 100 Loss: 0.00249824533239007\n",
            "Epoch: 16 Batch: 120 Loss: 0.002245998475700617\n",
            "Epoch: 16 Batch: 140 Loss: 0.02188720740377903\n",
            "Epoch: 16 Batch: 160 Loss: 0.43141239881515503\n",
            "Epoch: 16 Batch: 180 Loss: 0.09783001244068146\n",
            "Epoch: 16 Batch: 200 Loss: 0.0012182235950604081\n",
            "Epoch: 16 Batch: 220 Loss: 0.0017046928405761719\n",
            "Epoch: 16 Batch: 240 Loss: 0.00032329559326171875\n",
            "Epoch: 16 Batch: 260 Loss: 0.0006300926324911416\n",
            "Epoch: 16 Batch: 280 Loss: 0.08400402218103409\n",
            "Epoch: 16 Batch: 300 Loss: 0.0002330779971089214\n",
            "Epoch: 16 Batch: 320 Loss: 0.03815317153930664\n",
            "Epoch: 16 Batch: 340 Loss: 0.0026281357277184725\n",
            "Epoch: 16 Batch: 360 Loss: 0.269134521484375\n",
            "Epoch: 16 Batch: 380 Loss: 0.0057852743193507195\n",
            "Epoch: 16 Batch: 400 Loss: 0.014835357666015625\n",
            "Epoch: 16 Batch: 420 Loss: 0.0029458999633789062\n",
            "Epoch: 16 Batch: 440 Loss: 0.02291860617697239\n",
            "Epoch: 16 Batch: 460 Loss: 0.037915706634521484\n",
            "Epoch: 16 Batch: 480 Loss: 0.030475711449980736\n",
            "Epoch: 16 Batch: 500 Loss: 0.0044841766357421875\n",
            "Epoch: 16 Batch: 520 Loss: 0.04582185670733452\n",
            "Epoch: 16 Batch: 540 Loss: 0.022191811352968216\n",
            "Epoch: 16 Batch: 560 Loss: 0.004726218990981579\n",
            "Epoch: 16 Batch: 580 Loss: 0.04403696209192276\n",
            "Epoch: 16 Batch: 600 Loss: 0.11132554709911346\n",
            "Epoch: 16 Batch: 620 Loss: 0.3390485644340515\n",
            "Epoch: 16 Batch: 640 Loss: 0.013242530636489391\n",
            "Epoch: 16 Batch: 660 Loss: 0.0027854919899255037\n",
            "Epoch: 16 Batch: 680 Loss: 0.07194814831018448\n",
            "Epoch: 16 Batch: 700 Loss: 0.053000450134277344\n",
            "Epoch: 16 Batch: 720 Loss: 0.05150775983929634\n",
            "Epoch: 16 Batch: 740 Loss: 0.00038318632869049907\n",
            "Epoch: 16 Batch: 760 Loss: 0.16539554297924042\n",
            "Epoch: 16 Batch: 780 Loss: 0.10337720066308975\n",
            "Epoch: 16 Batch: 800 Loss: 0.1086040511727333\n",
            "Epoch: 16 Batch: 820 Loss: 0.11817741394042969\n",
            "Epoch: 16 Batch: 840 Loss: 0.2108747959136963\n",
            "Epoch: 16 Batch: 860 Loss: 0.09198613464832306\n",
            "Epoch: 16 Batch: 880 Loss: 0.016754627227783203\n",
            "Epoch: 16 Batch: 900 Loss: 0.0025941848289221525\n",
            "Epoch: 16 Batch: 920 Loss: 0.07934417575597763\n",
            "Epoch: 16 Batch: 940 Loss: 0.00336627964861691\n",
            "Epoch: 16 Batch: 960 Loss: 0.9721013307571411\n",
            "Epoch: 16 Batch: 980 Loss: 0.011422920040786266\n",
            "Epoch: 17 Batch: 0 Loss: 0.143775075674057\n",
            "Epoch: 17 Batch: 20 Loss: 0.00040435791015625\n",
            "Epoch: 17 Batch: 40 Loss: 0.17141227424144745\n",
            "Epoch: 17 Batch: 60 Loss: 4.653930591302924e-05\n",
            "Epoch: 17 Batch: 80 Loss: 0.015047932043671608\n",
            "Epoch: 17 Batch: 100 Loss: 0.0011607169872149825\n",
            "Epoch: 17 Batch: 120 Loss: 1.1049981117248535\n",
            "Epoch: 17 Batch: 140 Loss: 0.006054878234863281\n",
            "Epoch: 17 Batch: 160 Loss: 0.0011362076038494706\n",
            "Epoch: 17 Batch: 180 Loss: 0.07443676143884659\n",
            "Epoch: 17 Batch: 200 Loss: 0.006011390592902899\n",
            "Epoch: 17 Batch: 220 Loss: 0.021202469244599342\n",
            "Epoch: 17 Batch: 240 Loss: 0.0007226943853311241\n",
            "Epoch: 17 Batch: 260 Loss: 0.00202522287145257\n",
            "Epoch: 17 Batch: 280 Loss: 0.07088146358728409\n",
            "Epoch: 17 Batch: 300 Loss: 0.00018758773512672633\n",
            "Epoch: 17 Batch: 320 Loss: 0.001224517822265625\n",
            "Epoch: 17 Batch: 340 Loss: 0.012653350830078125\n",
            "Epoch: 17 Batch: 360 Loss: 0.015589332208037376\n",
            "Epoch: 17 Batch: 380 Loss: 0.003595542861148715\n",
            "Epoch: 17 Batch: 400 Loss: 0.001129150390625\n",
            "Epoch: 17 Batch: 420 Loss: 0.0003551483096089214\n",
            "Epoch: 17 Batch: 440 Loss: 0.0027819634415209293\n",
            "Epoch: 17 Batch: 460 Loss: 0.0009000778081826866\n",
            "Epoch: 17 Batch: 480 Loss: 0.01335611380636692\n",
            "Epoch: 17 Batch: 500 Loss: 0.001126956893131137\n",
            "Epoch: 17 Batch: 520 Loss: 0.08583040535449982\n",
            "Epoch: 17 Batch: 540 Loss: 0.008159637451171875\n",
            "Epoch: 17 Batch: 560 Loss: 0.0024279593490064144\n",
            "Epoch: 17 Batch: 580 Loss: 0.00799407996237278\n",
            "Epoch: 17 Batch: 600 Loss: 0.0005829810979776084\n",
            "Epoch: 17 Batch: 620 Loss: 0.184798002243042\n",
            "Epoch: 17 Batch: 640 Loss: 0.02966451644897461\n",
            "Epoch: 17 Batch: 660 Loss: 0.015728473663330078\n",
            "Epoch: 17 Batch: 680 Loss: 0.045647621154785156\n",
            "Epoch: 17 Batch: 700 Loss: 0.07783699035644531\n",
            "Epoch: 17 Batch: 720 Loss: 0.0018438339466229081\n",
            "Epoch: 17 Batch: 740 Loss: 0.0059600831009447575\n",
            "Epoch: 17 Batch: 760 Loss: 0.006217002868652344\n",
            "Epoch: 17 Batch: 780 Loss: 0.01361227035522461\n",
            "Epoch: 17 Batch: 800 Loss: 0.007713699247688055\n",
            "Epoch: 17 Batch: 820 Loss: 0.0005224227788858116\n",
            "Epoch: 17 Batch: 840 Loss: 0.0036614418495446444\n",
            "Epoch: 17 Batch: 860 Loss: 0.052998922765254974\n",
            "Epoch: 17 Batch: 880 Loss: 0.0007414817810058594\n",
            "Epoch: 17 Batch: 900 Loss: 0.03880252689123154\n",
            "Epoch: 17 Batch: 920 Loss: 0.0372372642159462\n",
            "Epoch: 17 Batch: 940 Loss: 0.05133981630206108\n",
            "Epoch: 17 Batch: 960 Loss: 0.016417885199189186\n",
            "Epoch: 17 Batch: 980 Loss: 0.0390050895512104\n",
            "Epoch: 18 Batch: 0 Loss: 0.0021270751021802425\n",
            "Epoch: 18 Batch: 20 Loss: 0.013015270233154297\n",
            "Epoch: 18 Batch: 40 Loss: 0.01474466361105442\n",
            "Epoch: 18 Batch: 60 Loss: 0.0038407326210290194\n",
            "Epoch: 18 Batch: 80 Loss: 0.04970502853393555\n",
            "Epoch: 18 Batch: 100 Loss: 0.006303978152573109\n",
            "Epoch: 18 Batch: 120 Loss: 0.011876583099365234\n",
            "Epoch: 18 Batch: 140 Loss: 0.011023426428437233\n",
            "Epoch: 18 Batch: 160 Loss: 0.0013208389282226562\n",
            "Epoch: 18 Batch: 180 Loss: 0.0017627716297283769\n",
            "Epoch: 18 Batch: 200 Loss: 0.008447552099823952\n",
            "Epoch: 18 Batch: 220 Loss: 0.001531314803287387\n",
            "Epoch: 18 Batch: 240 Loss: 0.022324562072753906\n",
            "Epoch: 18 Batch: 260 Loss: 0.07337136566638947\n",
            "Epoch: 18 Batch: 280 Loss: 0.0001426696835551411\n",
            "Epoch: 18 Batch: 300 Loss: 0.1083768829703331\n",
            "Epoch: 18 Batch: 320 Loss: 0.016213703900575638\n",
            "Epoch: 18 Batch: 340 Loss: 0.00129108433611691\n",
            "Epoch: 18 Batch: 360 Loss: 0.06271390616893768\n",
            "Epoch: 18 Batch: 380 Loss: 0.18074245750904083\n",
            "Epoch: 18 Batch: 400 Loss: 0.020093441009521484\n",
            "Epoch: 18 Batch: 420 Loss: 0.0018876076210290194\n",
            "Epoch: 18 Batch: 440 Loss: 0.04952831193804741\n",
            "Epoch: 18 Batch: 460 Loss: 0.02212514914572239\n",
            "Epoch: 18 Batch: 480 Loss: 0.5328543782234192\n",
            "Epoch: 18 Batch: 500 Loss: 0.007446670439094305\n",
            "Epoch: 18 Batch: 520 Loss: 0.0007144928094930947\n",
            "Epoch: 18 Batch: 540 Loss: 0.002317714737728238\n",
            "Epoch: 18 Batch: 560 Loss: 0.04310045391321182\n",
            "Epoch: 18 Batch: 580 Loss: 0.019052982330322266\n",
            "Epoch: 18 Batch: 600 Loss: 0.16512994468212128\n",
            "Epoch: 18 Batch: 620 Loss: 0.0006851196521893144\n",
            "Epoch: 18 Batch: 640 Loss: 0.00077056884765625\n",
            "Epoch: 18 Batch: 660 Loss: 0.0051825521513819695\n",
            "Epoch: 18 Batch: 680 Loss: 0.0027683258522301912\n",
            "Epoch: 18 Batch: 700 Loss: 0.00936746597290039\n",
            "Epoch: 18 Batch: 720 Loss: 0.004423904232680798\n",
            "Epoch: 18 Batch: 740 Loss: 0.006694125942885876\n",
            "Epoch: 18 Batch: 760 Loss: 0.0004554748593363911\n",
            "Epoch: 18 Batch: 780 Loss: 6.4849853515625e-05\n",
            "Epoch: 18 Batch: 800 Loss: 0.4381635785102844\n",
            "Epoch: 18 Batch: 820 Loss: 0.0457310676574707\n",
            "Epoch: 18 Batch: 840 Loss: 0.006042099092155695\n",
            "Epoch: 18 Batch: 860 Loss: 0.13548021018505096\n",
            "Epoch: 18 Batch: 880 Loss: 0.0075782774947583675\n",
            "Epoch: 18 Batch: 900 Loss: 0.010782718658447266\n",
            "Epoch: 18 Batch: 920 Loss: 0.32740432024002075\n",
            "Epoch: 18 Batch: 940 Loss: 0.014718150720000267\n",
            "Epoch: 18 Batch: 960 Loss: 0.2760852873325348\n",
            "Epoch: 18 Batch: 980 Loss: 0.0005966186290606856\n",
            "Epoch: 19 Batch: 0 Loss: 0.002326583955436945\n",
            "Epoch: 19 Batch: 20 Loss: 0.02820911444723606\n",
            "Epoch: 19 Batch: 40 Loss: 0.005648898892104626\n",
            "Epoch: 19 Batch: 60 Loss: 0.0005291939014568925\n",
            "Epoch: 19 Batch: 80 Loss: 0.12308244407176971\n",
            "Epoch: 19 Batch: 100 Loss: 0.011839580722153187\n",
            "Epoch: 19 Batch: 120 Loss: 0.05370211601257324\n",
            "Epoch: 19 Batch: 140 Loss: 0.0010165214771404862\n",
            "Epoch: 19 Batch: 160 Loss: 0.006020069122314453\n",
            "Epoch: 19 Batch: 180 Loss: 0.018172550946474075\n",
            "Epoch: 19 Batch: 200 Loss: 0.00022411346435546875\n",
            "Epoch: 19 Batch: 220 Loss: 0.0075439452193677425\n",
            "Epoch: 19 Batch: 240 Loss: 0.00012216568575240672\n",
            "Epoch: 19 Batch: 260 Loss: 0.2056674063205719\n",
            "Epoch: 19 Batch: 280 Loss: 0.010292244143784046\n",
            "Epoch: 19 Batch: 300 Loss: 0.0001163482666015625\n",
            "Epoch: 19 Batch: 320 Loss: 0.17474666237831116\n",
            "Epoch: 19 Batch: 340 Loss: 0.0037238120567053556\n",
            "Epoch: 19 Batch: 360 Loss: 0.29388493299484253\n",
            "Epoch: 19 Batch: 380 Loss: 0.019782161340117455\n",
            "Epoch: 19 Batch: 400 Loss: 0.11089291423559189\n",
            "Epoch: 19 Batch: 420 Loss: 0.1970527619123459\n",
            "Epoch: 19 Batch: 440 Loss: 0.00124187464825809\n",
            "Epoch: 19 Batch: 460 Loss: 0.051386453211307526\n",
            "Epoch: 19 Batch: 480 Loss: 0.09618782997131348\n",
            "Epoch: 19 Batch: 500 Loss: 0.5552757382392883\n",
            "Epoch: 19 Batch: 520 Loss: 0.04010286182165146\n",
            "Epoch: 19 Batch: 540 Loss: 0.0175933837890625\n",
            "Epoch: 19 Batch: 560 Loss: 0.056559037417173386\n",
            "Epoch: 19 Batch: 580 Loss: 2.9468536013155244e-05\n",
            "Epoch: 19 Batch: 600 Loss: 0.024699974805116653\n",
            "Epoch: 19 Batch: 620 Loss: 0.1034388542175293\n",
            "Epoch: 19 Batch: 640 Loss: 0.0005781173822470009\n",
            "Epoch: 19 Batch: 660 Loss: 0.14081096649169922\n",
            "Epoch: 19 Batch: 680 Loss: 0.00651626568287611\n",
            "Epoch: 19 Batch: 700 Loss: 0.0018575668800622225\n",
            "Epoch: 19 Batch: 720 Loss: 0.0008849144214764237\n",
            "Epoch: 19 Batch: 740 Loss: 0.22936506569385529\n",
            "Epoch: 19 Batch: 760 Loss: 0.03760995715856552\n",
            "Epoch: 19 Batch: 780 Loss: 0.0063302041962742805\n",
            "Epoch: 19 Batch: 800 Loss: 0.01209039706736803\n",
            "Epoch: 19 Batch: 820 Loss: 0.02258272096514702\n",
            "Epoch: 19 Batch: 840 Loss: 0.011875676922500134\n",
            "Epoch: 19 Batch: 860 Loss: 0.0020923614501953125\n",
            "Epoch: 19 Batch: 880 Loss: 0.12427063286304474\n",
            "Epoch: 19 Batch: 900 Loss: 0.0001502990780863911\n",
            "Epoch: 19 Batch: 920 Loss: 0.07806611061096191\n",
            "Epoch: 19 Batch: 940 Loss: 0.021758174523711205\n",
            "Epoch: 19 Batch: 960 Loss: 0.055966950953006744\n",
            "Epoch: 19 Batch: 980 Loss: 0.0012504577171057463\n",
            "Epoch: 20 Batch: 0 Loss: 0.00036792753962799907\n",
            "Epoch: 20 Batch: 20 Loss: 0.0006801605341024697\n",
            "Epoch: 20 Batch: 40 Loss: 0.007815646938979626\n",
            "Epoch: 20 Batch: 60 Loss: 0.01891651190817356\n",
            "Epoch: 20 Batch: 80 Loss: 0.0436222068965435\n",
            "Epoch: 20 Batch: 100 Loss: 0.005380439572036266\n",
            "Epoch: 20 Batch: 120 Loss: 0.03385772556066513\n",
            "Epoch: 20 Batch: 140 Loss: 0.0010074615711346269\n",
            "Epoch: 20 Batch: 160 Loss: 0.006007385440170765\n",
            "Epoch: 20 Batch: 180 Loss: 0.08185043185949326\n",
            "Epoch: 20 Batch: 200 Loss: 0.0108191492035985\n",
            "Epoch: 20 Batch: 220 Loss: 4.100799560546875e-05\n",
            "Epoch: 20 Batch: 240 Loss: 0.00013217926607467234\n",
            "Epoch: 20 Batch: 260 Loss: 0.00461997976526618\n",
            "Epoch: 20 Batch: 280 Loss: 0.006847286131232977\n",
            "Epoch: 20 Batch: 300 Loss: 0.02867879904806614\n",
            "Epoch: 20 Batch: 320 Loss: 4.024505687993951e-05\n",
            "Epoch: 20 Batch: 340 Loss: 0.3104793429374695\n",
            "Epoch: 20 Batch: 360 Loss: 0.0008323669317178428\n",
            "Epoch: 20 Batch: 380 Loss: 0.00900344830006361\n",
            "Epoch: 20 Batch: 400 Loss: 0.04224419593811035\n",
            "Epoch: 20 Batch: 420 Loss: 0.0001663207949604839\n",
            "Epoch: 20 Batch: 440 Loss: 0.005078506655991077\n",
            "Epoch: 20 Batch: 460 Loss: 0.0002452850458212197\n",
            "Epoch: 20 Batch: 480 Loss: 0.0005826950073242188\n",
            "Epoch: 20 Batch: 500 Loss: 0.0002189636288676411\n",
            "Epoch: 20 Batch: 520 Loss: 0.004313659854233265\n",
            "Epoch: 20 Batch: 540 Loss: 0.48954716324806213\n",
            "Epoch: 20 Batch: 560 Loss: 0.016571665182709694\n",
            "Epoch: 20 Batch: 580 Loss: 0.009511279873549938\n",
            "Epoch: 20 Batch: 600 Loss: 0.014218521304428577\n",
            "Epoch: 20 Batch: 620 Loss: 0.011270999908447266\n",
            "Epoch: 20 Batch: 640 Loss: 1.5180096626281738\n",
            "Epoch: 20 Batch: 660 Loss: 0.0009682655218057334\n",
            "Epoch: 20 Batch: 680 Loss: 0.03300518915057182\n",
            "Epoch: 20 Batch: 700 Loss: 0.03735151141881943\n",
            "Epoch: 20 Batch: 720 Loss: 0.34436994791030884\n",
            "Epoch: 20 Batch: 740 Loss: 0.0031253814231604338\n",
            "Epoch: 20 Batch: 760 Loss: 0.003421211149543524\n",
            "Epoch: 20 Batch: 780 Loss: 0.004958629608154297\n",
            "Epoch: 20 Batch: 800 Loss: 0.2252238690853119\n",
            "Epoch: 20 Batch: 820 Loss: 0.07241058349609375\n",
            "Epoch: 20 Batch: 840 Loss: 0.16877445578575134\n",
            "Epoch: 20 Batch: 860 Loss: 0.003700828645378351\n",
            "Epoch: 20 Batch: 880 Loss: 0.10259079933166504\n",
            "Epoch: 20 Batch: 900 Loss: 0.006228161044418812\n",
            "Epoch: 20 Batch: 920 Loss: 0.002235793974250555\n",
            "Epoch: 20 Batch: 940 Loss: 0.07565250247716904\n",
            "Epoch: 20 Batch: 960 Loss: 0.00800390262156725\n",
            "Epoch: 20 Batch: 980 Loss: 0.00048770903958939016\n",
            "Epoch: 21 Batch: 0 Loss: 0.20010347664356232\n",
            "Epoch: 21 Batch: 20 Loss: 0.04288196563720703\n",
            "Epoch: 21 Batch: 40 Loss: 0.006195926573127508\n",
            "Epoch: 21 Batch: 60 Loss: 5.855560448253527e-05\n",
            "Epoch: 21 Batch: 80 Loss: 0.09287433326244354\n",
            "Epoch: 21 Batch: 100 Loss: 0.022790003567934036\n",
            "Epoch: 21 Batch: 120 Loss: 0.05839033052325249\n",
            "Epoch: 21 Batch: 140 Loss: 0.1783636063337326\n",
            "Epoch: 21 Batch: 160 Loss: 0.008205985650420189\n",
            "Epoch: 21 Batch: 180 Loss: 0.0070175169967114925\n",
            "Epoch: 21 Batch: 200 Loss: 0.0065513611771166325\n",
            "Epoch: 21 Batch: 220 Loss: 0.0009696960332803428\n",
            "Epoch: 21 Batch: 240 Loss: 0.006880283355712891\n",
            "Epoch: 21 Batch: 260 Loss: 0.16614365577697754\n",
            "Epoch: 21 Batch: 280 Loss: 0.005753421690315008\n",
            "Epoch: 21 Batch: 300 Loss: 0.011220932006835938\n",
            "Epoch: 21 Batch: 320 Loss: 0.009535026736557484\n",
            "Epoch: 21 Batch: 340 Loss: 0.009976387023925781\n",
            "Epoch: 21 Batch: 360 Loss: 0.0535917766392231\n",
            "Epoch: 21 Batch: 380 Loss: 0.010850334540009499\n",
            "Epoch: 21 Batch: 400 Loss: 0.0001567840517964214\n",
            "Epoch: 21 Batch: 420 Loss: 0.0024334907066076994\n",
            "Epoch: 21 Batch: 440 Loss: 0.0037322044372558594\n",
            "Epoch: 21 Batch: 460 Loss: 0.03245363384485245\n",
            "Epoch: 21 Batch: 480 Loss: 0.0032999038230627775\n",
            "Epoch: 21 Batch: 500 Loss: 0.12298174202442169\n",
            "Epoch: 21 Batch: 520 Loss: 0.0006343841669149697\n",
            "Epoch: 21 Batch: 540 Loss: 0.0019105911487713456\n",
            "Epoch: 21 Batch: 560 Loss: 0.0012621879577636719\n",
            "Epoch: 21 Batch: 580 Loss: 0.82915860414505\n",
            "Epoch: 21 Batch: 600 Loss: 0.0012001991271972656\n",
            "Epoch: 21 Batch: 620 Loss: 0.04071688652038574\n",
            "Epoch: 21 Batch: 640 Loss: 0.0013846397632732987\n",
            "Epoch: 21 Batch: 660 Loss: 0.003086185548454523\n",
            "Epoch: 21 Batch: 680 Loss: 0.07305564731359482\n",
            "Epoch: 21 Batch: 700 Loss: 0.022436905652284622\n",
            "Epoch: 21 Batch: 720 Loss: 0.00013780593872070312\n",
            "Epoch: 21 Batch: 740 Loss: 0.04812593385577202\n",
            "Epoch: 21 Batch: 760 Loss: 0.03198046609759331\n",
            "Epoch: 21 Batch: 780 Loss: 0.0016301155555993319\n",
            "Epoch: 21 Batch: 800 Loss: 0.053965043276548386\n",
            "Epoch: 21 Batch: 820 Loss: 0.0032074928749352694\n",
            "Epoch: 21 Batch: 840 Loss: 0.6921552419662476\n",
            "Epoch: 21 Batch: 860 Loss: 0.2287263423204422\n",
            "Epoch: 21 Batch: 880 Loss: 0.0275726318359375\n",
            "Epoch: 21 Batch: 900 Loss: 0.25516143441200256\n",
            "Epoch: 21 Batch: 920 Loss: 4.57763671875e-05\n",
            "Epoch: 21 Batch: 940 Loss: 1.1444091796875e-05\n",
            "Epoch: 21 Batch: 960 Loss: 0.010856819339096546\n",
            "Epoch: 21 Batch: 980 Loss: 0.4279407560825348\n",
            "Epoch: 22 Batch: 0 Loss: 0.10798964649438858\n",
            "Epoch: 22 Batch: 20 Loss: 0.0038480758666992188\n",
            "Epoch: 22 Batch: 40 Loss: 0.029238510876893997\n",
            "Epoch: 22 Batch: 60 Loss: 0.000110626220703125\n",
            "Epoch: 22 Batch: 80 Loss: 0.00018405914306640625\n",
            "Epoch: 22 Batch: 100 Loss: 0.03158855438232422\n",
            "Epoch: 22 Batch: 120 Loss: 0.00027256010798737407\n",
            "Epoch: 22 Batch: 140 Loss: 0.00016727446927689016\n",
            "Epoch: 22 Batch: 160 Loss: 0.002787685487419367\n",
            "Epoch: 22 Batch: 180 Loss: 9.632110595703125e-05\n",
            "Epoch: 22 Batch: 200 Loss: 2.326965295651462e-05\n",
            "Epoch: 22 Batch: 220 Loss: 0.004438972566276789\n",
            "Epoch: 22 Batch: 240 Loss: 0.3601328730583191\n",
            "Epoch: 22 Batch: 260 Loss: 3.814697322468419e-07\n",
            "Epoch: 22 Batch: 280 Loss: 0.006269359495490789\n",
            "Epoch: 22 Batch: 300 Loss: 0.0036717415787279606\n",
            "Epoch: 22 Batch: 320 Loss: 0.005141735076904297\n",
            "Epoch: 22 Batch: 340 Loss: 0.0014878272777423263\n",
            "Epoch: 22 Batch: 360 Loss: 0.03391542285680771\n",
            "Epoch: 22 Batch: 380 Loss: 0.012285614386200905\n",
            "Epoch: 22 Batch: 400 Loss: 1.178052544593811\n",
            "Epoch: 22 Batch: 420 Loss: 0.005573940463364124\n",
            "Epoch: 22 Batch: 440 Loss: 0.01485223788768053\n",
            "Epoch: 22 Batch: 460 Loss: 0.011386776342988014\n",
            "Epoch: 22 Batch: 480 Loss: 0.07332222163677216\n",
            "Epoch: 22 Batch: 500 Loss: 0.04130582883954048\n",
            "Epoch: 22 Batch: 520 Loss: 0.02088141441345215\n",
            "Epoch: 22 Batch: 540 Loss: 1.15748929977417\n",
            "Epoch: 22 Batch: 560 Loss: 0.014249610714614391\n",
            "Epoch: 22 Batch: 580 Loss: 0.02445049211382866\n",
            "Epoch: 22 Batch: 600 Loss: 0.008493232540786266\n",
            "Epoch: 22 Batch: 620 Loss: 0.0035198212135583162\n",
            "Epoch: 22 Batch: 640 Loss: 0.22070951759815216\n",
            "Epoch: 22 Batch: 660 Loss: 0.4027772843837738\n",
            "Epoch: 22 Batch: 680 Loss: 0.07007346302270889\n",
            "Epoch: 22 Batch: 700 Loss: 0.15597228705883026\n",
            "Epoch: 22 Batch: 720 Loss: 0.00537452707067132\n",
            "Epoch: 22 Batch: 740 Loss: 0.027273844927549362\n",
            "Epoch: 22 Batch: 760 Loss: 2.136230432370212e-05\n",
            "Epoch: 22 Batch: 780 Loss: 0.13215327262878418\n",
            "Epoch: 22 Batch: 800 Loss: 0.005385112948715687\n",
            "Epoch: 22 Batch: 820 Loss: 4.1961669921875e-05\n",
            "Epoch: 22 Batch: 840 Loss: 0.0006771087646484375\n",
            "Epoch: 22 Batch: 860 Loss: 0.0422857291996479\n",
            "Epoch: 22 Batch: 880 Loss: 0.5116778612136841\n",
            "Epoch: 22 Batch: 900 Loss: 0.0024780272506177425\n",
            "Epoch: 22 Batch: 920 Loss: 0.00020122528076171875\n",
            "Epoch: 22 Batch: 940 Loss: 8.79287690622732e-05\n",
            "Epoch: 22 Batch: 960 Loss: 0.0027206421364098787\n",
            "Epoch: 22 Batch: 980 Loss: 0.0012329102028161287\n",
            "Epoch: 23 Batch: 0 Loss: 0.29219040274620056\n",
            "Epoch: 23 Batch: 20 Loss: 0.15909957885742188\n",
            "Epoch: 23 Batch: 40 Loss: 0.0014859199291095138\n",
            "Epoch: 23 Batch: 60 Loss: 0.0007081031799316406\n",
            "Epoch: 23 Batch: 80 Loss: 0.00022010803513694555\n",
            "Epoch: 23 Batch: 100 Loss: 0.0014742851490154862\n",
            "Epoch: 23 Batch: 120 Loss: 0.31238365173339844\n",
            "Epoch: 23 Batch: 140 Loss: 0.12157955020666122\n",
            "Epoch: 23 Batch: 160 Loss: 0.004415321163833141\n",
            "Epoch: 23 Batch: 180 Loss: 0.0005849838489666581\n",
            "Epoch: 23 Batch: 200 Loss: 0.006104660220444202\n",
            "Epoch: 23 Batch: 220 Loss: 0.002834415528923273\n",
            "Epoch: 23 Batch: 240 Loss: 0.0006367683527059853\n",
            "Epoch: 23 Batch: 260 Loss: 0.00026302336482331157\n",
            "Epoch: 23 Batch: 280 Loss: 0.3486606478691101\n",
            "Epoch: 23 Batch: 300 Loss: 0.2810385823249817\n",
            "Epoch: 23 Batch: 320 Loss: 0.025337791070342064\n",
            "Epoch: 23 Batch: 340 Loss: 0.00019979476928710938\n",
            "Epoch: 23 Batch: 360 Loss: 0.00013809204392600805\n",
            "Epoch: 23 Batch: 380 Loss: 0.17401857674121857\n",
            "Epoch: 23 Batch: 400 Loss: 0.03096914291381836\n",
            "Epoch: 23 Batch: 420 Loss: 0.0014455795753747225\n",
            "Epoch: 23 Batch: 440 Loss: 0.08948306739330292\n",
            "Epoch: 23 Batch: 460 Loss: 0.005567216780036688\n",
            "Epoch: 23 Batch: 480 Loss: 0.0007287979242391884\n",
            "Epoch: 23 Batch: 500 Loss: 0.0030232430435717106\n",
            "Epoch: 23 Batch: 520 Loss: 0.0007995605701580644\n",
            "Epoch: 23 Batch: 540 Loss: 0.16835574805736542\n",
            "Epoch: 23 Batch: 560 Loss: 0.5901469588279724\n",
            "Epoch: 23 Batch: 580 Loss: 0.0052551268599927425\n",
            "Epoch: 23 Batch: 600 Loss: 0.00033483505831100047\n",
            "Epoch: 23 Batch: 620 Loss: 0.0018695831531658769\n",
            "Epoch: 23 Batch: 640 Loss: 0.0019344330066815019\n",
            "Epoch: 23 Batch: 660 Loss: 1.4264013767242432\n",
            "Epoch: 23 Batch: 680 Loss: 0.08414864540100098\n",
            "Epoch: 23 Batch: 700 Loss: 0.005555152893066406\n",
            "Epoch: 23 Batch: 720 Loss: 0.0020749091636389494\n",
            "Epoch: 23 Batch: 740 Loss: 0.007098960690200329\n",
            "Epoch: 23 Batch: 760 Loss: 0.0005537986871786416\n",
            "Epoch: 23 Batch: 780 Loss: 0.004958057310432196\n",
            "Epoch: 23 Batch: 800 Loss: 0.0008180618169717491\n",
            "Epoch: 23 Batch: 820 Loss: 0.058358289301395416\n",
            "Epoch: 23 Batch: 840 Loss: 7.82012921263231e-06\n",
            "Epoch: 23 Batch: 860 Loss: 0.0026890754234045744\n",
            "Epoch: 23 Batch: 880 Loss: 0.0002738952753134072\n",
            "Epoch: 23 Batch: 900 Loss: 0.013324165716767311\n",
            "Epoch: 23 Batch: 920 Loss: 0.02194690704345703\n",
            "Epoch: 23 Batch: 940 Loss: 0.012484359554946423\n",
            "Epoch: 23 Batch: 960 Loss: 0.5460702776908875\n",
            "Epoch: 23 Batch: 980 Loss: 0.49212369322776794\n",
            "Epoch: 24 Batch: 0 Loss: 0.021024322137236595\n",
            "Epoch: 24 Batch: 20 Loss: 0.00020914078049827367\n",
            "Epoch: 24 Batch: 40 Loss: 4.4155120122013614e-05\n",
            "Epoch: 24 Batch: 60 Loss: 0.0931737869977951\n",
            "Epoch: 24 Batch: 80 Loss: 0.0005197525024414062\n",
            "Epoch: 24 Batch: 100 Loss: 0.05016288906335831\n",
            "Epoch: 24 Batch: 120 Loss: 0.0005004882696084678\n",
            "Epoch: 24 Batch: 140 Loss: 0.0015943527687340975\n",
            "Epoch: 24 Batch: 160 Loss: 0.032746411859989166\n",
            "Epoch: 24 Batch: 180 Loss: 0.04373989254236221\n",
            "Epoch: 24 Batch: 200 Loss: 0.01162796001881361\n",
            "Epoch: 24 Batch: 220 Loss: 0.00031032561673782766\n",
            "Epoch: 24 Batch: 240 Loss: 0.0020112991333007812\n",
            "Epoch: 24 Batch: 260 Loss: 0.0006973266717977822\n",
            "Epoch: 24 Batch: 280 Loss: 0.0004347801150288433\n",
            "Epoch: 24 Batch: 300 Loss: 0.007583999540656805\n",
            "Epoch: 24 Batch: 320 Loss: 0.010351752862334251\n",
            "Epoch: 24 Batch: 340 Loss: 0.0002006530703511089\n",
            "Epoch: 24 Batch: 360 Loss: 7.43865966796875e-05\n",
            "Epoch: 24 Batch: 380 Loss: 0.01029825210571289\n",
            "Epoch: 24 Batch: 400 Loss: 0.01935863494873047\n",
            "Epoch: 24 Batch: 420 Loss: 0.004175662994384766\n",
            "Epoch: 24 Batch: 440 Loss: 0.19721662998199463\n",
            "Epoch: 24 Batch: 460 Loss: 0.0022021294571459293\n",
            "Epoch: 24 Batch: 480 Loss: 0.00792541541159153\n",
            "Epoch: 24 Batch: 500 Loss: 0.13398876786231995\n",
            "Epoch: 24 Batch: 520 Loss: 1.3401461839675903\n",
            "Epoch: 24 Batch: 540 Loss: 0.00032176970853470266\n",
            "Epoch: 24 Batch: 560 Loss: 0.004847907926887274\n",
            "Epoch: 24 Batch: 580 Loss: 0.2586010992527008\n",
            "Epoch: 24 Batch: 600 Loss: 0.0014687537914142013\n",
            "Epoch: 24 Batch: 620 Loss: 0.041648197919130325\n",
            "Epoch: 24 Batch: 640 Loss: 0.00615692138671875\n",
            "Epoch: 24 Batch: 660 Loss: 0.0025424002669751644\n",
            "Epoch: 24 Batch: 680 Loss: 0.05613422393798828\n",
            "Epoch: 24 Batch: 700 Loss: 0.00011110305786132812\n",
            "Epoch: 24 Batch: 720 Loss: 0.00018596649169921875\n",
            "Epoch: 24 Batch: 740 Loss: 1.3798694610595703\n",
            "Epoch: 24 Batch: 760 Loss: 0.3419681489467621\n",
            "Epoch: 24 Batch: 780 Loss: 0.0849694237112999\n",
            "Epoch: 24 Batch: 800 Loss: 0.00046291350736282766\n",
            "Epoch: 24 Batch: 820 Loss: 0.014664078131318092\n",
            "Epoch: 24 Batch: 840 Loss: 0.029357243329286575\n",
            "Epoch: 24 Batch: 860 Loss: 0.024557018652558327\n",
            "Epoch: 24 Batch: 880 Loss: 0.02328472211956978\n",
            "Epoch: 24 Batch: 900 Loss: 0.06663904339075089\n",
            "Epoch: 24 Batch: 920 Loss: 0.5243483185768127\n",
            "Epoch: 24 Batch: 940 Loss: 0.018070315942168236\n",
            "Epoch: 24 Batch: 960 Loss: 0.004264163784682751\n",
            "Epoch: 24 Batch: 980 Loss: 0.0050102234818041325\n",
            "Epoch: 25 Batch: 0 Loss: 0.01145019568502903\n",
            "Epoch: 25 Batch: 20 Loss: 0.006137371063232422\n",
            "Epoch: 25 Batch: 40 Loss: 1.258850079466356e-05\n",
            "Epoch: 25 Batch: 60 Loss: 0.0713191032409668\n",
            "Epoch: 25 Batch: 80 Loss: 0.001446628593839705\n",
            "Epoch: 25 Batch: 100 Loss: 0.0005708694225177169\n",
            "Epoch: 25 Batch: 120 Loss: 0.004690838046371937\n",
            "Epoch: 25 Batch: 140 Loss: 0.0037785531021654606\n",
            "Epoch: 25 Batch: 160 Loss: 0.0007291793590411544\n",
            "Epoch: 25 Batch: 180 Loss: 0.00040264130802825093\n",
            "Epoch: 25 Batch: 200 Loss: 0.018021583557128906\n",
            "Epoch: 25 Batch: 220 Loss: 0.005690288729965687\n",
            "Epoch: 25 Batch: 240 Loss: 9.574890282237902e-05\n",
            "Epoch: 25 Batch: 260 Loss: 0.0024992942344397306\n",
            "Epoch: 25 Batch: 280 Loss: 0.0015119552845135331\n",
            "Epoch: 25 Batch: 300 Loss: 0.0036662102211266756\n",
            "Epoch: 25 Batch: 320 Loss: 0.12100525200366974\n",
            "Epoch: 25 Batch: 340 Loss: 0.5134986042976379\n",
            "Epoch: 25 Batch: 360 Loss: 0.014185619540512562\n",
            "Epoch: 25 Batch: 380 Loss: 0.0026954649947583675\n",
            "Epoch: 25 Batch: 400 Loss: 0.005302620120346546\n",
            "Epoch: 25 Batch: 420 Loss: 3.662109520519152e-05\n",
            "Epoch: 25 Batch: 440 Loss: 0.32038742303848267\n",
            "Epoch: 25 Batch: 460 Loss: 0.00019207000150345266\n",
            "Epoch: 25 Batch: 480 Loss: 0.04358243942260742\n",
            "Epoch: 25 Batch: 500 Loss: 0.13190260529518127\n",
            "Epoch: 25 Batch: 520 Loss: 0.00035562514676712453\n",
            "Epoch: 25 Batch: 540 Loss: 0.0353119857609272\n",
            "Epoch: 25 Batch: 560 Loss: 0.06889839470386505\n",
            "Epoch: 25 Batch: 580 Loss: 0.012713050469756126\n",
            "Epoch: 25 Batch: 600 Loss: 0.007517910096794367\n",
            "Epoch: 25 Batch: 620 Loss: 0.03956308215856552\n",
            "Epoch: 25 Batch: 640 Loss: 0.0014848709106445312\n",
            "Epoch: 25 Batch: 660 Loss: 0.002515125321224332\n",
            "Epoch: 25 Batch: 680 Loss: 0.06321001052856445\n",
            "Epoch: 25 Batch: 700 Loss: 0.004880762193351984\n",
            "Epoch: 25 Batch: 720 Loss: 0.0001661300630075857\n",
            "Epoch: 25 Batch: 740 Loss: 0.015540885739028454\n",
            "Epoch: 25 Batch: 760 Loss: 0.036447715014219284\n",
            "Epoch: 25 Batch: 780 Loss: 0.0007888794061727822\n",
            "Epoch: 25 Batch: 800 Loss: 0.0002830505254678428\n",
            "Epoch: 25 Batch: 820 Loss: 0.0012319565284997225\n",
            "Epoch: 25 Batch: 840 Loss: 0.0005386352422647178\n",
            "Epoch: 25 Batch: 860 Loss: 0.15500840544700623\n",
            "Epoch: 25 Batch: 880 Loss: 0.0002157211274607107\n",
            "Epoch: 25 Batch: 900 Loss: 0.006286430172622204\n",
            "Epoch: 25 Batch: 920 Loss: 0.03188486024737358\n",
            "Epoch: 25 Batch: 940 Loss: 0.004744052886962891\n",
            "Epoch: 25 Batch: 960 Loss: 0.01505746878683567\n",
            "Epoch: 25 Batch: 980 Loss: 0.013760089874267578\n",
            "Epoch: 26 Batch: 0 Loss: 0.013346100226044655\n",
            "Epoch: 26 Batch: 20 Loss: 0.00043888093205168843\n",
            "Epoch: 26 Batch: 40 Loss: 0.035874079912900925\n",
            "Epoch: 26 Batch: 60 Loss: 0.011394786648452282\n",
            "Epoch: 26 Batch: 80 Loss: 0.12178721278905869\n",
            "Epoch: 26 Batch: 100 Loss: 0.07061252743005753\n",
            "Epoch: 26 Batch: 120 Loss: 0.6734317541122437\n",
            "Epoch: 26 Batch: 140 Loss: 0.14954128861427307\n",
            "Epoch: 26 Batch: 160 Loss: 0.01658611372113228\n",
            "Epoch: 26 Batch: 180 Loss: 0.2232324630022049\n",
            "Epoch: 26 Batch: 200 Loss: 0.4148663580417633\n",
            "Epoch: 26 Batch: 220 Loss: 0.000240325927734375\n",
            "Epoch: 26 Batch: 240 Loss: 0.09310932457447052\n",
            "Epoch: 26 Batch: 260 Loss: 0.005948829464614391\n",
            "Epoch: 26 Batch: 280 Loss: 0.013111782260239124\n",
            "Epoch: 26 Batch: 300 Loss: 0.00046749116154387593\n",
            "Epoch: 26 Batch: 320 Loss: 0.07738018035888672\n",
            "Epoch: 26 Batch: 340 Loss: 0.02822427824139595\n",
            "Epoch: 26 Batch: 360 Loss: 0.003257656004279852\n",
            "Epoch: 26 Batch: 380 Loss: 0.0014170646900311112\n",
            "Epoch: 26 Batch: 400 Loss: 0.00014419555373024195\n",
            "Epoch: 26 Batch: 420 Loss: 0.12867555022239685\n",
            "Epoch: 26 Batch: 440 Loss: 2.231597864010837e-05\n",
            "Epoch: 26 Batch: 460 Loss: 0.07089457660913467\n",
            "Epoch: 26 Batch: 480 Loss: 0.03532733768224716\n",
            "Epoch: 26 Batch: 500 Loss: 0.013778304681181908\n",
            "Epoch: 26 Batch: 520 Loss: 0.001986599061638117\n",
            "Epoch: 26 Batch: 540 Loss: 0.006981277372688055\n",
            "Epoch: 26 Batch: 560 Loss: 0.007025241851806641\n",
            "Epoch: 26 Batch: 580 Loss: 0.0001352310209767893\n",
            "Epoch: 26 Batch: 600 Loss: 0.005146312527358532\n",
            "Epoch: 26 Batch: 620 Loss: 0.0026962279807776213\n",
            "Epoch: 26 Batch: 640 Loss: 0.6782799959182739\n",
            "Epoch: 26 Batch: 660 Loss: 0.0004973411560058594\n",
            "Epoch: 26 Batch: 680 Loss: 0.001951503800228238\n",
            "Epoch: 26 Batch: 700 Loss: 0.0868992805480957\n",
            "Epoch: 26 Batch: 720 Loss: 0.0019439697498455644\n",
            "Epoch: 26 Batch: 740 Loss: 0.024993229657411575\n",
            "Epoch: 26 Batch: 760 Loss: 0.0016460418701171875\n",
            "Epoch: 26 Batch: 780 Loss: 8.468628220725805e-05\n",
            "Epoch: 26 Batch: 800 Loss: 0.003986549563705921\n",
            "Epoch: 26 Batch: 820 Loss: 0.024191951379179955\n",
            "Epoch: 26 Batch: 840 Loss: 0.008254051208496094\n",
            "Epoch: 26 Batch: 860 Loss: 0.0014011382590979338\n",
            "Epoch: 26 Batch: 880 Loss: 0.005261135287582874\n",
            "Epoch: 26 Batch: 900 Loss: 0.08586730808019638\n",
            "Epoch: 26 Batch: 920 Loss: 0.00022945404634810984\n",
            "Epoch: 26 Batch: 940 Loss: 0.17892351746559143\n",
            "Epoch: 26 Batch: 960 Loss: 0.10864200443029404\n",
            "Epoch: 26 Batch: 980 Loss: 0.005780220031738281\n",
            "Epoch: 27 Batch: 0 Loss: 0.03686494752764702\n",
            "Epoch: 27 Batch: 20 Loss: 0.00019359588623046875\n",
            "Epoch: 27 Batch: 40 Loss: 0.0011259078746661544\n",
            "Epoch: 27 Batch: 60 Loss: 0.0037734031211584806\n",
            "Epoch: 27 Batch: 80 Loss: 0.016457080841064453\n",
            "Epoch: 27 Batch: 100 Loss: 0.015384674072265625\n",
            "Epoch: 27 Batch: 120 Loss: 0.002444648649543524\n",
            "Epoch: 27 Batch: 140 Loss: 0.007501697633415461\n",
            "Epoch: 27 Batch: 160 Loss: 0.00298728933557868\n",
            "Epoch: 27 Batch: 180 Loss: 0.0014171600341796875\n",
            "Epoch: 27 Batch: 200 Loss: 0.009646987542510033\n",
            "Epoch: 27 Batch: 220 Loss: 0.0007213592762127519\n",
            "Epoch: 27 Batch: 240 Loss: 0.002748775528743863\n",
            "Epoch: 27 Batch: 260 Loss: 0.0024628639221191406\n",
            "Epoch: 27 Batch: 280 Loss: 6.67572021484375e-06\n",
            "Epoch: 27 Batch: 300 Loss: 0.0005174636607989669\n",
            "Epoch: 27 Batch: 320 Loss: 0.02915506437420845\n",
            "Epoch: 27 Batch: 340 Loss: 4.062652442371473e-05\n",
            "Epoch: 27 Batch: 360 Loss: 0.0044387816451489925\n",
            "Epoch: 27 Batch: 380 Loss: 0.09092869609594345\n",
            "Epoch: 27 Batch: 400 Loss: 0.10162553936243057\n",
            "Epoch: 27 Batch: 420 Loss: 0.00019512177095748484\n",
            "Epoch: 27 Batch: 440 Loss: 0.07616474479436874\n",
            "Epoch: 27 Batch: 460 Loss: 0.003918266389518976\n",
            "Epoch: 27 Batch: 480 Loss: 0.004613018129020929\n",
            "Epoch: 27 Batch: 500 Loss: 7.629394644936838e-07\n",
            "Epoch: 27 Batch: 520 Loss: 0.0037870407104492188\n",
            "Epoch: 27 Batch: 540 Loss: 0.014245033264160156\n",
            "Epoch: 27 Batch: 560 Loss: 0.0018731117015704513\n",
            "Epoch: 27 Batch: 580 Loss: 0.008281325921416283\n",
            "Epoch: 27 Batch: 600 Loss: 0.0002307891845703125\n",
            "Epoch: 27 Batch: 620 Loss: 0.26581665873527527\n",
            "Epoch: 27 Batch: 640 Loss: 0.09320459514856339\n",
            "Epoch: 27 Batch: 660 Loss: 0.5337910056114197\n",
            "Epoch: 27 Batch: 680 Loss: 0.04934721067547798\n",
            "Epoch: 27 Batch: 700 Loss: 0.6207925081253052\n",
            "Epoch: 27 Batch: 720 Loss: 0.026365280151367188\n",
            "Epoch: 27 Batch: 740 Loss: 0.008804703131318092\n",
            "Epoch: 27 Batch: 760 Loss: 0.0015451430808752775\n",
            "Epoch: 27 Batch: 780 Loss: 0.01685037650167942\n",
            "Epoch: 27 Batch: 800 Loss: 0.1539168804883957\n",
            "Epoch: 27 Batch: 820 Loss: 0.036568544805049896\n",
            "Epoch: 27 Batch: 840 Loss: 0.341045618057251\n",
            "Epoch: 27 Batch: 860 Loss: 9.34600848268019e-06\n",
            "Epoch: 27 Batch: 880 Loss: 0.019646549597382545\n",
            "Epoch: 27 Batch: 900 Loss: 0.8300625085830688\n",
            "Epoch: 27 Batch: 920 Loss: 3.852844383800402e-05\n",
            "Epoch: 27 Batch: 940 Loss: 0.001840782118961215\n",
            "Epoch: 27 Batch: 960 Loss: 0.005936813540756702\n",
            "Epoch: 27 Batch: 980 Loss: 0.0014547348255291581\n",
            "Epoch: 28 Batch: 0 Loss: 0.012637901119887829\n",
            "Epoch: 28 Batch: 20 Loss: 0.0036643981002271175\n",
            "Epoch: 28 Batch: 40 Loss: 0.0015391350025311112\n",
            "Epoch: 28 Batch: 60 Loss: 0.08063745498657227\n",
            "Epoch: 28 Batch: 80 Loss: 0.005262183956801891\n",
            "Epoch: 28 Batch: 100 Loss: 0.02253742143511772\n",
            "Epoch: 28 Batch: 120 Loss: 0.0023653029929846525\n",
            "Epoch: 28 Batch: 140 Loss: 0.0023399353958666325\n",
            "Epoch: 28 Batch: 160 Loss: 0.003092479659244418\n",
            "Epoch: 28 Batch: 180 Loss: 0.02181272581219673\n",
            "Epoch: 28 Batch: 200 Loss: 0.03900594636797905\n",
            "Epoch: 28 Batch: 220 Loss: 5.3119660151423886e-05\n",
            "Epoch: 28 Batch: 240 Loss: 0.00012149810936534777\n",
            "Epoch: 28 Batch: 260 Loss: 1.120035171508789\n",
            "Epoch: 28 Batch: 280 Loss: 0.00021800995455123484\n",
            "Epoch: 28 Batch: 300 Loss: 0.007079601287841797\n",
            "Epoch: 28 Batch: 320 Loss: 4.158019874012098e-05\n",
            "Epoch: 28 Batch: 340 Loss: 0.0003490447998046875\n",
            "Epoch: 28 Batch: 360 Loss: 0.12048473209142685\n",
            "Epoch: 28 Batch: 380 Loss: 0.0006055831909179688\n",
            "Epoch: 28 Batch: 400 Loss: 0.0001489639253122732\n",
            "Epoch: 28 Batch: 420 Loss: 0.005348110105842352\n",
            "Epoch: 28 Batch: 440 Loss: 0.023560428991913795\n",
            "Epoch: 28 Batch: 460 Loss: 0.0026370049454271793\n",
            "Epoch: 28 Batch: 480 Loss: 0.1273597776889801\n",
            "Epoch: 28 Batch: 500 Loss: 0.011553669348359108\n",
            "Epoch: 28 Batch: 520 Loss: 0.10712628066539764\n",
            "Epoch: 28 Batch: 540 Loss: 0.0021695136092603207\n",
            "Epoch: 28 Batch: 560 Loss: 0.00258216867223382\n",
            "Epoch: 28 Batch: 580 Loss: 0.13988928496837616\n",
            "Epoch: 28 Batch: 600 Loss: 0.01180200558155775\n",
            "Epoch: 28 Batch: 620 Loss: 0.0028171539306640625\n",
            "Epoch: 28 Batch: 640 Loss: 2.174377368646674e-05\n",
            "Epoch: 28 Batch: 660 Loss: 0.001247406005859375\n",
            "Epoch: 28 Batch: 680 Loss: 0.005386543460190296\n",
            "Epoch: 28 Batch: 700 Loss: 0.10120048373937607\n",
            "Epoch: 28 Batch: 720 Loss: 0.012104702182114124\n",
            "Epoch: 28 Batch: 740 Loss: 0.0007640838739462197\n",
            "Epoch: 28 Batch: 760 Loss: 0.0005167961353436112\n",
            "Epoch: 28 Batch: 780 Loss: 0.0023258209694176912\n",
            "Epoch: 28 Batch: 800 Loss: 0.3075169026851654\n",
            "Epoch: 28 Batch: 820 Loss: 0.0012497901916503906\n",
            "Epoch: 28 Batch: 840 Loss: 0.0008590698125772178\n",
            "Epoch: 28 Batch: 860 Loss: 0.0009077072027139366\n",
            "Epoch: 28 Batch: 880 Loss: 0.0001695632963674143\n",
            "Epoch: 28 Batch: 900 Loss: 0.02109346352517605\n",
            "Epoch: 28 Batch: 920 Loss: 0.008704185485839844\n",
            "Epoch: 28 Batch: 940 Loss: 0.001096439315006137\n",
            "Epoch: 28 Batch: 960 Loss: 0.00229301443323493\n",
            "Epoch: 28 Batch: 980 Loss: 9.35554489842616e-05\n",
            "Epoch: 29 Batch: 0 Loss: 0.0004425048828125\n",
            "Epoch: 29 Batch: 20 Loss: 0.27858948707580566\n",
            "Epoch: 29 Batch: 40 Loss: 0.0016034126747399569\n",
            "Epoch: 29 Batch: 60 Loss: 0.0020270347595214844\n",
            "Epoch: 29 Batch: 80 Loss: 0.05564069747924805\n",
            "Epoch: 29 Batch: 100 Loss: 0.03918399661779404\n",
            "Epoch: 29 Batch: 120 Loss: 0.0016237258678302169\n",
            "Epoch: 29 Batch: 140 Loss: 0.004281806759536266\n",
            "Epoch: 29 Batch: 160 Loss: 0.11065330356359482\n",
            "Epoch: 29 Batch: 180 Loss: 0.022989368066191673\n",
            "Epoch: 29 Batch: 200 Loss: 0.03547639772295952\n",
            "Epoch: 29 Batch: 220 Loss: 0.006751537322998047\n",
            "Epoch: 29 Batch: 240 Loss: 0.0005562782171182334\n",
            "Epoch: 29 Batch: 260 Loss: 0.003226852510124445\n",
            "Epoch: 29 Batch: 280 Loss: 0.0018686294788494706\n",
            "Epoch: 29 Batch: 300 Loss: 0.022429943084716797\n",
            "Epoch: 29 Batch: 320 Loss: 0.00020141601271461695\n",
            "Epoch: 29 Batch: 340 Loss: 0.27944451570510864\n",
            "Epoch: 29 Batch: 360 Loss: 0.002702903700992465\n",
            "Epoch: 29 Batch: 380 Loss: 0.009601974859833717\n",
            "Epoch: 29 Batch: 400 Loss: 0.009484386071562767\n",
            "Epoch: 29 Batch: 420 Loss: 0.00903863925486803\n",
            "Epoch: 29 Batch: 440 Loss: 0.20928630232810974\n",
            "Epoch: 29 Batch: 460 Loss: 0.01154546719044447\n",
            "Epoch: 29 Batch: 480 Loss: 0.0001104354887502268\n",
            "Epoch: 29 Batch: 500 Loss: 0.013250636868178844\n",
            "Epoch: 29 Batch: 520 Loss: 0.002716159913688898\n",
            "Epoch: 29 Batch: 540 Loss: 0.03427696228027344\n",
            "Epoch: 29 Batch: 560 Loss: 9.803772263694555e-05\n",
            "Epoch: 29 Batch: 580 Loss: 0.043964385986328125\n",
            "Epoch: 29 Batch: 600 Loss: 0.0013353347312659025\n",
            "Epoch: 29 Batch: 620 Loss: 0.05461721494793892\n",
            "Epoch: 29 Batch: 640 Loss: 0.000781154609285295\n",
            "Epoch: 29 Batch: 660 Loss: 0.0019053459400311112\n",
            "Epoch: 29 Batch: 680 Loss: 8.678436279296875e-05\n",
            "Epoch: 29 Batch: 700 Loss: 3.280639793956652e-05\n",
            "Epoch: 29 Batch: 720 Loss: 0.01807260513305664\n",
            "Epoch: 29 Batch: 740 Loss: 0.0026943206321448088\n",
            "Epoch: 29 Batch: 760 Loss: 0.3777078092098236\n",
            "Epoch: 29 Batch: 780 Loss: 0.00044612883357331157\n",
            "Epoch: 29 Batch: 800 Loss: 0.10824403911828995\n",
            "Epoch: 29 Batch: 820 Loss: 0.00010328293137717992\n",
            "Epoch: 29 Batch: 840 Loss: 0.0018008232582360506\n",
            "Epoch: 29 Batch: 860 Loss: 0.0001815795840229839\n",
            "Epoch: 29 Batch: 880 Loss: 0.2535727024078369\n",
            "Epoch: 29 Batch: 900 Loss: 1.133617639541626\n",
            "Epoch: 29 Batch: 920 Loss: 0.011592483147978783\n",
            "Epoch: 29 Batch: 940 Loss: 0.00531692523509264\n",
            "Epoch: 29 Batch: 960 Loss: 0.30362480878829956\n",
            "Epoch: 29 Batch: 980 Loss: 0.13369865715503693\n",
            "Epoch: 30 Batch: 0 Loss: 0.10717830806970596\n",
            "Epoch: 30 Batch: 20 Loss: 0.0100555419921875\n",
            "Epoch: 30 Batch: 40 Loss: 0.002408313797786832\n",
            "Epoch: 30 Batch: 60 Loss: 0.0004417419550009072\n",
            "Epoch: 30 Batch: 80 Loss: 0.009485721588134766\n",
            "Epoch: 30 Batch: 100 Loss: 0.00038909912109375\n",
            "Epoch: 30 Batch: 120 Loss: 0.0004120826779399067\n",
            "Epoch: 30 Batch: 140 Loss: 0.016374778002500534\n",
            "Epoch: 30 Batch: 160 Loss: 0.2410295009613037\n",
            "Epoch: 30 Batch: 180 Loss: 0.0054724691435694695\n",
            "Epoch: 30 Batch: 200 Loss: 0.00039882661076262593\n",
            "Epoch: 30 Batch: 220 Loss: 0.02944054640829563\n",
            "Epoch: 30 Batch: 240 Loss: 0.0014677047729492188\n",
            "Epoch: 30 Batch: 260 Loss: 0.00011863708641612902\n",
            "Epoch: 30 Batch: 280 Loss: 0.0004127502324990928\n",
            "Epoch: 30 Batch: 300 Loss: 0.004578876309096813\n",
            "Epoch: 30 Batch: 320 Loss: 0.025565242394804955\n",
            "Epoch: 30 Batch: 340 Loss: 0.00036535263643600047\n",
            "Epoch: 30 Batch: 360 Loss: 0.7345792055130005\n",
            "Epoch: 30 Batch: 380 Loss: 0.002129077911376953\n",
            "Epoch: 30 Batch: 400 Loss: 0.5579302906990051\n",
            "Epoch: 30 Batch: 420 Loss: 0.00033845900907181203\n",
            "Epoch: 30 Batch: 440 Loss: 0.0026001930236816406\n",
            "Epoch: 30 Batch: 460 Loss: 0.03571515157818794\n",
            "Epoch: 30 Batch: 480 Loss: 0.012146377936005592\n",
            "Epoch: 30 Batch: 500 Loss: 0.0007137298816815019\n",
            "Epoch: 30 Batch: 520 Loss: 0.003277683164924383\n",
            "Epoch: 30 Batch: 540 Loss: 0.0022422790061682463\n",
            "Epoch: 30 Batch: 560 Loss: 0.04919233173131943\n",
            "Epoch: 30 Batch: 580 Loss: 0.045255087316036224\n",
            "Epoch: 30 Batch: 600 Loss: 0.00268726353533566\n",
            "Epoch: 30 Batch: 620 Loss: 0.00816421490162611\n",
            "Epoch: 30 Batch: 640 Loss: 0.002948284149169922\n",
            "Epoch: 30 Batch: 660 Loss: 0.000735378242097795\n",
            "Epoch: 30 Batch: 680 Loss: 0.0036038397811353207\n",
            "Epoch: 30 Batch: 700 Loss: 9.72747802734375e-05\n",
            "Epoch: 30 Batch: 720 Loss: 0.0010153769981116056\n",
            "Epoch: 30 Batch: 740 Loss: 0.010814666748046875\n",
            "Epoch: 30 Batch: 760 Loss: 0.024347925558686256\n",
            "Epoch: 30 Batch: 780 Loss: 0.0034173012245446444\n",
            "Epoch: 30 Batch: 800 Loss: 0.005516910459846258\n",
            "Epoch: 30 Batch: 820 Loss: 0.2290448248386383\n",
            "Epoch: 30 Batch: 840 Loss: 0.0011508942116051912\n",
            "Epoch: 30 Batch: 860 Loss: 0.00026950836763717234\n",
            "Epoch: 30 Batch: 880 Loss: 0.0028285980224609375\n",
            "Epoch: 30 Batch: 900 Loss: 0.09265394508838654\n",
            "Epoch: 30 Batch: 920 Loss: 0.0004267692565917969\n",
            "Epoch: 30 Batch: 940 Loss: 0.002931785536929965\n",
            "Epoch: 30 Batch: 960 Loss: 0.0027109147049486637\n",
            "Epoch: 30 Batch: 980 Loss: 0.00031785963801667094\n",
            "Epoch: 31 Batch: 0 Loss: 0.0005958557012490928\n",
            "Epoch: 31 Batch: 20 Loss: 0.0005929946782998741\n",
            "Epoch: 31 Batch: 40 Loss: 0.00011157989501953125\n",
            "Epoch: 31 Batch: 60 Loss: 0.0009544372442178428\n",
            "Epoch: 31 Batch: 80 Loss: 0.0005052566411904991\n",
            "Epoch: 31 Batch: 100 Loss: 0.027721595019102097\n",
            "Epoch: 31 Batch: 120 Loss: 0.0012065886985510588\n",
            "Epoch: 31 Batch: 140 Loss: 0.00043849943904206157\n",
            "Epoch: 31 Batch: 160 Loss: 0.0011252403492107987\n",
            "Epoch: 31 Batch: 180 Loss: 0.0006967544322833419\n",
            "Epoch: 31 Batch: 200 Loss: 0.008327102288603783\n",
            "Epoch: 31 Batch: 220 Loss: 0.004129314329475164\n",
            "Epoch: 31 Batch: 240 Loss: 0.07626257091760635\n",
            "Epoch: 31 Batch: 260 Loss: 0.0010921477805823088\n",
            "Epoch: 31 Batch: 280 Loss: 0.0005380630609579384\n",
            "Epoch: 31 Batch: 300 Loss: 0.014445686712861061\n",
            "Epoch: 31 Batch: 320 Loss: 0.0004826545773539692\n",
            "Epoch: 31 Batch: 340 Loss: 0.009673500433564186\n",
            "Epoch: 31 Batch: 360 Loss: 0.48100709915161133\n",
            "Epoch: 31 Batch: 380 Loss: 0.004281044006347656\n",
            "Epoch: 31 Batch: 400 Loss: 0.0996282547712326\n",
            "Epoch: 31 Batch: 420 Loss: 0.030060147866606712\n",
            "Epoch: 31 Batch: 440 Loss: 0.12065792083740234\n",
            "Epoch: 31 Batch: 460 Loss: 5.817413330078125e-05\n",
            "Epoch: 31 Batch: 480 Loss: 0.020704936236143112\n",
            "Epoch: 31 Batch: 500 Loss: 0.00022726059250999242\n",
            "Epoch: 31 Batch: 520 Loss: 0.010393237695097923\n",
            "Epoch: 31 Batch: 540 Loss: 0.005224800202995539\n",
            "Epoch: 31 Batch: 560 Loss: 5.550384594243951e-05\n",
            "Epoch: 31 Batch: 580 Loss: 0.0001050949067575857\n",
            "Epoch: 31 Batch: 600 Loss: 0.013028335757553577\n",
            "Epoch: 31 Batch: 620 Loss: 1.8978118532686494e-05\n",
            "Epoch: 31 Batch: 640 Loss: 0.05656271055340767\n",
            "Epoch: 31 Batch: 660 Loss: 0.016681289300322533\n",
            "Epoch: 31 Batch: 680 Loss: 1.468658410885837e-05\n",
            "Epoch: 31 Batch: 700 Loss: 0.017279338091611862\n",
            "Epoch: 31 Batch: 720 Loss: 0.0022382736206054688\n",
            "Epoch: 31 Batch: 740 Loss: 9.794234938453883e-05\n",
            "Epoch: 31 Batch: 760 Loss: 3.719329833984375e-05\n",
            "Epoch: 31 Batch: 780 Loss: 0.0005119323614053428\n",
            "Epoch: 31 Batch: 800 Loss: 0.00012073516700183973\n",
            "Epoch: 31 Batch: 820 Loss: 0.028790760785341263\n",
            "Epoch: 31 Batch: 840 Loss: 0.013171386905014515\n",
            "Epoch: 31 Batch: 860 Loss: 0.0489443764090538\n",
            "Epoch: 31 Batch: 880 Loss: 0.0015873908996582031\n",
            "Epoch: 31 Batch: 900 Loss: 0.0001867294340627268\n",
            "Epoch: 31 Batch: 920 Loss: 0.001054954482242465\n",
            "Epoch: 31 Batch: 940 Loss: 0.011216163635253906\n",
            "Epoch: 31 Batch: 960 Loss: 0.04209404066205025\n",
            "Epoch: 31 Batch: 980 Loss: 0.07667970657348633\n",
            "Epoch: 32 Batch: 0 Loss: 4.043579247081652e-05\n",
            "Epoch: 32 Batch: 20 Loss: 0.0016320229042321444\n",
            "Epoch: 32 Batch: 40 Loss: 0.0017883300315588713\n",
            "Epoch: 32 Batch: 60 Loss: 0.03504962846636772\n",
            "Epoch: 32 Batch: 80 Loss: 0.002709293272346258\n",
            "Epoch: 32 Batch: 100 Loss: 0.0025471686385571957\n",
            "Epoch: 32 Batch: 120 Loss: 0.0027462958823889494\n",
            "Epoch: 32 Batch: 140 Loss: 0.020264720544219017\n",
            "Epoch: 32 Batch: 160 Loss: 0.00023775100999046117\n",
            "Epoch: 32 Batch: 180 Loss: 0.7704232931137085\n",
            "Epoch: 32 Batch: 200 Loss: 0.0005571365472860634\n",
            "Epoch: 32 Batch: 220 Loss: 0.003920936491340399\n",
            "Epoch: 32 Batch: 240 Loss: 0.9797893762588501\n",
            "Epoch: 32 Batch: 260 Loss: 0.18061503767967224\n",
            "Epoch: 32 Batch: 280 Loss: 0.0009395599481649697\n",
            "Epoch: 32 Batch: 300 Loss: 0.47361820936203003\n",
            "Epoch: 32 Batch: 320 Loss: 0.12229128181934357\n",
            "Epoch: 32 Batch: 340 Loss: 0.10236310958862305\n",
            "Epoch: 32 Batch: 360 Loss: 0.03462381288409233\n",
            "Epoch: 32 Batch: 380 Loss: 0.0021571158431470394\n",
            "Epoch: 32 Batch: 400 Loss: 0.0048982142470777035\n",
            "Epoch: 32 Batch: 420 Loss: 0.001161861466243863\n",
            "Epoch: 32 Batch: 440 Loss: 0.002275657607242465\n",
            "Epoch: 32 Batch: 460 Loss: 0.11950421333312988\n",
            "Epoch: 32 Batch: 480 Loss: 0.0014785766834393144\n",
            "Epoch: 32 Batch: 500 Loss: 0.0037212371826171875\n",
            "Epoch: 32 Batch: 520 Loss: 0.005167865660041571\n",
            "Epoch: 32 Batch: 540 Loss: 0.0012069701915606856\n",
            "Epoch: 32 Batch: 560 Loss: 0.004844474606215954\n",
            "Epoch: 32 Batch: 580 Loss: 0.0012861251598224044\n",
            "Epoch: 32 Batch: 600 Loss: 0.00018930435180664062\n",
            "Epoch: 32 Batch: 620 Loss: 0.019304942339658737\n",
            "Epoch: 32 Batch: 640 Loss: 0.0030754089821130037\n",
            "Epoch: 32 Batch: 660 Loss: 0.05170898512005806\n",
            "Epoch: 32 Batch: 680 Loss: 0.006022548768669367\n",
            "Epoch: 32 Batch: 700 Loss: 5.683898780262098e-05\n",
            "Epoch: 32 Batch: 720 Loss: 0.00024623872013762593\n",
            "Epoch: 32 Batch: 740 Loss: 3.06129441014491e-05\n",
            "Epoch: 32 Batch: 760 Loss: 0.00026388169499114156\n",
            "Epoch: 32 Batch: 780 Loss: 0.5567388534545898\n",
            "Epoch: 32 Batch: 800 Loss: 0.0003028869687113911\n",
            "Epoch: 32 Batch: 820 Loss: 0.11607003211975098\n",
            "Epoch: 32 Batch: 840 Loss: 0.01666564866900444\n",
            "Epoch: 32 Batch: 860 Loss: 0.00035228728665970266\n",
            "Epoch: 32 Batch: 880 Loss: 2.899169885495212e-05\n",
            "Epoch: 32 Batch: 900 Loss: 0.002544879913330078\n",
            "Epoch: 32 Batch: 920 Loss: 0.0009656905895099044\n",
            "Epoch: 32 Batch: 940 Loss: 0.0009401321294717491\n",
            "Epoch: 32 Batch: 960 Loss: 0.21811790764331818\n",
            "Epoch: 32 Batch: 980 Loss: 0.2614038586616516\n",
            "Epoch: 33 Batch: 0 Loss: 0.002326488494873047\n",
            "Epoch: 33 Batch: 20 Loss: 0.0023428916465491056\n",
            "Epoch: 33 Batch: 40 Loss: 0.011115455999970436\n",
            "Epoch: 33 Batch: 60 Loss: 0.11682729423046112\n",
            "Epoch: 33 Batch: 80 Loss: 0.006062698550522327\n",
            "Epoch: 33 Batch: 100 Loss: 1.9073486328125e-06\n",
            "Epoch: 33 Batch: 120 Loss: 0.00042133330134674907\n",
            "Epoch: 33 Batch: 140 Loss: 0.00021028518676757812\n",
            "Epoch: 33 Batch: 160 Loss: 0.025690650567412376\n",
            "Epoch: 33 Batch: 180 Loss: 8.115768287098035e-05\n",
            "Epoch: 33 Batch: 200 Loss: 0.0001773834228515625\n",
            "Epoch: 33 Batch: 220 Loss: 0.00048274995060637593\n",
            "Epoch: 33 Batch: 240 Loss: 0.0018634796142578125\n",
            "Epoch: 33 Batch: 260 Loss: 0.012398337945342064\n",
            "Epoch: 33 Batch: 280 Loss: 0.03791246563196182\n",
            "Epoch: 33 Batch: 300 Loss: 3.299712989246473e-05\n",
            "Epoch: 33 Batch: 320 Loss: 0.0006863594171591103\n",
            "Epoch: 33 Batch: 340 Loss: 0.006836986634880304\n",
            "Epoch: 33 Batch: 360 Loss: 0.47469988465309143\n",
            "Epoch: 33 Batch: 380 Loss: 0.020530128851532936\n",
            "Epoch: 33 Batch: 400 Loss: 0.058316897600889206\n",
            "Epoch: 33 Batch: 420 Loss: 0.018307019025087357\n",
            "Epoch: 33 Batch: 440 Loss: 0.0009618758922442794\n",
            "Epoch: 33 Batch: 460 Loss: 0.18289422988891602\n",
            "Epoch: 33 Batch: 480 Loss: 0.010213756933808327\n",
            "Epoch: 33 Batch: 500 Loss: 0.0023191452492028475\n",
            "Epoch: 33 Batch: 520 Loss: 0.011263561435043812\n",
            "Epoch: 33 Batch: 540 Loss: 0.015226935967803001\n",
            "Epoch: 33 Batch: 560 Loss: 0.057817935943603516\n",
            "Epoch: 33 Batch: 580 Loss: 0.015056610107421875\n",
            "Epoch: 33 Batch: 600 Loss: 0.0001066207914846018\n",
            "Epoch: 33 Batch: 620 Loss: 0.00016860962205100805\n",
            "Epoch: 33 Batch: 640 Loss: 0.003120803739875555\n",
            "Epoch: 33 Batch: 660 Loss: 0.0007946014520712197\n",
            "Epoch: 33 Batch: 680 Loss: 0.05981254577636719\n",
            "Epoch: 33 Batch: 700 Loss: 0.0022111893631517887\n",
            "Epoch: 33 Batch: 720 Loss: 0.003456020262092352\n",
            "Epoch: 33 Batch: 740 Loss: 0.0005653381231240928\n",
            "Epoch: 33 Batch: 760 Loss: 0.0001604080171091482\n",
            "Epoch: 33 Batch: 780 Loss: 0.08726463466882706\n",
            "Epoch: 33 Batch: 800 Loss: 0.0005223274347372353\n",
            "Epoch: 33 Batch: 820 Loss: 0.00036487579927779734\n",
            "Epoch: 33 Batch: 840 Loss: 0.0013585090637207031\n",
            "Epoch: 33 Batch: 860 Loss: 0.07508983463048935\n",
            "Epoch: 33 Batch: 880 Loss: 0.005634975619614124\n",
            "Epoch: 33 Batch: 900 Loss: 0.0008017540094442666\n",
            "Epoch: 33 Batch: 920 Loss: 0.11136622726917267\n",
            "Epoch: 33 Batch: 940 Loss: 0.04360561445355415\n",
            "Epoch: 33 Batch: 960 Loss: 0.004149436950683594\n",
            "Epoch: 33 Batch: 980 Loss: 0.2539570927619934\n",
            "Epoch: 34 Batch: 0 Loss: 0.0016701698768883944\n",
            "Epoch: 34 Batch: 20 Loss: 0.0025090216659009457\n",
            "Epoch: 34 Batch: 40 Loss: 1.831054760259576e-05\n",
            "Epoch: 34 Batch: 60 Loss: 0.012530517764389515\n",
            "Epoch: 34 Batch: 80 Loss: 0.00028162001399323344\n",
            "Epoch: 34 Batch: 100 Loss: 0.00131311418954283\n",
            "Epoch: 34 Batch: 120 Loss: 0.001125335693359375\n",
            "Epoch: 34 Batch: 140 Loss: 0.0005511284107342362\n",
            "Epoch: 34 Batch: 160 Loss: 0.00159540178719908\n",
            "Epoch: 34 Batch: 180 Loss: 0.09939050674438477\n",
            "Epoch: 34 Batch: 200 Loss: 0.006720542907714844\n",
            "Epoch: 34 Batch: 220 Loss: 0.005339145660400391\n",
            "Epoch: 34 Batch: 240 Loss: 0.0009224891546182334\n",
            "Epoch: 34 Batch: 260 Loss: 3.280639793956652e-05\n",
            "Epoch: 34 Batch: 280 Loss: 0.17404012382030487\n",
            "Epoch: 34 Batch: 300 Loss: 0.0018733025062829256\n",
            "Epoch: 34 Batch: 320 Loss: 0.0024421692360192537\n",
            "Epoch: 34 Batch: 340 Loss: 0.0007761001470498741\n",
            "Epoch: 34 Batch: 360 Loss: 0.001104164170101285\n",
            "Epoch: 34 Batch: 380 Loss: 0.07140465080738068\n",
            "Epoch: 34 Batch: 400 Loss: 0.9497295618057251\n",
            "Epoch: 34 Batch: 420 Loss: 0.12798404693603516\n",
            "Epoch: 34 Batch: 440 Loss: 0.05089578777551651\n",
            "Epoch: 34 Batch: 460 Loss: 0.002872371580451727\n",
            "Epoch: 34 Batch: 480 Loss: 0.0006825447198934853\n",
            "Epoch: 34 Batch: 500 Loss: 0.00023155212693382055\n",
            "Epoch: 34 Batch: 520 Loss: 0.0005107879405841231\n",
            "Epoch: 34 Batch: 540 Loss: 0.003514576004818082\n",
            "Epoch: 34 Batch: 560 Loss: 0.0010211945045739412\n",
            "Epoch: 34 Batch: 580 Loss: 0.0009959221351891756\n",
            "Epoch: 34 Batch: 600 Loss: 0.011049079708755016\n",
            "Epoch: 34 Batch: 620 Loss: 0.0024499893188476562\n",
            "Epoch: 34 Batch: 640 Loss: 0.0038573264610022306\n",
            "Epoch: 34 Batch: 660 Loss: 0.009683704003691673\n",
            "Epoch: 34 Batch: 680 Loss: 0.029464054852724075\n",
            "Epoch: 34 Batch: 700 Loss: 0.01120977383106947\n",
            "Epoch: 34 Batch: 720 Loss: 0.011170292273163795\n",
            "Epoch: 34 Batch: 740 Loss: 0.6512676477432251\n",
            "Epoch: 34 Batch: 760 Loss: 0.03841076046228409\n",
            "Epoch: 34 Batch: 780 Loss: 0.013095760717988014\n",
            "Epoch: 34 Batch: 800 Loss: 0.03859739378094673\n",
            "Epoch: 34 Batch: 820 Loss: 0.21159705519676208\n",
            "Epoch: 34 Batch: 840 Loss: 0.0635344535112381\n",
            "Epoch: 34 Batch: 860 Loss: 0.009425019845366478\n",
            "Epoch: 34 Batch: 880 Loss: 0.10352373123168945\n",
            "Epoch: 34 Batch: 900 Loss: 0.0035521506797522306\n",
            "Epoch: 34 Batch: 920 Loss: 0.01074748020619154\n",
            "Epoch: 34 Batch: 940 Loss: 0.00015697479830123484\n",
            "Epoch: 34 Batch: 960 Loss: 0.0011466980213299394\n",
            "Epoch: 34 Batch: 980 Loss: 0.023128699511289597\n",
            "Epoch: 35 Batch: 0 Loss: 0.00028247834416106343\n",
            "Epoch: 35 Batch: 20 Loss: 0.002984714461490512\n",
            "Epoch: 35 Batch: 40 Loss: 0.002195644425228238\n",
            "Epoch: 35 Batch: 60 Loss: 0.07626018673181534\n",
            "Epoch: 35 Batch: 80 Loss: 0.11741767078638077\n",
            "Epoch: 35 Batch: 100 Loss: 0.3314668536186218\n",
            "Epoch: 35 Batch: 120 Loss: 0.005360507871955633\n",
            "Epoch: 35 Batch: 140 Loss: 0.005338907241821289\n",
            "Epoch: 35 Batch: 160 Loss: 0.0003124237118754536\n",
            "Epoch: 35 Batch: 180 Loss: 0.01638183556497097\n",
            "Epoch: 35 Batch: 200 Loss: 0.0019231796031817794\n",
            "Epoch: 35 Batch: 220 Loss: 0.00015888214693404734\n",
            "Epoch: 35 Batch: 240 Loss: 0.005989647004753351\n",
            "Epoch: 35 Batch: 260 Loss: 0.0011726379161700606\n",
            "Epoch: 35 Batch: 280 Loss: 0.0011440276866778731\n",
            "Epoch: 35 Batch: 300 Loss: 0.0011030196910724044\n",
            "Epoch: 35 Batch: 320 Loss: 0.26716452836990356\n",
            "Epoch: 35 Batch: 340 Loss: 0.018960094079375267\n",
            "Epoch: 35 Batch: 360 Loss: 0.03526105731725693\n",
            "Epoch: 35 Batch: 380 Loss: 0.00959930382668972\n",
            "Epoch: 35 Batch: 400 Loss: 0.0015901565784588456\n",
            "Epoch: 35 Batch: 420 Loss: 0.014653491787612438\n",
            "Epoch: 35 Batch: 440 Loss: 0.0002424240083200857\n",
            "Epoch: 35 Batch: 460 Loss: 0.003837204072624445\n",
            "Epoch: 35 Batch: 480 Loss: 0.0006076812860555947\n",
            "Epoch: 35 Batch: 500 Loss: 0.003828334854915738\n",
            "Epoch: 35 Batch: 520 Loss: 0.041222285479307175\n",
            "Epoch: 35 Batch: 540 Loss: 0.0002368926943745464\n",
            "Epoch: 35 Batch: 560 Loss: 0.029913758859038353\n",
            "Epoch: 35 Batch: 580 Loss: 0.00037288665771484375\n",
            "Epoch: 35 Batch: 600 Loss: 0.013474464416503906\n",
            "Epoch: 35 Batch: 620 Loss: 0.004967975430190563\n",
            "Epoch: 35 Batch: 640 Loss: 0.015761757269501686\n",
            "Epoch: 35 Batch: 660 Loss: 0.049054812639951706\n",
            "Epoch: 35 Batch: 680 Loss: 0.0003264427068643272\n",
            "Epoch: 35 Batch: 700 Loss: 0.0038272857200354338\n",
            "Epoch: 35 Batch: 720 Loss: 0.00022001266188453883\n",
            "Epoch: 35 Batch: 740 Loss: 0.0002728462277445942\n",
            "Epoch: 35 Batch: 760 Loss: 0.0007028579711914062\n",
            "Epoch: 35 Batch: 780 Loss: 0.0014996528625488281\n",
            "Epoch: 35 Batch: 800 Loss: 0.0005534171941690147\n",
            "Epoch: 35 Batch: 820 Loss: 0.005203723907470703\n",
            "Epoch: 35 Batch: 840 Loss: 0.007014417555183172\n",
            "Epoch: 35 Batch: 860 Loss: 0.0007265090825967491\n",
            "Epoch: 35 Batch: 880 Loss: 0.0005332946893759072\n",
            "Epoch: 35 Batch: 900 Loss: 0.014628124423325062\n",
            "Epoch: 35 Batch: 920 Loss: 0.0017118453979492188\n",
            "Epoch: 35 Batch: 940 Loss: 0.04405813291668892\n",
            "Epoch: 35 Batch: 960 Loss: 0.003139495849609375\n",
            "Epoch: 35 Batch: 980 Loss: 0.2962268888950348\n",
            "Epoch: 36 Batch: 0 Loss: 0.0006559371831826866\n",
            "Epoch: 36 Batch: 20 Loss: 0.00035734177799895406\n",
            "Epoch: 36 Batch: 40 Loss: 0.0004909515264444053\n",
            "Epoch: 36 Batch: 60 Loss: 4.844665454584174e-05\n",
            "Epoch: 36 Batch: 80 Loss: 0.0030519485007971525\n",
            "Epoch: 36 Batch: 100 Loss: 0.0004675865056924522\n",
            "Epoch: 36 Batch: 120 Loss: 0.042170144617557526\n",
            "Epoch: 36 Batch: 140 Loss: 0.0005212783580645919\n",
            "Epoch: 36 Batch: 160 Loss: 0.17788143455982208\n",
            "Epoch: 36 Batch: 180 Loss: 2.708435022213962e-05\n",
            "Epoch: 36 Batch: 200 Loss: 0.005163955502212048\n",
            "Epoch: 36 Batch: 220 Loss: 0.0005283355712890625\n",
            "Epoch: 36 Batch: 240 Loss: 1.373290979245212e-05\n",
            "Epoch: 36 Batch: 260 Loss: 0.0020742416381835938\n",
            "Epoch: 36 Batch: 280 Loss: 0.00022029876708984375\n",
            "Epoch: 36 Batch: 300 Loss: 3.910064697265625e-05\n",
            "Epoch: 36 Batch: 320 Loss: 0.0006439209100790322\n",
            "Epoch: 36 Batch: 340 Loss: 0.001859474228695035\n",
            "Epoch: 36 Batch: 360 Loss: 0.00688514718785882\n",
            "Epoch: 36 Batch: 380 Loss: 0.0038845061790198088\n",
            "Epoch: 36 Batch: 400 Loss: 0.009267330169677734\n",
            "Epoch: 36 Batch: 420 Loss: 0.08965597301721573\n",
            "Epoch: 36 Batch: 440 Loss: 2.346038854739163e-05\n",
            "Epoch: 36 Batch: 460 Loss: 0.03824644163250923\n",
            "Epoch: 36 Batch: 480 Loss: 0.14978165924549103\n",
            "Epoch: 36 Batch: 500 Loss: 0.00047512055607512593\n",
            "Epoch: 36 Batch: 520 Loss: 0.025457000359892845\n",
            "Epoch: 36 Batch: 540 Loss: 0.0003453254757914692\n",
            "Epoch: 36 Batch: 560 Loss: 0.0006843566661700606\n",
            "Epoch: 36 Batch: 580 Loss: 0.009471798315644264\n",
            "Epoch: 36 Batch: 600 Loss: 0.001222801161929965\n",
            "Epoch: 36 Batch: 620 Loss: 0.0001585006684763357\n",
            "Epoch: 36 Batch: 640 Loss: 0.015980815514922142\n",
            "Epoch: 36 Batch: 660 Loss: 0.06048870086669922\n",
            "Epoch: 36 Batch: 680 Loss: 0.015217399224638939\n",
            "Epoch: 36 Batch: 700 Loss: 0.0013660431141033769\n",
            "Epoch: 36 Batch: 720 Loss: 0.004416561219841242\n",
            "Epoch: 36 Batch: 740 Loss: 0.007393264677375555\n",
            "Epoch: 36 Batch: 760 Loss: 0.002777480985969305\n",
            "Epoch: 36 Batch: 780 Loss: 0.02558431588113308\n",
            "Epoch: 36 Batch: 800 Loss: 0.00035266875056549907\n",
            "Epoch: 36 Batch: 820 Loss: 0.0003742217959370464\n",
            "Epoch: 36 Batch: 840 Loss: 0.08122177422046661\n",
            "Epoch: 36 Batch: 860 Loss: 0.005273151211440563\n",
            "Epoch: 36 Batch: 880 Loss: 0.004859542939811945\n",
            "Epoch: 36 Batch: 900 Loss: 0.0001201629638671875\n",
            "Epoch: 36 Batch: 920 Loss: 0.00016021728515625\n",
            "Epoch: 36 Batch: 940 Loss: 0.00016794205293990672\n",
            "Epoch: 36 Batch: 960 Loss: 0.06239719316363335\n",
            "Epoch: 36 Batch: 980 Loss: 0.007571506313979626\n",
            "Epoch: 37 Batch: 0 Loss: 6.29425039733178e-06\n",
            "Epoch: 37 Batch: 20 Loss: 0.00601272564381361\n",
            "Epoch: 37 Batch: 40 Loss: 4.38690176451928e-06\n",
            "Epoch: 37 Batch: 60 Loss: 0.036675263196229935\n",
            "Epoch: 37 Batch: 80 Loss: 0.0004008293035440147\n",
            "Epoch: 37 Batch: 100 Loss: 0.00029878615168854594\n",
            "Epoch: 37 Batch: 120 Loss: 0.0015058517456054688\n",
            "Epoch: 37 Batch: 140 Loss: 0.0001586914004292339\n",
            "Epoch: 37 Batch: 160 Loss: 0.0009265899425372481\n",
            "Epoch: 37 Batch: 180 Loss: 0.0002650260867085308\n",
            "Epoch: 37 Batch: 200 Loss: 0.005111312959343195\n",
            "Epoch: 37 Batch: 220 Loss: 0.1174558624625206\n",
            "Epoch: 37 Batch: 240 Loss: 0.01698150672018528\n",
            "Epoch: 37 Batch: 260 Loss: 0.0039047240279614925\n",
            "Epoch: 37 Batch: 280 Loss: 0.16696682572364807\n",
            "Epoch: 37 Batch: 300 Loss: 0.01896190643310547\n",
            "Epoch: 37 Batch: 320 Loss: 0.0005094528314657509\n",
            "Epoch: 37 Batch: 340 Loss: 0.008602142333984375\n",
            "Epoch: 37 Batch: 360 Loss: 7.743835158180445e-05\n",
            "Epoch: 37 Batch: 380 Loss: 0.004631996154785156\n",
            "Epoch: 37 Batch: 400 Loss: 0.0009490966913290322\n",
            "Epoch: 37 Batch: 420 Loss: 0.07184901088476181\n",
            "Epoch: 37 Batch: 440 Loss: 0.00014495849609375\n",
            "Epoch: 37 Batch: 460 Loss: 0.00033893584623001516\n",
            "Epoch: 37 Batch: 480 Loss: 0.0017210006481036544\n",
            "Epoch: 37 Batch: 500 Loss: 0.00562210101634264\n",
            "Epoch: 37 Batch: 520 Loss: 0.017173100262880325\n",
            "Epoch: 37 Batch: 540 Loss: 0.0010090827709063888\n",
            "Epoch: 37 Batch: 560 Loss: 2.250671423098538e-05\n",
            "Epoch: 37 Batch: 580 Loss: 0.006072139833122492\n",
            "Epoch: 37 Batch: 600 Loss: 0.02329692803323269\n",
            "Epoch: 37 Batch: 620 Loss: 0.020137405022978783\n",
            "Epoch: 37 Batch: 640 Loss: 0.07227258384227753\n",
            "Epoch: 37 Batch: 660 Loss: 0.0001394271821482107\n",
            "Epoch: 37 Batch: 680 Loss: 0.007846927270293236\n",
            "Epoch: 37 Batch: 700 Loss: 0.3930708169937134\n",
            "Epoch: 37 Batch: 720 Loss: 0.0015234947204589844\n",
            "Epoch: 37 Batch: 740 Loss: 0.5097726583480835\n",
            "Epoch: 37 Batch: 760 Loss: 0.0018591880798339844\n",
            "Epoch: 37 Batch: 780 Loss: 0.010913372039794922\n",
            "Epoch: 37 Batch: 800 Loss: 0.17712774872779846\n",
            "Epoch: 37 Batch: 820 Loss: 0.0026039122603833675\n",
            "Epoch: 37 Batch: 840 Loss: 0.027541637420654297\n",
            "Epoch: 37 Batch: 860 Loss: 0.046546079218387604\n",
            "Epoch: 37 Batch: 880 Loss: 0.0016412734985351562\n",
            "Epoch: 37 Batch: 900 Loss: 0.022855376824736595\n",
            "Epoch: 37 Batch: 920 Loss: 0.005176067352294922\n",
            "Epoch: 37 Batch: 940 Loss: 0.004721164703369141\n",
            "Epoch: 37 Batch: 960 Loss: 0.00031490327091887593\n",
            "Epoch: 37 Batch: 980 Loss: 8.096695091808215e-05\n",
            "Epoch: 38 Batch: 0 Loss: 0.0019220352405682206\n",
            "Epoch: 38 Batch: 20 Loss: 1.888275073724799e-05\n",
            "Epoch: 38 Batch: 40 Loss: 6.370544724632055e-05\n",
            "Epoch: 38 Batch: 60 Loss: 0.0044380188919603825\n",
            "Epoch: 38 Batch: 80 Loss: 0.00019550323486328125\n",
            "Epoch: 38 Batch: 100 Loss: 0.0006975174183025956\n",
            "Epoch: 38 Batch: 120 Loss: 0.0028603554237633944\n",
            "Epoch: 38 Batch: 140 Loss: 0.0002650260867085308\n",
            "Epoch: 38 Batch: 160 Loss: 0.01599292829632759\n",
            "Epoch: 38 Batch: 180 Loss: 0.059418391436338425\n",
            "Epoch: 38 Batch: 200 Loss: 0.004701518919318914\n",
            "Epoch: 38 Batch: 220 Loss: 2.307891918462701e-05\n",
            "Epoch: 38 Batch: 240 Loss: 0.00030040740966796875\n",
            "Epoch: 38 Batch: 260 Loss: 4.520416405284777e-05\n",
            "Epoch: 38 Batch: 280 Loss: 0.003760051680728793\n",
            "Epoch: 38 Batch: 300 Loss: 0.00042209625826217234\n",
            "Epoch: 38 Batch: 320 Loss: 0.00013065338134765625\n",
            "Epoch: 38 Batch: 340 Loss: 5.874633643543348e-05\n",
            "Epoch: 38 Batch: 360 Loss: 0.0009634018060751259\n",
            "Epoch: 38 Batch: 380 Loss: 0.0003050804079975933\n",
            "Epoch: 38 Batch: 400 Loss: 1.468658410885837e-05\n",
            "Epoch: 38 Batch: 420 Loss: 0.02258138731122017\n",
            "Epoch: 38 Batch: 440 Loss: 7.629394644936838e-07\n",
            "Epoch: 38 Batch: 460 Loss: 0.0008609771612100303\n",
            "Epoch: 38 Batch: 480 Loss: 0.012647151947021484\n",
            "Epoch: 38 Batch: 500 Loss: 0.0014429092407226562\n",
            "Epoch: 38 Batch: 520 Loss: 2.899169885495212e-05\n",
            "Epoch: 38 Batch: 540 Loss: 0.05892886966466904\n",
            "Epoch: 38 Batch: 560 Loss: 0.0001371383696096018\n",
            "Epoch: 38 Batch: 580 Loss: 0.00296363839879632\n",
            "Epoch: 38 Batch: 600 Loss: 0.02327880822122097\n",
            "Epoch: 38 Batch: 620 Loss: 0.00024728773860260844\n",
            "Epoch: 38 Batch: 640 Loss: 0.006641483400017023\n",
            "Epoch: 38 Batch: 660 Loss: 0.00026702880859375\n",
            "Epoch: 38 Batch: 680 Loss: 0.0036004066932946444\n",
            "Epoch: 38 Batch: 700 Loss: 5.168914867681451e-05\n",
            "Epoch: 38 Batch: 720 Loss: 0.0503239631652832\n",
            "Epoch: 38 Batch: 740 Loss: 1.411438006471144e-05\n",
            "Epoch: 38 Batch: 760 Loss: 0.00032253266545012593\n",
            "Epoch: 38 Batch: 780 Loss: 7.05718994140625e-05\n",
            "Epoch: 38 Batch: 800 Loss: 0.6331976652145386\n",
            "Epoch: 38 Batch: 820 Loss: 9.241104271495715e-05\n",
            "Epoch: 38 Batch: 840 Loss: 0.02773256227374077\n",
            "Epoch: 38 Batch: 860 Loss: 0.0032994269859045744\n",
            "Epoch: 38 Batch: 880 Loss: 0.12696199119091034\n",
            "Epoch: 38 Batch: 900 Loss: 7.24792471373803e-06\n",
            "Epoch: 38 Batch: 920 Loss: 0.007254695985466242\n",
            "Epoch: 38 Batch: 940 Loss: 0.40825819969177246\n",
            "Epoch: 38 Batch: 960 Loss: 0.0018481254810467362\n",
            "Epoch: 38 Batch: 980 Loss: 0.03359527513384819\n",
            "Epoch: 39 Batch: 0 Loss: 0.00015153884305618703\n",
            "Epoch: 39 Batch: 20 Loss: 0.007502746768295765\n",
            "Epoch: 39 Batch: 40 Loss: 0.008755492977797985\n",
            "Epoch: 39 Batch: 60 Loss: 0.0007640838739462197\n",
            "Epoch: 39 Batch: 80 Loss: 0.0003341674746479839\n",
            "Epoch: 39 Batch: 100 Loss: 0.059076786041259766\n",
            "Epoch: 39 Batch: 120 Loss: 0.015854978933930397\n",
            "Epoch: 39 Batch: 140 Loss: 0.013032341375946999\n",
            "Epoch: 39 Batch: 160 Loss: 0.013010025024414062\n",
            "Epoch: 39 Batch: 180 Loss: 0.025065993890166283\n",
            "Epoch: 39 Batch: 200 Loss: 0.036278724670410156\n",
            "Epoch: 39 Batch: 220 Loss: 0.007308578584343195\n",
            "Epoch: 39 Batch: 240 Loss: 0.05825605243444443\n",
            "Epoch: 39 Batch: 260 Loss: 1.678466833254788e-05\n",
            "Epoch: 39 Batch: 280 Loss: 0.0012672424782067537\n",
            "Epoch: 39 Batch: 300 Loss: 0.003963470458984375\n",
            "Epoch: 39 Batch: 320 Loss: 0.5464143753051758\n",
            "Epoch: 39 Batch: 340 Loss: 7.591247413074598e-05\n",
            "Epoch: 39 Batch: 360 Loss: 0.5895261764526367\n",
            "Epoch: 39 Batch: 380 Loss: 0.013987827114760876\n",
            "Epoch: 39 Batch: 400 Loss: 0.0015987396473065019\n",
            "Epoch: 39 Batch: 420 Loss: 0.003511714981868863\n",
            "Epoch: 39 Batch: 440 Loss: 0.08034944534301758\n",
            "Epoch: 39 Batch: 460 Loss: 0.13796977698802948\n",
            "Epoch: 39 Batch: 480 Loss: 0.00016679763211868703\n",
            "Epoch: 39 Batch: 500 Loss: 0.0016651153564453125\n",
            "Epoch: 39 Batch: 520 Loss: 9.51766996877268e-05\n",
            "Epoch: 39 Batch: 540 Loss: 0.0006763458368368447\n",
            "Epoch: 39 Batch: 560 Loss: 1.6183135509490967\n",
            "Epoch: 39 Batch: 580 Loss: 0.00014190674119163305\n",
            "Epoch: 39 Batch: 600 Loss: 0.06771431118249893\n",
            "Epoch: 39 Batch: 620 Loss: 0.0038126944564282894\n",
            "Epoch: 39 Batch: 640 Loss: 0.00664939871057868\n",
            "Epoch: 39 Batch: 660 Loss: 0.0010917664039880037\n",
            "Epoch: 39 Batch: 680 Loss: 9.1552734375e-05\n",
            "Epoch: 39 Batch: 700 Loss: 0.004463768098503351\n",
            "Epoch: 39 Batch: 720 Loss: 0.5024786591529846\n",
            "Epoch: 39 Batch: 740 Loss: 0.25376096367836\n",
            "Epoch: 39 Batch: 760 Loss: 0.00358161935582757\n",
            "Epoch: 39 Batch: 780 Loss: 2.498626781743951e-05\n",
            "Epoch: 39 Batch: 800 Loss: 0.00023202896409202367\n",
            "Epoch: 39 Batch: 820 Loss: 0.002814006758853793\n",
            "Epoch: 39 Batch: 840 Loss: 0.007708072662353516\n",
            "Epoch: 39 Batch: 860 Loss: 0.0031570433638989925\n",
            "Epoch: 39 Batch: 880 Loss: 0.0004467010439839214\n",
            "Epoch: 39 Batch: 900 Loss: 0.025649070739746094\n",
            "Epoch: 39 Batch: 920 Loss: 0.012471770867705345\n",
            "Epoch: 39 Batch: 940 Loss: 0.00087060930673033\n",
            "Epoch: 39 Batch: 960 Loss: 0.007923126220703125\n",
            "Epoch: 39 Batch: 980 Loss: 0.00016326904005836695\n",
            "Epoch: 40 Batch: 0 Loss: 0.0038226128090173006\n",
            "Epoch: 40 Batch: 20 Loss: 0.000255584716796875\n",
            "Epoch: 40 Batch: 40 Loss: 0.221827894449234\n",
            "Epoch: 40 Batch: 60 Loss: 0.005805683322250843\n",
            "Epoch: 40 Batch: 80 Loss: 0.0025577545166015625\n",
            "Epoch: 40 Batch: 100 Loss: 0.002582550048828125\n",
            "Epoch: 40 Batch: 120 Loss: 0.0017886161804199219\n",
            "Epoch: 40 Batch: 140 Loss: 0.0014227867359295487\n",
            "Epoch: 40 Batch: 160 Loss: 0.08496637642383575\n",
            "Epoch: 40 Batch: 180 Loss: 2.288818359375e-05\n",
            "Epoch: 40 Batch: 200 Loss: 0.027649497613310814\n",
            "Epoch: 40 Batch: 220 Loss: 0.0016351699596270919\n",
            "Epoch: 40 Batch: 240 Loss: 0.01179971732199192\n",
            "Epoch: 40 Batch: 260 Loss: 0.0012028694618493319\n",
            "Epoch: 40 Batch: 280 Loss: 0.011728000827133656\n",
            "Epoch: 40 Batch: 300 Loss: 9.660721116233617e-05\n",
            "Epoch: 40 Batch: 320 Loss: 0.04056110233068466\n",
            "Epoch: 40 Batch: 340 Loss: 0.0015143394703045487\n",
            "Epoch: 40 Batch: 360 Loss: 0.009047413244843483\n",
            "Epoch: 40 Batch: 380 Loss: 0.07278366386890411\n",
            "Epoch: 40 Batch: 400 Loss: 4.978180004400201e-05\n",
            "Epoch: 40 Batch: 420 Loss: 0.001963329268619418\n",
            "Epoch: 40 Batch: 440 Loss: 0.0037516593001782894\n",
            "Epoch: 40 Batch: 460 Loss: 0.0038739205338060856\n",
            "Epoch: 40 Batch: 480 Loss: 0.005156421568244696\n",
            "Epoch: 40 Batch: 500 Loss: 0.0063345907256007195\n",
            "Epoch: 40 Batch: 520 Loss: 0.060227394104003906\n",
            "Epoch: 40 Batch: 540 Loss: 0.0006846428150311112\n",
            "Epoch: 40 Batch: 560 Loss: 1.029968279908644e-05\n",
            "Epoch: 40 Batch: 580 Loss: 0.0063949585892260075\n",
            "Epoch: 40 Batch: 600 Loss: 0.0016353607643395662\n",
            "Epoch: 40 Batch: 620 Loss: 0.00043992995051667094\n",
            "Epoch: 40 Batch: 640 Loss: 0.013467025943100452\n",
            "Epoch: 40 Batch: 660 Loss: 0.00013084411330055445\n",
            "Epoch: 40 Batch: 680 Loss: 0.06732487678527832\n",
            "Epoch: 40 Batch: 700 Loss: 0.00018558502779342234\n",
            "Epoch: 40 Batch: 720 Loss: 0.026316260918974876\n",
            "Epoch: 40 Batch: 740 Loss: 0.007335567381232977\n",
            "Epoch: 40 Batch: 760 Loss: 9.5367431640625e-07\n",
            "Epoch: 40 Batch: 780 Loss: 0.32664480805397034\n",
            "Epoch: 40 Batch: 800 Loss: 0.0014963150024414062\n",
            "Epoch: 40 Batch: 820 Loss: 0.000118255615234375\n",
            "Epoch: 40 Batch: 840 Loss: 0.014940738677978516\n",
            "Epoch: 40 Batch: 860 Loss: 0.7905975580215454\n",
            "Epoch: 40 Batch: 880 Loss: 0.005820846650749445\n",
            "Epoch: 40 Batch: 900 Loss: 0.00598487863317132\n",
            "Epoch: 40 Batch: 920 Loss: 0.0005363464588299394\n",
            "Epoch: 40 Batch: 940 Loss: 0.0005597114795818925\n",
            "Epoch: 40 Batch: 960 Loss: 0.09630446135997772\n",
            "Epoch: 40 Batch: 980 Loss: 0.006535148713737726\n",
            "Epoch: 41 Batch: 0 Loss: 0.05971164628863335\n",
            "Epoch: 41 Batch: 20 Loss: 0.5153104662895203\n",
            "Epoch: 41 Batch: 40 Loss: 0.0003926277277059853\n",
            "Epoch: 41 Batch: 60 Loss: 0.00053491594735533\n",
            "Epoch: 41 Batch: 80 Loss: 0.0003304481506347656\n",
            "Epoch: 41 Batch: 100 Loss: 0.0016132354503497481\n",
            "Epoch: 41 Batch: 120 Loss: 0.0003158569452352822\n",
            "Epoch: 41 Batch: 140 Loss: 0.0022475242149084806\n",
            "Epoch: 41 Batch: 160 Loss: 0.00788269005715847\n",
            "Epoch: 41 Batch: 180 Loss: 0.032361604273319244\n",
            "Epoch: 41 Batch: 200 Loss: 0.010778045281767845\n",
            "Epoch: 41 Batch: 220 Loss: 1.5258789289873675e-06\n",
            "Epoch: 41 Batch: 240 Loss: 0.0001756668061716482\n",
            "Epoch: 41 Batch: 260 Loss: 0.0009149551624432206\n",
            "Epoch: 41 Batch: 280 Loss: 2.212524486822076e-05\n",
            "Epoch: 41 Batch: 300 Loss: 0.008014869876205921\n",
            "Epoch: 41 Batch: 320 Loss: 0.0017004013061523438\n",
            "Epoch: 41 Batch: 340 Loss: 1.788185477256775\n",
            "Epoch: 41 Batch: 360 Loss: 0.024525929242372513\n",
            "Epoch: 41 Batch: 380 Loss: 0.004422378726303577\n",
            "Epoch: 41 Batch: 400 Loss: 0.03556108474731445\n",
            "Epoch: 41 Batch: 420 Loss: 0.008081627078354359\n",
            "Epoch: 41 Batch: 440 Loss: 0.038347482681274414\n",
            "Epoch: 41 Batch: 460 Loss: 0.0011255263816565275\n",
            "Epoch: 41 Batch: 480 Loss: 0.02279653586447239\n",
            "Epoch: 41 Batch: 500 Loss: 0.0016828536754474044\n",
            "Epoch: 41 Batch: 520 Loss: 0.31298500299453735\n",
            "Epoch: 41 Batch: 540 Loss: 0.14870338141918182\n",
            "Epoch: 41 Batch: 560 Loss: 0.6260338425636292\n",
            "Epoch: 41 Batch: 580 Loss: 0.0001697540283203125\n",
            "Epoch: 41 Batch: 600 Loss: 0.0004264831659384072\n",
            "Epoch: 41 Batch: 620 Loss: 0.001688289688900113\n",
            "Epoch: 41 Batch: 640 Loss: 0.00046863555326126516\n",
            "Epoch: 41 Batch: 660 Loss: 0.00012054443504894152\n",
            "Epoch: 41 Batch: 680 Loss: 0.02147998847067356\n",
            "Epoch: 41 Batch: 700 Loss: 0.040270231664180756\n",
            "Epoch: 41 Batch: 720 Loss: 0.061283111572265625\n",
            "Epoch: 41 Batch: 740 Loss: 0.45909199118614197\n",
            "Epoch: 41 Batch: 760 Loss: 6.904602196300402e-05\n",
            "Epoch: 41 Batch: 780 Loss: 0.024410057812929153\n",
            "Epoch: 41 Batch: 800 Loss: 0.00631294259801507\n",
            "Epoch: 41 Batch: 820 Loss: 0.009376859292387962\n",
            "Epoch: 41 Batch: 840 Loss: 0.0016197204822674394\n",
            "Epoch: 41 Batch: 860 Loss: 0.0005811691517010331\n",
            "Epoch: 41 Batch: 880 Loss: 3.24249276673072e-06\n",
            "Epoch: 41 Batch: 900 Loss: 0.00336456298828125\n",
            "Epoch: 41 Batch: 920 Loss: 0.007026290986686945\n",
            "Epoch: 41 Batch: 940 Loss: 0.3506174683570862\n",
            "Epoch: 41 Batch: 960 Loss: 1.506805438111769e-05\n",
            "Epoch: 41 Batch: 980 Loss: 0.009427070617675781\n",
            "Epoch: 42 Batch: 0 Loss: 0.0016494750743731856\n",
            "Epoch: 42 Batch: 20 Loss: 0.06755523383617401\n",
            "Epoch: 42 Batch: 40 Loss: 0.0018470764625817537\n",
            "Epoch: 42 Batch: 60 Loss: 0.00043430327787064016\n",
            "Epoch: 42 Batch: 80 Loss: 0.0015399933326989412\n",
            "Epoch: 42 Batch: 100 Loss: 0.00035228728665970266\n",
            "Epoch: 42 Batch: 120 Loss: 0.022243022918701172\n",
            "Epoch: 42 Batch: 140 Loss: 0.0001657485991017893\n",
            "Epoch: 42 Batch: 160 Loss: 0.3284018039703369\n",
            "Epoch: 42 Batch: 180 Loss: 0.0017371177673339844\n",
            "Epoch: 42 Batch: 200 Loss: 8.02040085545741e-05\n",
            "Epoch: 42 Batch: 220 Loss: 0.1747075766324997\n",
            "Epoch: 42 Batch: 240 Loss: 0.0016265868907794356\n",
            "Epoch: 42 Batch: 260 Loss: 0.001047420548275113\n",
            "Epoch: 42 Batch: 280 Loss: 0.0012969970703125\n",
            "Epoch: 42 Batch: 300 Loss: 0.006057930178940296\n",
            "Epoch: 42 Batch: 320 Loss: 5.7697296142578125e-05\n",
            "Epoch: 42 Batch: 340 Loss: 0.00031108857365325093\n",
            "Epoch: 42 Batch: 360 Loss: 0.006112098693847656\n",
            "Epoch: 42 Batch: 380 Loss: 0.0017614364624023438\n",
            "Epoch: 42 Batch: 400 Loss: 0.0017206191550940275\n",
            "Epoch: 42 Batch: 420 Loss: 0.0036492347717285156\n",
            "Epoch: 42 Batch: 440 Loss: 0.00153264997061342\n",
            "Epoch: 42 Batch: 460 Loss: 0.000423431396484375\n",
            "Epoch: 42 Batch: 480 Loss: 0.028826236724853516\n",
            "Epoch: 42 Batch: 500 Loss: 0.01015319861471653\n",
            "Epoch: 42 Batch: 520 Loss: 0.08664669841527939\n",
            "Epoch: 42 Batch: 540 Loss: 0.00013599396334029734\n",
            "Epoch: 42 Batch: 560 Loss: 0.006943130400031805\n",
            "Epoch: 42 Batch: 580 Loss: 0.017148207873106003\n",
            "Epoch: 42 Batch: 600 Loss: 0.00978631991893053\n",
            "Epoch: 42 Batch: 620 Loss: 0.7610551714897156\n",
            "Epoch: 42 Batch: 640 Loss: 0.0008282661437988281\n",
            "Epoch: 42 Batch: 660 Loss: 0.00015163421630859375\n",
            "Epoch: 42 Batch: 680 Loss: 0.0007949828868731856\n",
            "Epoch: 42 Batch: 700 Loss: 0.018103599548339844\n",
            "Epoch: 42 Batch: 720 Loss: 1.811981201171875e-05\n",
            "Epoch: 42 Batch: 740 Loss: 0.002063178922981024\n",
            "Epoch: 42 Batch: 760 Loss: 8.077621168922633e-05\n",
            "Epoch: 42 Batch: 780 Loss: 0.00030384064302779734\n",
            "Epoch: 42 Batch: 800 Loss: 0.0010378838051110506\n",
            "Epoch: 42 Batch: 820 Loss: 0.06597576290369034\n",
            "Epoch: 42 Batch: 840 Loss: 0.0009052276727743447\n",
            "Epoch: 42 Batch: 860 Loss: 0.0033665657974779606\n",
            "Epoch: 42 Batch: 880 Loss: 0.0016633033519610763\n",
            "Epoch: 42 Batch: 900 Loss: 0.013912486843764782\n",
            "Epoch: 42 Batch: 920 Loss: 0.00016326904005836695\n",
            "Epoch: 42 Batch: 940 Loss: 0.007896041497588158\n",
            "Epoch: 42 Batch: 960 Loss: 0.0013018607860431075\n",
            "Epoch: 42 Batch: 980 Loss: 0.20257988572120667\n",
            "Epoch: 43 Batch: 0 Loss: 0.033733319491147995\n",
            "Epoch: 43 Batch: 20 Loss: 0.0014511108165606856\n",
            "Epoch: 43 Batch: 40 Loss: 0.002212619874626398\n",
            "Epoch: 43 Batch: 60 Loss: 7.43865984986769e-06\n",
            "Epoch: 43 Batch: 80 Loss: 2.384185791015625e-05\n",
            "Epoch: 43 Batch: 100 Loss: 0.0002117156982421875\n",
            "Epoch: 43 Batch: 120 Loss: 0.09162960201501846\n",
            "Epoch: 43 Batch: 140 Loss: 0.00023298263840842992\n",
            "Epoch: 43 Batch: 160 Loss: 0.016555214300751686\n",
            "Epoch: 43 Batch: 180 Loss: 0.00026721955509856343\n",
            "Epoch: 43 Batch: 200 Loss: 6.732940528308973e-05\n",
            "Epoch: 43 Batch: 220 Loss: 5.416870044427924e-05\n",
            "Epoch: 43 Batch: 240 Loss: 0.001422023749910295\n",
            "Epoch: 43 Batch: 260 Loss: 4.76837158203125e-05\n",
            "Epoch: 43 Batch: 280 Loss: 2.86102294921875e-05\n",
            "Epoch: 43 Batch: 300 Loss: 0.061005402356386185\n",
            "Epoch: 43 Batch: 320 Loss: 0.019621467217803\n",
            "Epoch: 43 Batch: 340 Loss: 0.0007829666137695312\n",
            "Epoch: 43 Batch: 360 Loss: 0.0002522468566894531\n",
            "Epoch: 43 Batch: 380 Loss: 0.01766233518719673\n",
            "Epoch: 43 Batch: 400 Loss: 0.045534323900938034\n",
            "Epoch: 43 Batch: 420 Loss: 0.0008304595830850303\n",
            "Epoch: 43 Batch: 440 Loss: 0.0014560699928551912\n",
            "Epoch: 43 Batch: 460 Loss: 0.003156471299007535\n",
            "Epoch: 43 Batch: 480 Loss: 7.04765334376134e-05\n",
            "Epoch: 43 Batch: 500 Loss: 1.487731969973538e-05\n",
            "Epoch: 43 Batch: 520 Loss: 0.0011873245239257812\n",
            "Epoch: 43 Batch: 540 Loss: 0.14388465881347656\n",
            "Epoch: 43 Batch: 560 Loss: 0.0011887550354003906\n",
            "Epoch: 43 Batch: 580 Loss: 0.017934417352080345\n",
            "Epoch: 43 Batch: 600 Loss: 0.1320343017578125\n",
            "Epoch: 43 Batch: 620 Loss: 0.01834850385785103\n",
            "Epoch: 43 Batch: 640 Loss: 5.359649730962701e-05\n",
            "Epoch: 43 Batch: 660 Loss: 0.011668110266327858\n",
            "Epoch: 43 Batch: 680 Loss: 0.0013530731666833162\n",
            "Epoch: 43 Batch: 700 Loss: 0.0004805564822163433\n",
            "Epoch: 43 Batch: 720 Loss: 0.007430076599121094\n",
            "Epoch: 43 Batch: 740 Loss: 0.005042362026870251\n",
            "Epoch: 43 Batch: 760 Loss: 0.0006358146783895791\n",
            "Epoch: 43 Batch: 780 Loss: 0.0006359100225381553\n",
            "Epoch: 43 Batch: 800 Loss: 0.19619926810264587\n",
            "Epoch: 43 Batch: 820 Loss: 0.6329292058944702\n",
            "Epoch: 43 Batch: 840 Loss: 0.0007135391351766884\n",
            "Epoch: 43 Batch: 860 Loss: 0.0001871108979685232\n",
            "Epoch: 43 Batch: 880 Loss: 0.01453242264688015\n",
            "Epoch: 43 Batch: 900 Loss: 0.211052805185318\n",
            "Epoch: 43 Batch: 920 Loss: 0.4122251868247986\n",
            "Epoch: 43 Batch: 940 Loss: 0.18682345747947693\n",
            "Epoch: 43 Batch: 960 Loss: 0.04192237928509712\n",
            "Epoch: 43 Batch: 980 Loss: 0.001490688300691545\n",
            "Epoch: 44 Batch: 0 Loss: 0.07464218139648438\n",
            "Epoch: 44 Batch: 20 Loss: 0.4944990277290344\n",
            "Epoch: 44 Batch: 40 Loss: 0.010680007748305798\n",
            "Epoch: 44 Batch: 60 Loss: 0.015776444226503372\n",
            "Epoch: 44 Batch: 80 Loss: 0.0009060859447345138\n",
            "Epoch: 44 Batch: 100 Loss: 0.13338318467140198\n",
            "Epoch: 44 Batch: 120 Loss: 0.013363838195800781\n",
            "Epoch: 44 Batch: 140 Loss: 0.001706886338070035\n",
            "Epoch: 44 Batch: 160 Loss: 0.10444159805774689\n",
            "Epoch: 44 Batch: 180 Loss: 0.004152393434196711\n",
            "Epoch: 44 Batch: 200 Loss: 0.22090110182762146\n",
            "Epoch: 44 Batch: 220 Loss: 0.0007326126215048134\n",
            "Epoch: 44 Batch: 240 Loss: 0.043825339525938034\n",
            "Epoch: 44 Batch: 260 Loss: 0.11565113067626953\n",
            "Epoch: 44 Batch: 280 Loss: 0.0009171486017294228\n",
            "Epoch: 44 Batch: 300 Loss: 0.041034601628780365\n",
            "Epoch: 44 Batch: 320 Loss: 0.1509539633989334\n",
            "Epoch: 44 Batch: 340 Loss: 8.926391456043348e-05\n",
            "Epoch: 44 Batch: 360 Loss: 0.00220832834020257\n",
            "Epoch: 44 Batch: 380 Loss: 0.00047054290189407766\n",
            "Epoch: 44 Batch: 400 Loss: 0.04383697360754013\n",
            "Epoch: 44 Batch: 420 Loss: 3.013610876223538e-05\n",
            "Epoch: 44 Batch: 440 Loss: 0.0006609916454181075\n",
            "Epoch: 44 Batch: 460 Loss: 0.00037555693415924907\n",
            "Epoch: 44 Batch: 480 Loss: 0.0024296760093420744\n",
            "Epoch: 44 Batch: 500 Loss: 0.46905747056007385\n",
            "Epoch: 44 Batch: 520 Loss: 0.001361846923828125\n",
            "Epoch: 44 Batch: 540 Loss: 0.02205676957964897\n",
            "Epoch: 44 Batch: 560 Loss: 0.0731411948800087\n",
            "Epoch: 44 Batch: 580 Loss: 0.03235568851232529\n",
            "Epoch: 44 Batch: 600 Loss: 0.013382148928940296\n",
            "Epoch: 44 Batch: 620 Loss: 0.015408039093017578\n",
            "Epoch: 44 Batch: 640 Loss: 0.00010995865159202367\n",
            "Epoch: 44 Batch: 660 Loss: 0.0004063606320414692\n",
            "Epoch: 44 Batch: 680 Loss: 1.1062707901000977\n",
            "Epoch: 44 Batch: 700 Loss: 1.4125235080718994\n",
            "Epoch: 44 Batch: 720 Loss: 0.036455534398555756\n",
            "Epoch: 44 Batch: 740 Loss: 0.003531837370246649\n",
            "Epoch: 44 Batch: 760 Loss: 0.03376789018511772\n",
            "Epoch: 44 Batch: 780 Loss: 0.013227462768554688\n",
            "Epoch: 44 Batch: 800 Loss: 0.0006196975591592491\n",
            "Epoch: 44 Batch: 820 Loss: 0.0004112243768759072\n",
            "Epoch: 44 Batch: 840 Loss: 0.18457579612731934\n",
            "Epoch: 44 Batch: 860 Loss: 1.449584942747606e-05\n",
            "Epoch: 44 Batch: 880 Loss: 0.0005321502685546875\n",
            "Epoch: 44 Batch: 900 Loss: 0.005240058992058039\n",
            "Epoch: 44 Batch: 920 Loss: 0.00814523734152317\n",
            "Epoch: 44 Batch: 940 Loss: 0.002643108367919922\n",
            "Epoch: 44 Batch: 960 Loss: 0.04036378860473633\n",
            "Epoch: 44 Batch: 980 Loss: 0.18878407776355743\n",
            "Epoch: 45 Batch: 0 Loss: 0.008926963433623314\n",
            "Epoch: 45 Batch: 20 Loss: 0.1233799010515213\n",
            "Epoch: 45 Batch: 40 Loss: 0.0002388000430073589\n",
            "Epoch: 45 Batch: 60 Loss: 0.0009393692016601562\n",
            "Epoch: 45 Batch: 80 Loss: 0.08322830498218536\n",
            "Epoch: 45 Batch: 100 Loss: 0.12637662887573242\n",
            "Epoch: 45 Batch: 120 Loss: 0.0012763977283611894\n",
            "Epoch: 45 Batch: 140 Loss: 0.00012111663818359375\n",
            "Epoch: 45 Batch: 160 Loss: 0.002815818879753351\n",
            "Epoch: 45 Batch: 180 Loss: 0.044747352600097656\n",
            "Epoch: 45 Batch: 200 Loss: 2.079009937006049e-05\n",
            "Epoch: 45 Batch: 220 Loss: 0.004328536801040173\n",
            "Epoch: 45 Batch: 240 Loss: 2.613067590573337e-05\n",
            "Epoch: 45 Batch: 260 Loss: 0.02298150025308132\n",
            "Epoch: 45 Batch: 280 Loss: 2.2792815798311494e-05\n",
            "Epoch: 45 Batch: 300 Loss: 0.0009777068626135588\n",
            "Epoch: 45 Batch: 320 Loss: 0.002773857209831476\n",
            "Epoch: 45 Batch: 340 Loss: 0.0005833625909872353\n",
            "Epoch: 45 Batch: 360 Loss: 0.0006319999811239541\n",
            "Epoch: 45 Batch: 380 Loss: 0.00015153884305618703\n",
            "Epoch: 45 Batch: 400 Loss: 0.019649982452392578\n",
            "Epoch: 45 Batch: 420 Loss: 0.03356447070837021\n",
            "Epoch: 45 Batch: 440 Loss: 4.329681542003527e-05\n",
            "Epoch: 45 Batch: 460 Loss: 0.45172062516212463\n",
            "Epoch: 45 Batch: 480 Loss: 0.5970298647880554\n",
            "Epoch: 45 Batch: 500 Loss: 0.12332029640674591\n",
            "Epoch: 45 Batch: 520 Loss: 0.011678600683808327\n",
            "Epoch: 45 Batch: 540 Loss: 0.00013217926607467234\n",
            "Epoch: 45 Batch: 560 Loss: 0.0008857726934365928\n",
            "Epoch: 45 Batch: 580 Loss: 0.31696540117263794\n",
            "Epoch: 45 Batch: 600 Loss: 0.024280166253447533\n",
            "Epoch: 45 Batch: 620 Loss: 0.0006276130443438888\n",
            "Epoch: 45 Batch: 640 Loss: 0.0016672133933752775\n",
            "Epoch: 45 Batch: 660 Loss: 0.39696627855300903\n",
            "Epoch: 45 Batch: 680 Loss: 0.0028149604331701994\n",
            "Epoch: 45 Batch: 700 Loss: 2.098083541568485e-06\n",
            "Epoch: 45 Batch: 720 Loss: 1.9073486328125e-06\n",
            "Epoch: 45 Batch: 740 Loss: 0.0007522582891397178\n",
            "Epoch: 45 Batch: 760 Loss: 6.50405854685232e-05\n",
            "Epoch: 45 Batch: 780 Loss: 0.0068457601591944695\n",
            "Epoch: 45 Batch: 800 Loss: 0.3011740744113922\n",
            "Epoch: 45 Batch: 820 Loss: 0.004412174224853516\n",
            "Epoch: 45 Batch: 840 Loss: 0.0016154289478436112\n",
            "Epoch: 45 Batch: 860 Loss: 0.00011978149268543348\n",
            "Epoch: 45 Batch: 880 Loss: 0.0003635406610555947\n",
            "Epoch: 45 Batch: 900 Loss: 0.0036143301986157894\n",
            "Epoch: 45 Batch: 920 Loss: 0.016773700714111328\n",
            "Epoch: 45 Batch: 940 Loss: 0.23964376747608185\n",
            "Epoch: 45 Batch: 960 Loss: 2.040863000729587e-05\n",
            "Epoch: 45 Batch: 980 Loss: 0.06006889417767525\n",
            "Epoch: 46 Batch: 0 Loss: 0.0005399704095907509\n",
            "Epoch: 46 Batch: 20 Loss: 0.26706498861312866\n",
            "Epoch: 46 Batch: 40 Loss: 0.0030311583541333675\n",
            "Epoch: 46 Batch: 60 Loss: 0.002715683076530695\n",
            "Epoch: 46 Batch: 80 Loss: 0.5533558130264282\n",
            "Epoch: 46 Batch: 100 Loss: 0.0005404472467489541\n",
            "Epoch: 46 Batch: 120 Loss: 0.0038628578186035156\n",
            "Epoch: 46 Batch: 140 Loss: 0.002466869307681918\n",
            "Epoch: 46 Batch: 160 Loss: 0.03144550323486328\n",
            "Epoch: 46 Batch: 180 Loss: 0.047094009816646576\n",
            "Epoch: 46 Batch: 200 Loss: 0.38190698623657227\n",
            "Epoch: 46 Batch: 220 Loss: 0.0008812904125079513\n",
            "Epoch: 46 Batch: 240 Loss: 0.00017156600370071828\n",
            "Epoch: 46 Batch: 260 Loss: 0.0013895988231524825\n",
            "Epoch: 46 Batch: 280 Loss: 0.005044269375503063\n",
            "Epoch: 46 Batch: 300 Loss: 4.100799560546875e-05\n",
            "Epoch: 46 Batch: 320 Loss: 0.0003380775451660156\n",
            "Epoch: 46 Batch: 340 Loss: 0.02129058912396431\n",
            "Epoch: 46 Batch: 360 Loss: 0.014271354302763939\n",
            "Epoch: 46 Batch: 380 Loss: 0.00018768310837913305\n",
            "Epoch: 46 Batch: 400 Loss: 5.264282299322076e-05\n",
            "Epoch: 46 Batch: 420 Loss: 5.14984139954322e-06\n",
            "Epoch: 46 Batch: 440 Loss: 0.0003143310605082661\n",
            "Epoch: 46 Batch: 460 Loss: 4.673004150390625e-05\n",
            "Epoch: 46 Batch: 480 Loss: 1.888275073724799e-05\n",
            "Epoch: 46 Batch: 500 Loss: 0.00020923613919876516\n",
            "Epoch: 46 Batch: 520 Loss: 8.96453857421875e-05\n",
            "Epoch: 46 Batch: 540 Loss: 0.2065233290195465\n",
            "Epoch: 46 Batch: 560 Loss: 4.472732689464465e-05\n",
            "Epoch: 46 Batch: 580 Loss: 6.10351571594947e-06\n",
            "Epoch: 46 Batch: 600 Loss: 0.0018486976623535156\n",
            "Epoch: 46 Batch: 620 Loss: 0.012530612759292126\n",
            "Epoch: 46 Batch: 640 Loss: 2.1382758617401123\n",
            "Epoch: 46 Batch: 660 Loss: 3.604888843256049e-05\n",
            "Epoch: 46 Batch: 680 Loss: 0.09162969887256622\n",
            "Epoch: 46 Batch: 700 Loss: 9.098053124034777e-05\n",
            "Epoch: 46 Batch: 720 Loss: 0.0012241363292559981\n",
            "Epoch: 46 Batch: 740 Loss: 0.008994007483124733\n",
            "Epoch: 46 Batch: 760 Loss: 0.00012311936006881297\n",
            "Epoch: 46 Batch: 780 Loss: 0.004640770144760609\n",
            "Epoch: 46 Batch: 800 Loss: 0.0001890182466013357\n",
            "Epoch: 46 Batch: 820 Loss: 0.00037384033203125\n",
            "Epoch: 46 Batch: 840 Loss: 0.00118341448251158\n",
            "Epoch: 46 Batch: 860 Loss: 0.00014591217041015625\n",
            "Epoch: 46 Batch: 880 Loss: 0.0012577057350426912\n",
            "Epoch: 46 Batch: 900 Loss: 0.0010142326354980469\n",
            "Epoch: 46 Batch: 920 Loss: 9.403228614246473e-05\n",
            "Epoch: 46 Batch: 940 Loss: 0.0026138306129723787\n",
            "Epoch: 46 Batch: 960 Loss: 0.0007731437799520791\n",
            "Epoch: 46 Batch: 980 Loss: 0.00020647048950195312\n",
            "Epoch: 47 Batch: 0 Loss: 0.0018516540294513106\n",
            "Epoch: 47 Batch: 20 Loss: 0.00067987444344908\n",
            "Epoch: 47 Batch: 40 Loss: 0.00017280578322242945\n",
            "Epoch: 47 Batch: 60 Loss: 0.0003234863397665322\n",
            "Epoch: 47 Batch: 80 Loss: 1.2111663636460435e-05\n",
            "Epoch: 47 Batch: 100 Loss: 0.0028825760819017887\n",
            "Epoch: 47 Batch: 120 Loss: 0.006831169128417969\n",
            "Epoch: 47 Batch: 140 Loss: 8.96453821042087e-06\n",
            "Epoch: 47 Batch: 160 Loss: 0.17452840507030487\n",
            "Epoch: 47 Batch: 180 Loss: 0.0006006240728311241\n",
            "Epoch: 47 Batch: 200 Loss: 0.0007240295526571572\n",
            "Epoch: 47 Batch: 220 Loss: 0.013405132107436657\n",
            "Epoch: 47 Batch: 240 Loss: 0.0011464118724688888\n",
            "Epoch: 47 Batch: 260 Loss: 0.0\n",
            "Epoch: 47 Batch: 280 Loss: 0.0023584365844726562\n",
            "Epoch: 47 Batch: 300 Loss: 4.38690176451928e-06\n",
            "Epoch: 47 Batch: 320 Loss: 0.0006145477527752519\n",
            "Epoch: 47 Batch: 340 Loss: 0.0031336783431470394\n",
            "Epoch: 47 Batch: 360 Loss: 9.632110595703125e-05\n",
            "Epoch: 47 Batch: 380 Loss: 0.7499570846557617\n",
            "Epoch: 47 Batch: 400 Loss: 2.86102294921875e-05\n",
            "Epoch: 47 Batch: 420 Loss: 0.2937399744987488\n",
            "Epoch: 47 Batch: 440 Loss: 0.06215381622314453\n",
            "Epoch: 47 Batch: 460 Loss: 0.008745384402573109\n",
            "Epoch: 47 Batch: 480 Loss: 0.0008236885187216103\n",
            "Epoch: 47 Batch: 500 Loss: 0.026535987854003906\n",
            "Epoch: 47 Batch: 520 Loss: 6.10351571594947e-06\n",
            "Epoch: 47 Batch: 540 Loss: 9.803772263694555e-05\n",
            "Epoch: 47 Batch: 560 Loss: 7.24792471373803e-06\n",
            "Epoch: 47 Batch: 580 Loss: 0.0020170211791992188\n",
            "Epoch: 47 Batch: 600 Loss: 0.001177883124910295\n",
            "Epoch: 47 Batch: 620 Loss: 0.08363509178161621\n",
            "Epoch: 47 Batch: 640 Loss: 0.0018235206371173263\n",
            "Epoch: 47 Batch: 660 Loss: 0.33813318610191345\n",
            "Epoch: 47 Batch: 680 Loss: 0.007525444030761719\n",
            "Epoch: 47 Batch: 700 Loss: 0.000979518867097795\n",
            "Epoch: 47 Batch: 720 Loss: 0.1583690643310547\n",
            "Epoch: 47 Batch: 740 Loss: 0.0006606102106161416\n",
            "Epoch: 47 Batch: 760 Loss: 0.003748226212337613\n",
            "Epoch: 47 Batch: 780 Loss: 0.0005643844488076866\n",
            "Epoch: 47 Batch: 800 Loss: 0.0017843246459960938\n",
            "Epoch: 47 Batch: 820 Loss: 0.0019829750526696444\n",
            "Epoch: 47 Batch: 840 Loss: 0.001409244490787387\n",
            "Epoch: 47 Batch: 860 Loss: 0.5994150042533875\n",
            "Epoch: 47 Batch: 880 Loss: 0.0010852813720703125\n",
            "Epoch: 47 Batch: 900 Loss: 0.0014064789284020662\n",
            "Epoch: 47 Batch: 920 Loss: 2.0382847785949707\n",
            "Epoch: 47 Batch: 940 Loss: 0.00919108372181654\n",
            "Epoch: 47 Batch: 960 Loss: 2.040863000729587e-05\n",
            "Epoch: 47 Batch: 980 Loss: 0.006007957272231579\n",
            "Epoch: 48 Batch: 0 Loss: 0.013492679223418236\n",
            "Epoch: 48 Batch: 20 Loss: 0.00016641616821289062\n",
            "Epoch: 48 Batch: 40 Loss: 4.844665454584174e-05\n",
            "Epoch: 48 Batch: 60 Loss: 0.023697663098573685\n",
            "Epoch: 48 Batch: 80 Loss: 1.3237022161483765\n",
            "Epoch: 48 Batch: 100 Loss: 0.005819892976433039\n",
            "Epoch: 48 Batch: 120 Loss: 0.0008717536693438888\n",
            "Epoch: 48 Batch: 140 Loss: 0.0051708221435546875\n",
            "Epoch: 48 Batch: 160 Loss: 4.57763690064894e-06\n",
            "Epoch: 48 Batch: 180 Loss: 0.05142617225646973\n",
            "Epoch: 48 Batch: 200 Loss: 0.042855072766542435\n",
            "Epoch: 48 Batch: 220 Loss: 8.01086389401462e-06\n",
            "Epoch: 48 Batch: 240 Loss: 0.0005767822149209678\n",
            "Epoch: 48 Batch: 260 Loss: 0.0007527351262979209\n",
            "Epoch: 48 Batch: 280 Loss: 0.034781504422426224\n",
            "Epoch: 48 Batch: 300 Loss: 0.0047134398482739925\n",
            "Epoch: 48 Batch: 320 Loss: 0.20507927238941193\n",
            "Epoch: 48 Batch: 340 Loss: 0.004393577575683594\n",
            "Epoch: 48 Batch: 360 Loss: 0.015784312039613724\n",
            "Epoch: 48 Batch: 380 Loss: 0.025409698486328125\n",
            "Epoch: 48 Batch: 400 Loss: 0.004469776060432196\n",
            "Epoch: 48 Batch: 420 Loss: 0.003582477569580078\n",
            "Epoch: 48 Batch: 440 Loss: 0.0005075454828329384\n",
            "Epoch: 48 Batch: 460 Loss: 0.012324619106948376\n",
            "Epoch: 48 Batch: 480 Loss: 7.953643944347277e-05\n",
            "Epoch: 48 Batch: 500 Loss: 1.945495569088962e-05\n",
            "Epoch: 48 Batch: 520 Loss: 0.0014332771534100175\n",
            "Epoch: 48 Batch: 540 Loss: 0.00040836335392668843\n",
            "Epoch: 48 Batch: 560 Loss: 0.0032868385314941406\n",
            "Epoch: 48 Batch: 580 Loss: 0.00032901763916015625\n",
            "Epoch: 48 Batch: 600 Loss: 0.3991384506225586\n",
            "Epoch: 48 Batch: 620 Loss: 0.0008417129283770919\n",
            "Epoch: 48 Batch: 640 Loss: 3.623962356869015e-06\n",
            "Epoch: 48 Batch: 660 Loss: 0.00012235641770530492\n",
            "Epoch: 48 Batch: 680 Loss: 8.640289161121473e-05\n",
            "Epoch: 48 Batch: 700 Loss: 0.22593355178833008\n",
            "Epoch: 48 Batch: 720 Loss: 0.011578750796616077\n",
            "Epoch: 48 Batch: 740 Loss: 0.0018768310546875\n",
            "Epoch: 48 Batch: 760 Loss: 0.0009913444519042969\n",
            "Epoch: 48 Batch: 780 Loss: 0.0036798478104174137\n",
            "Epoch: 48 Batch: 800 Loss: 0.0006419181590899825\n",
            "Epoch: 48 Batch: 820 Loss: 0.0007854461437091231\n",
            "Epoch: 48 Batch: 840 Loss: 0.0008718490717001259\n",
            "Epoch: 48 Batch: 860 Loss: 0.0003472328244242817\n",
            "Epoch: 48 Batch: 880 Loss: 0.09908322989940643\n",
            "Epoch: 48 Batch: 900 Loss: 0.0016034126747399569\n",
            "Epoch: 48 Batch: 920 Loss: 0.017863083630800247\n",
            "Epoch: 48 Batch: 940 Loss: 0.010839462280273438\n",
            "Epoch: 48 Batch: 960 Loss: 0.00021200180344749242\n",
            "Epoch: 48 Batch: 980 Loss: 0.0009472846868447959\n",
            "Epoch: 49 Batch: 0 Loss: 0.011282634921371937\n",
            "Epoch: 49 Batch: 20 Loss: 0.004762554075568914\n",
            "Epoch: 49 Batch: 40 Loss: 0.004271316342055798\n",
            "Epoch: 49 Batch: 60 Loss: 0.015338802710175514\n",
            "Epoch: 49 Batch: 80 Loss: 0.0024097443092614412\n",
            "Epoch: 49 Batch: 100 Loss: 0.0014358520274981856\n",
            "Epoch: 49 Batch: 120 Loss: 0.0003647804260253906\n",
            "Epoch: 49 Batch: 140 Loss: 0.015078830532729626\n",
            "Epoch: 49 Batch: 160 Loss: 0.027832459658384323\n",
            "Epoch: 49 Batch: 180 Loss: 0.010911178775131702\n",
            "Epoch: 49 Batch: 200 Loss: 0.02268042601644993\n",
            "Epoch: 49 Batch: 220 Loss: 0.01701812818646431\n",
            "Epoch: 49 Batch: 240 Loss: 0.0003510475216899067\n",
            "Epoch: 49 Batch: 260 Loss: 0.011661911383271217\n",
            "Epoch: 49 Batch: 280 Loss: 0.0002563476446084678\n",
            "Epoch: 49 Batch: 300 Loss: 0.007029247470200062\n",
            "Epoch: 49 Batch: 320 Loss: 2.651214526849799e-05\n",
            "Epoch: 49 Batch: 340 Loss: 0.000590419746004045\n",
            "Epoch: 49 Batch: 360 Loss: 0.0008698463207110763\n",
            "Epoch: 49 Batch: 380 Loss: 0.00010833740088855848\n",
            "Epoch: 49 Batch: 400 Loss: 0.008878612890839577\n",
            "Epoch: 49 Batch: 420 Loss: 0.00015716553025413305\n",
            "Epoch: 49 Batch: 440 Loss: 0.0002876281796488911\n",
            "Epoch: 49 Batch: 460 Loss: 0.0026638030540198088\n",
            "Epoch: 49 Batch: 480 Loss: 0.019197845831513405\n",
            "Epoch: 49 Batch: 500 Loss: 0.036135196685791016\n",
            "Epoch: 49 Batch: 520 Loss: 0.09600629657506943\n",
            "Epoch: 49 Batch: 540 Loss: 0.0013500213390216231\n",
            "Epoch: 49 Batch: 560 Loss: 0.011200618930161\n",
            "Epoch: 49 Batch: 580 Loss: 0.0005215645069256425\n",
            "Epoch: 49 Batch: 600 Loss: 0.01581430435180664\n",
            "Epoch: 49 Batch: 620 Loss: 0.002693557646125555\n",
            "Epoch: 49 Batch: 640 Loss: 0.0003531455877237022\n",
            "Epoch: 49 Batch: 660 Loss: 0.004147720523178577\n",
            "Epoch: 49 Batch: 680 Loss: 0.059742022305727005\n",
            "Epoch: 49 Batch: 700 Loss: 1.621246337890625e-05\n",
            "Epoch: 49 Batch: 720 Loss: 2.231597864010837e-05\n",
            "Epoch: 49 Batch: 740 Loss: 0.0028166770935058594\n",
            "Epoch: 49 Batch: 760 Loss: 7.572174217784777e-05\n",
            "Epoch: 49 Batch: 780 Loss: 0.00014009475125931203\n",
            "Epoch: 49 Batch: 800 Loss: 0.047498129308223724\n",
            "Epoch: 49 Batch: 820 Loss: 0.0002728462277445942\n",
            "Epoch: 49 Batch: 840 Loss: 0.0008206367492675781\n",
            "Epoch: 49 Batch: 860 Loss: 0.00858240108937025\n",
            "Epoch: 49 Batch: 880 Loss: 0.004839801695197821\n",
            "Epoch: 49 Batch: 900 Loss: 4.95910626341356e-06\n",
            "Epoch: 49 Batch: 920 Loss: 0.0018773078918457031\n",
            "Epoch: 49 Batch: 940 Loss: 1.1421500444412231\n",
            "Epoch: 49 Batch: 960 Loss: 0.000727748847566545\n",
            "Epoch: 49 Batch: 980 Loss: 0.0010081290965899825\n",
            "Epoch: 50 Batch: 0 Loss: 0.011105060577392578\n",
            "Epoch: 50 Batch: 20 Loss: 0.0018925666809082031\n",
            "Epoch: 50 Batch: 40 Loss: 0.004766273312270641\n",
            "Epoch: 50 Batch: 60 Loss: 3.623962356869015e-06\n",
            "Epoch: 50 Batch: 80 Loss: 0.001100730849429965\n",
            "Epoch: 50 Batch: 100 Loss: 0.00013818740262649953\n",
            "Epoch: 50 Batch: 120 Loss: 0.01257467269897461\n",
            "Epoch: 50 Batch: 140 Loss: 0.01412048377096653\n",
            "Epoch: 50 Batch: 160 Loss: 0.00018987656221725047\n",
            "Epoch: 50 Batch: 180 Loss: 0.00031948089599609375\n",
            "Epoch: 50 Batch: 200 Loss: 0.0005303382640704513\n",
            "Epoch: 50 Batch: 220 Loss: 0.00012769698514603078\n",
            "Epoch: 50 Batch: 240 Loss: 0.0005727767711505294\n",
            "Epoch: 50 Batch: 260 Loss: 0.0004445076046977192\n",
            "Epoch: 50 Batch: 280 Loss: 0.08758725970983505\n",
            "Epoch: 50 Batch: 300 Loss: 0.009795474819839\n",
            "Epoch: 50 Batch: 320 Loss: 0.00016050339036155492\n",
            "Epoch: 50 Batch: 340 Loss: 0.005502033047378063\n",
            "Epoch: 50 Batch: 360 Loss: 0.0005909919855184853\n",
            "Epoch: 50 Batch: 380 Loss: 8.58306884765625e-06\n",
            "Epoch: 50 Batch: 400 Loss: 0.000125885009765625\n",
            "Epoch: 50 Batch: 420 Loss: 0.011858558282256126\n",
            "Epoch: 50 Batch: 440 Loss: 0.010995578952133656\n",
            "Epoch: 50 Batch: 460 Loss: 0.003946208860725164\n",
            "Epoch: 50 Batch: 480 Loss: 0.045386217534542084\n",
            "Epoch: 50 Batch: 500 Loss: 0.0011712074046954513\n",
            "Epoch: 50 Batch: 520 Loss: 0.0002811431768350303\n",
            "Epoch: 50 Batch: 540 Loss: 2.47955313170678e-06\n",
            "Epoch: 50 Batch: 560 Loss: 1.8215179807157256e-05\n",
            "Epoch: 50 Batch: 580 Loss: 0.0013620376121252775\n",
            "Epoch: 50 Batch: 600 Loss: 0.0034749030601233244\n",
            "Epoch: 50 Batch: 620 Loss: 0.0005423545953817666\n",
            "Epoch: 50 Batch: 640 Loss: 0.18212890625\n",
            "Epoch: 50 Batch: 660 Loss: 0.03126249462366104\n",
            "Epoch: 50 Batch: 680 Loss: 0.036179304122924805\n",
            "Epoch: 50 Batch: 700 Loss: 0.004619502928107977\n",
            "Epoch: 50 Batch: 720 Loss: 0.0030221939086914062\n",
            "Epoch: 50 Batch: 740 Loss: 0.0159741397947073\n",
            "Epoch: 50 Batch: 760 Loss: 0.010256337933242321\n",
            "Epoch: 50 Batch: 780 Loss: 0.0004076957702636719\n",
            "Epoch: 50 Batch: 800 Loss: 0.0002445220889057964\n",
            "Epoch: 50 Batch: 820 Loss: 0.006465244106948376\n",
            "Epoch: 50 Batch: 840 Loss: 0.006002998445183039\n",
            "Epoch: 50 Batch: 860 Loss: 0.0005365371471270919\n",
            "Epoch: 50 Batch: 880 Loss: 1.659393274167087e-05\n",
            "Epoch: 50 Batch: 900 Loss: 0.028502512723207474\n",
            "Epoch: 50 Batch: 920 Loss: 0.021124649792909622\n",
            "Epoch: 50 Batch: 940 Loss: 0.02366657182574272\n",
            "Epoch: 50 Batch: 960 Loss: 0.029545020312070847\n",
            "Epoch: 50 Batch: 980 Loss: 0.0010781288146972656\n",
            "Epoch: 51 Batch: 0 Loss: 0.12306632846593857\n",
            "Epoch: 51 Batch: 20 Loss: 0.00040435791015625\n",
            "Epoch: 51 Batch: 40 Loss: 4.39643845311366e-05\n",
            "Epoch: 51 Batch: 60 Loss: 0.00025177001953125\n",
            "Epoch: 51 Batch: 80 Loss: 7.43865984986769e-06\n",
            "Epoch: 51 Batch: 100 Loss: 1.087188684323337e-05\n",
            "Epoch: 51 Batch: 120 Loss: 0.38879114389419556\n",
            "Epoch: 51 Batch: 140 Loss: 0.01271600741893053\n",
            "Epoch: 51 Batch: 160 Loss: 0.00018959045701194555\n",
            "Epoch: 51 Batch: 180 Loss: 0.7319677472114563\n",
            "Epoch: 51 Batch: 200 Loss: 0.0026116371154785156\n",
            "Epoch: 51 Batch: 220 Loss: 0.005863285157829523\n",
            "Epoch: 51 Batch: 240 Loss: 0.007623195648193359\n",
            "Epoch: 51 Batch: 260 Loss: 0.00035648344783112407\n",
            "Epoch: 51 Batch: 280 Loss: 0.00042238234891556203\n",
            "Epoch: 51 Batch: 300 Loss: 0.0006685256958007812\n",
            "Epoch: 51 Batch: 320 Loss: 0.0016657828819006681\n",
            "Epoch: 51 Batch: 340 Loss: 1.6689300537109375e-05\n",
            "Epoch: 51 Batch: 360 Loss: 0.016491031274199486\n",
            "Epoch: 51 Batch: 380 Loss: 0.28805476427078247\n",
            "Epoch: 51 Batch: 400 Loss: 0.022562742233276367\n",
            "Epoch: 51 Batch: 420 Loss: 5.722046125811175e-07\n",
            "Epoch: 51 Batch: 440 Loss: 0.000827789306640625\n",
            "Epoch: 51 Batch: 460 Loss: 0.0008394241449423134\n",
            "Epoch: 51 Batch: 480 Loss: 7.762909081066027e-05\n",
            "Epoch: 51 Batch: 500 Loss: 0.00011386871483409777\n",
            "Epoch: 51 Batch: 520 Loss: 0.0010819435119628906\n",
            "Epoch: 51 Batch: 540 Loss: 0.021266842260956764\n",
            "Epoch: 51 Batch: 560 Loss: 0.006070995237678289\n",
            "Epoch: 51 Batch: 580 Loss: 0.0001770019589457661\n",
            "Epoch: 51 Batch: 600 Loss: 0.3262019157409668\n",
            "Epoch: 51 Batch: 620 Loss: 0.018239308148622513\n",
            "Epoch: 51 Batch: 640 Loss: 0.0018958091968670487\n",
            "Epoch: 51 Batch: 660 Loss: 0.014057779684662819\n",
            "Epoch: 51 Batch: 680 Loss: 6.599425978492945e-05\n",
            "Epoch: 51 Batch: 700 Loss: 0.000979518867097795\n",
            "Epoch: 51 Batch: 720 Loss: 8.821487426757812e-05\n",
            "Epoch: 51 Batch: 740 Loss: 0.07624640315771103\n",
            "Epoch: 51 Batch: 760 Loss: 0.0011623383034020662\n",
            "Epoch: 51 Batch: 780 Loss: 0.007309150882065296\n",
            "Epoch: 51 Batch: 800 Loss: 0.0010376930003985763\n",
            "Epoch: 51 Batch: 820 Loss: 0.20605334639549255\n",
            "Epoch: 51 Batch: 840 Loss: 0.09854743629693985\n",
            "Epoch: 51 Batch: 860 Loss: 0.009180259890854359\n",
            "Epoch: 51 Batch: 880 Loss: 0.0009962081676349044\n",
            "Epoch: 51 Batch: 900 Loss: 0.05718717724084854\n",
            "Epoch: 51 Batch: 920 Loss: 0.010065650567412376\n",
            "Epoch: 51 Batch: 940 Loss: 0.1265708953142166\n",
            "Epoch: 51 Batch: 960 Loss: 0.0011061668628826737\n",
            "Epoch: 51 Batch: 980 Loss: 0.005035209469497204\n",
            "Epoch: 52 Batch: 0 Loss: 0.0027506828773766756\n",
            "Epoch: 52 Batch: 20 Loss: 0.07876744121313095\n",
            "Epoch: 52 Batch: 40 Loss: 0.00048503876314498484\n",
            "Epoch: 52 Batch: 60 Loss: 5.53131103515625e-05\n",
            "Epoch: 52 Batch: 80 Loss: 0.0026514052879065275\n",
            "Epoch: 52 Batch: 100 Loss: 0.0001712799130473286\n",
            "Epoch: 52 Batch: 120 Loss: 0.012681007385253906\n",
            "Epoch: 52 Batch: 140 Loss: 0.00034780503483489156\n",
            "Epoch: 52 Batch: 160 Loss: 0.00010919570922851562\n",
            "Epoch: 52 Batch: 180 Loss: 4.38690185546875e-05\n",
            "Epoch: 52 Batch: 200 Loss: 0.0008083343273028731\n",
            "Epoch: 52 Batch: 220 Loss: 3.623962356869015e-06\n",
            "Epoch: 52 Batch: 240 Loss: 9.250640869140625e-05\n",
            "Epoch: 52 Batch: 260 Loss: 0.00111560826189816\n",
            "Epoch: 52 Batch: 280 Loss: 0.00622978201135993\n",
            "Epoch: 52 Batch: 300 Loss: 0.0881948471069336\n",
            "Epoch: 52 Batch: 320 Loss: 0.09499263763427734\n",
            "Epoch: 52 Batch: 340 Loss: 0.00043087004451081157\n",
            "Epoch: 52 Batch: 360 Loss: 0.0001510620058979839\n",
            "Epoch: 52 Batch: 380 Loss: 0.0006290435558184981\n",
            "Epoch: 52 Batch: 400 Loss: 0.0003131866396870464\n",
            "Epoch: 52 Batch: 420 Loss: 0.00450058002024889\n",
            "Epoch: 52 Batch: 440 Loss: 0.0002865791320800781\n",
            "Epoch: 52 Batch: 460 Loss: 0.0001789093075785786\n",
            "Epoch: 52 Batch: 480 Loss: 0.009737109765410423\n",
            "Epoch: 52 Batch: 500 Loss: 0.0048618316650390625\n",
            "Epoch: 52 Batch: 520 Loss: 0.00516090402379632\n",
            "Epoch: 52 Batch: 540 Loss: 0.0003170967102050781\n",
            "Epoch: 52 Batch: 560 Loss: 1.945495569088962e-05\n",
            "Epoch: 52 Batch: 580 Loss: 0.028789091855287552\n",
            "Epoch: 52 Batch: 600 Loss: 0.00014238357834983617\n",
            "Epoch: 52 Batch: 620 Loss: 0.01973857916891575\n",
            "Epoch: 52 Batch: 640 Loss: 0.0077670575119555\n",
            "Epoch: 52 Batch: 660 Loss: 0.016939450055360794\n",
            "Epoch: 52 Batch: 680 Loss: 0.01132669486105442\n",
            "Epoch: 52 Batch: 700 Loss: 0.0028735636733472347\n",
            "Epoch: 52 Batch: 720 Loss: 0.00015993117995094508\n",
            "Epoch: 52 Batch: 740 Loss: 0.0025777816772460938\n",
            "Epoch: 52 Batch: 760 Loss: 0.0010489464038982987\n",
            "Epoch: 52 Batch: 780 Loss: 0.00025157927302643657\n",
            "Epoch: 52 Batch: 800 Loss: 0.0008437156793661416\n",
            "Epoch: 52 Batch: 820 Loss: 0.020861148834228516\n",
            "Epoch: 52 Batch: 840 Loss: 0.00010395050048828125\n",
            "Epoch: 52 Batch: 860 Loss: 0.0011815071338787675\n",
            "Epoch: 52 Batch: 880 Loss: 0.003768062684684992\n",
            "Epoch: 52 Batch: 900 Loss: 0.0033753395546227694\n",
            "Epoch: 52 Batch: 920 Loss: 0.01525726355612278\n",
            "Epoch: 52 Batch: 940 Loss: 0.0074787139892578125\n",
            "Epoch: 52 Batch: 960 Loss: 0.00018491744413040578\n",
            "Epoch: 52 Batch: 980 Loss: 0.00017366409883834422\n",
            "Epoch: 53 Batch: 0 Loss: 0.0002208709775004536\n",
            "Epoch: 53 Batch: 20 Loss: 0.004073905758559704\n",
            "Epoch: 53 Batch: 40 Loss: 0.0004295349062886089\n",
            "Epoch: 53 Batch: 60 Loss: 0.014756202697753906\n",
            "Epoch: 53 Batch: 80 Loss: 0.0003741264226846397\n",
            "Epoch: 53 Batch: 100 Loss: 1.144409225162235e-06\n",
            "Epoch: 53 Batch: 120 Loss: 0.15868672728538513\n",
            "Epoch: 53 Batch: 140 Loss: 0.000522613525390625\n",
            "Epoch: 53 Batch: 160 Loss: 0.010211372748017311\n",
            "Epoch: 53 Batch: 180 Loss: 0.00035772324190475047\n",
            "Epoch: 53 Batch: 200 Loss: 0.005301475524902344\n",
            "Epoch: 53 Batch: 220 Loss: 0.17531690001487732\n",
            "Epoch: 53 Batch: 240 Loss: 0.8596265912055969\n",
            "Epoch: 53 Batch: 260 Loss: 0.0019940375350415707\n",
            "Epoch: 53 Batch: 280 Loss: 0.12908315658569336\n",
            "Epoch: 53 Batch: 300 Loss: 0.0040645599365234375\n",
            "Epoch: 53 Batch: 320 Loss: 0.00014591217041015625\n",
            "Epoch: 53 Batch: 340 Loss: 0.02654438093304634\n",
            "Epoch: 53 Batch: 360 Loss: 0.02485666237771511\n",
            "Epoch: 53 Batch: 380 Loss: 0.00010643005225574598\n",
            "Epoch: 53 Batch: 400 Loss: 0.00012912749662064016\n",
            "Epoch: 53 Batch: 420 Loss: 2.384185791015625e-05\n",
            "Epoch: 53 Batch: 440 Loss: 0.06983260810375214\n",
            "Epoch: 53 Batch: 460 Loss: 0.005984783172607422\n",
            "Epoch: 53 Batch: 480 Loss: 0.000823164009489119\n",
            "Epoch: 53 Batch: 500 Loss: 2.422332727292087e-05\n",
            "Epoch: 53 Batch: 520 Loss: 0.003328037215396762\n",
            "Epoch: 53 Batch: 540 Loss: 4.615783836925402e-05\n",
            "Epoch: 53 Batch: 560 Loss: 0.0014963150024414062\n",
            "Epoch: 53 Batch: 580 Loss: 0.014842796139419079\n",
            "Epoch: 53 Batch: 600 Loss: 2.0683372020721436\n",
            "Epoch: 53 Batch: 620 Loss: 0.05212344974279404\n",
            "Epoch: 53 Batch: 640 Loss: 0.3112166225910187\n",
            "Epoch: 53 Batch: 660 Loss: 0.00037384033203125\n",
            "Epoch: 53 Batch: 680 Loss: 0.00030803680419921875\n",
            "Epoch: 53 Batch: 700 Loss: 0.003002166748046875\n",
            "Epoch: 53 Batch: 720 Loss: 0.00977392215281725\n",
            "Epoch: 53 Batch: 740 Loss: 0.0009538650629110634\n",
            "Epoch: 53 Batch: 760 Loss: 0.017757892608642578\n",
            "Epoch: 53 Batch: 780 Loss: 0.09654407203197479\n",
            "Epoch: 53 Batch: 800 Loss: 0.05950770527124405\n",
            "Epoch: 53 Batch: 820 Loss: 4.100799560546875e-05\n",
            "Epoch: 53 Batch: 840 Loss: 0.0015463829040527344\n",
            "Epoch: 53 Batch: 860 Loss: 0.3161827027797699\n",
            "Epoch: 53 Batch: 880 Loss: 0.017966460436582565\n",
            "Epoch: 53 Batch: 900 Loss: 0.23432870209217072\n",
            "Epoch: 53 Batch: 920 Loss: 0.09713463485240936\n",
            "Epoch: 53 Batch: 940 Loss: 0.00010423660569358617\n",
            "Epoch: 53 Batch: 960 Loss: 0.11065497249364853\n",
            "Epoch: 53 Batch: 980 Loss: 0.2001422941684723\n",
            "Epoch: 54 Batch: 0 Loss: 4.367828296381049e-05\n",
            "Epoch: 54 Batch: 20 Loss: 0.00041980744572356343\n",
            "Epoch: 54 Batch: 40 Loss: 0.0002155303955078125\n",
            "Epoch: 54 Batch: 60 Loss: 0.17796143889427185\n",
            "Epoch: 54 Batch: 80 Loss: 0.0005923270946368575\n",
            "Epoch: 54 Batch: 100 Loss: 1.71661376953125e-05\n",
            "Epoch: 54 Batch: 120 Loss: 0.0010210036998614669\n",
            "Epoch: 54 Batch: 140 Loss: 0.011701202020049095\n",
            "Epoch: 54 Batch: 160 Loss: 0.0005596160772256553\n",
            "Epoch: 54 Batch: 180 Loss: 8.831024024402723e-05\n",
            "Epoch: 54 Batch: 200 Loss: 0.001161289168521762\n",
            "Epoch: 54 Batch: 220 Loss: 0.00028905869112350047\n",
            "Epoch: 54 Batch: 240 Loss: 0.021743010729551315\n",
            "Epoch: 54 Batch: 260 Loss: 0.8194368481636047\n",
            "Epoch: 54 Batch: 280 Loss: 0.00027408599271439016\n",
            "Epoch: 54 Batch: 300 Loss: 0.00035495759220793843\n",
            "Epoch: 54 Batch: 320 Loss: 0.002013778779655695\n",
            "Epoch: 54 Batch: 340 Loss: 0.0001811981201171875\n",
            "Epoch: 54 Batch: 360 Loss: 0.0006391525384970009\n",
            "Epoch: 54 Batch: 380 Loss: 0.0847073569893837\n",
            "Epoch: 54 Batch: 400 Loss: 0.000347137451171875\n",
            "Epoch: 54 Batch: 420 Loss: 0.0007459640619345009\n",
            "Epoch: 54 Batch: 440 Loss: 0.0005561828729696572\n",
            "Epoch: 54 Batch: 460 Loss: 5.7220458984375e-06\n",
            "Epoch: 54 Batch: 480 Loss: 0.0005455970531329513\n",
            "Epoch: 54 Batch: 500 Loss: 0.0012649536365643144\n",
            "Epoch: 54 Batch: 520 Loss: 0.0014417648781090975\n",
            "Epoch: 54 Batch: 540 Loss: 0.0012277603382244706\n",
            "Epoch: 54 Batch: 560 Loss: 0.0001255035458598286\n",
            "Epoch: 54 Batch: 580 Loss: 0.0018180847400799394\n",
            "Epoch: 54 Batch: 600 Loss: 0.0005898475646972656\n",
            "Epoch: 54 Batch: 620 Loss: 0.008035373874008656\n",
            "Epoch: 54 Batch: 640 Loss: 0.20727582275867462\n",
            "Epoch: 54 Batch: 660 Loss: 0.019693374633789062\n",
            "Epoch: 54 Batch: 680 Loss: 9.72747784544481e-06\n",
            "Epoch: 54 Batch: 700 Loss: 4.520416405284777e-05\n",
            "Epoch: 54 Batch: 720 Loss: 0.0005842208629474044\n",
            "Epoch: 54 Batch: 740 Loss: 0.4822036325931549\n",
            "Epoch: 54 Batch: 760 Loss: 0.10931777954101562\n",
            "Epoch: 54 Batch: 780 Loss: 0.0008272171253338456\n",
            "Epoch: 54 Batch: 800 Loss: 0.0019190788734704256\n",
            "Epoch: 54 Batch: 820 Loss: 1.182556115963962e-05\n",
            "Epoch: 54 Batch: 840 Loss: 0.0009984016651287675\n",
            "Epoch: 54 Batch: 860 Loss: 0.195753812789917\n",
            "Epoch: 54 Batch: 880 Loss: 0.0045791626907885075\n",
            "Epoch: 54 Batch: 900 Loss: 8.487701416015625e-05\n",
            "Epoch: 54 Batch: 920 Loss: 5.836486889165826e-05\n",
            "Epoch: 54 Batch: 940 Loss: 4.558563159662299e-05\n",
            "Epoch: 54 Batch: 960 Loss: 1.106262243411038e-05\n",
            "Epoch: 54 Batch: 980 Loss: 8.20159948489163e-06\n",
            "Epoch: 55 Batch: 0 Loss: 0.0008502959972247481\n",
            "Epoch: 55 Batch: 20 Loss: 6.256103370105848e-05\n",
            "Epoch: 55 Batch: 40 Loss: 0.00019979476928710938\n",
            "Epoch: 55 Batch: 60 Loss: 0.21662922203540802\n",
            "Epoch: 55 Batch: 80 Loss: 0.002793979598209262\n",
            "Epoch: 55 Batch: 100 Loss: 9.765625145519152e-05\n",
            "Epoch: 55 Batch: 120 Loss: 0.007874393835663795\n",
            "Epoch: 55 Batch: 140 Loss: 5.474090721691027e-05\n",
            "Epoch: 55 Batch: 160 Loss: 0.0024653435684740543\n",
            "Epoch: 55 Batch: 180 Loss: 0.0036861419212073088\n",
            "Epoch: 55 Batch: 200 Loss: 0.00027523041353560984\n",
            "Epoch: 55 Batch: 220 Loss: 0.005591678433120251\n",
            "Epoch: 55 Batch: 240 Loss: 0.021180152893066406\n",
            "Epoch: 55 Batch: 260 Loss: 0.3123685419559479\n",
            "Epoch: 55 Batch: 280 Loss: 0.0004104614199604839\n",
            "Epoch: 55 Batch: 300 Loss: 0.6731487512588501\n",
            "Epoch: 55 Batch: 320 Loss: 0.0007669448968954384\n",
            "Epoch: 55 Batch: 340 Loss: 7.629394644936838e-07\n",
            "Epoch: 55 Batch: 360 Loss: 0.0036674500443041325\n",
            "Epoch: 55 Batch: 380 Loss: 0.15512943267822266\n",
            "Epoch: 55 Batch: 400 Loss: 0.0006940841558389366\n",
            "Epoch: 55 Batch: 420 Loss: 0.0004661559942178428\n",
            "Epoch: 55 Batch: 440 Loss: 0.006793117616325617\n",
            "Epoch: 55 Batch: 460 Loss: 0.0008062362903729081\n",
            "Epoch: 55 Batch: 480 Loss: 0.0030486583709716797\n",
            "Epoch: 55 Batch: 500 Loss: 4.95910626341356e-06\n",
            "Epoch: 55 Batch: 520 Loss: 0.000576877617277205\n",
            "Epoch: 55 Batch: 540 Loss: 0.0036139488220214844\n",
            "Epoch: 55 Batch: 560 Loss: 1.158182144165039\n",
            "Epoch: 55 Batch: 580 Loss: 0.0026405334938317537\n",
            "Epoch: 55 Batch: 600 Loss: 0.0005718230968341231\n",
            "Epoch: 55 Batch: 620 Loss: 0.10001106560230255\n",
            "Epoch: 55 Batch: 640 Loss: 1.277923547604587e-05\n",
            "Epoch: 55 Batch: 660 Loss: 0.006703853607177734\n",
            "Epoch: 55 Batch: 680 Loss: 0.00018787384033203125\n",
            "Epoch: 55 Batch: 700 Loss: 0.0004976272466592491\n",
            "Epoch: 55 Batch: 720 Loss: 0.002521038055419922\n",
            "Epoch: 55 Batch: 740 Loss: 0.0017033576732501388\n",
            "Epoch: 55 Batch: 760 Loss: 0.10254392772912979\n",
            "Epoch: 55 Batch: 780 Loss: 0.0011108398903161287\n",
            "Epoch: 55 Batch: 800 Loss: 0.003993702121078968\n",
            "Epoch: 55 Batch: 820 Loss: 0.1495629847049713\n",
            "Epoch: 55 Batch: 840 Loss: 0.0008963585132732987\n",
            "Epoch: 55 Batch: 860 Loss: 0.0036857128143310547\n",
            "Epoch: 55 Batch: 880 Loss: 0.3686734139919281\n",
            "Epoch: 55 Batch: 900 Loss: 0.007013988681137562\n",
            "Epoch: 55 Batch: 920 Loss: 0.7865194082260132\n",
            "Epoch: 55 Batch: 940 Loss: 0.0647641196846962\n",
            "Epoch: 55 Batch: 960 Loss: 0.0012830734485760331\n",
            "Epoch: 55 Batch: 980 Loss: 0.04712648317217827\n",
            "Epoch: 56 Batch: 0 Loss: 0.03150172159075737\n",
            "Epoch: 56 Batch: 20 Loss: 0.02516651153564453\n",
            "Epoch: 56 Batch: 40 Loss: 7.572174217784777e-05\n",
            "Epoch: 56 Batch: 60 Loss: 0.0004883765941485763\n",
            "Epoch: 56 Batch: 80 Loss: 0.0007318496936932206\n",
            "Epoch: 56 Batch: 100 Loss: 0.00019569396681617945\n",
            "Epoch: 56 Batch: 120 Loss: 0.027326297014951706\n",
            "Epoch: 56 Batch: 140 Loss: 0.0005171775701455772\n",
            "Epoch: 56 Batch: 160 Loss: 0.035796403884887695\n",
            "Epoch: 56 Batch: 180 Loss: 0.04841756820678711\n",
            "Epoch: 56 Batch: 200 Loss: 0.00017194748215842992\n",
            "Epoch: 56 Batch: 220 Loss: 5.15937790623866e-05\n",
            "Epoch: 56 Batch: 240 Loss: 0.0006341934204101562\n",
            "Epoch: 56 Batch: 260 Loss: 0.927452564239502\n",
            "Epoch: 56 Batch: 280 Loss: 7.05719003235572e-06\n",
            "Epoch: 56 Batch: 300 Loss: 9.32693510549143e-05\n",
            "Epoch: 56 Batch: 320 Loss: 0.0009425163152627647\n",
            "Epoch: 56 Batch: 340 Loss: 0.019746208563447\n",
            "Epoch: 56 Batch: 360 Loss: 0.0004004478396382183\n",
            "Epoch: 56 Batch: 380 Loss: 5.378723290050402e-05\n",
            "Epoch: 56 Batch: 400 Loss: 1.583099401614163e-05\n",
            "Epoch: 56 Batch: 420 Loss: 0.0071655274368822575\n",
            "Epoch: 56 Batch: 440 Loss: 0.06648483127355576\n",
            "Epoch: 56 Batch: 460 Loss: 0.44033461809158325\n",
            "Epoch: 56 Batch: 480 Loss: 0.028189945966005325\n",
            "Epoch: 56 Batch: 500 Loss: 0.0036140442825853825\n",
            "Epoch: 56 Batch: 520 Loss: 0.0026018142234534025\n",
            "Epoch: 56 Batch: 540 Loss: 8.20159948489163e-06\n",
            "Epoch: 56 Batch: 560 Loss: 0.0001911163271870464\n",
            "Epoch: 56 Batch: 580 Loss: 0.0005456924554891884\n",
            "Epoch: 56 Batch: 600 Loss: 0.0005312919383868575\n",
            "Epoch: 56 Batch: 620 Loss: 0.00286273960955441\n",
            "Epoch: 56 Batch: 640 Loss: 0.004094123840332031\n",
            "Epoch: 56 Batch: 660 Loss: 0.7841287851333618\n",
            "Epoch: 56 Batch: 680 Loss: 2.86102294921875e-05\n",
            "Epoch: 56 Batch: 700 Loss: 0.0014904976123943925\n",
            "Epoch: 56 Batch: 720 Loss: 0.00016565322584938258\n",
            "Epoch: 56 Batch: 740 Loss: 0.00019321442232467234\n",
            "Epoch: 56 Batch: 760 Loss: 0.0001162529006251134\n",
            "Epoch: 56 Batch: 780 Loss: 0.0008710861438885331\n",
            "Epoch: 56 Batch: 800 Loss: 0.0001642227143747732\n",
            "Epoch: 56 Batch: 820 Loss: 0.5307633876800537\n",
            "Epoch: 56 Batch: 840 Loss: 0.1394929140806198\n",
            "Epoch: 56 Batch: 860 Loss: 0.018633175641298294\n",
            "Epoch: 56 Batch: 880 Loss: 0.0019941329956054688\n",
            "Epoch: 56 Batch: 900 Loss: 0.03502292558550835\n",
            "Epoch: 56 Batch: 920 Loss: 0.0002785682736430317\n",
            "Epoch: 56 Batch: 940 Loss: 2.841949390131049e-05\n",
            "Epoch: 56 Batch: 960 Loss: 0.00024814606877043843\n",
            "Epoch: 56 Batch: 980 Loss: 0.02931680716574192\n",
            "Epoch: 57 Batch: 0 Loss: 0.0004536628839559853\n",
            "Epoch: 57 Batch: 20 Loss: 4.901886131847277e-05\n",
            "Epoch: 57 Batch: 40 Loss: 0.0007941246149130166\n",
            "Epoch: 57 Batch: 60 Loss: 1.659393274167087e-05\n",
            "Epoch: 57 Batch: 80 Loss: 0.0025037764571607113\n",
            "Epoch: 57 Batch: 100 Loss: 0.005187130067497492\n",
            "Epoch: 57 Batch: 120 Loss: 0.00035915375337935984\n",
            "Epoch: 57 Batch: 140 Loss: 0.014852285385131836\n",
            "Epoch: 57 Batch: 160 Loss: 0.0006109237438067794\n",
            "Epoch: 57 Batch: 180 Loss: 0.00044612883357331157\n",
            "Epoch: 57 Batch: 200 Loss: 0.00018215179443359375\n",
            "Epoch: 57 Batch: 220 Loss: 0.019664382562041283\n",
            "Epoch: 57 Batch: 240 Loss: 8.707046799827367e-05\n",
            "Epoch: 57 Batch: 260 Loss: 0.00026426315889693797\n",
            "Epoch: 57 Batch: 280 Loss: 0.00012168884131824598\n",
            "Epoch: 57 Batch: 300 Loss: 6.10351571594947e-06\n",
            "Epoch: 57 Batch: 320 Loss: 0.0626211166381836\n",
            "Epoch: 57 Batch: 340 Loss: 0.0\n",
            "Epoch: 57 Batch: 360 Loss: 7.114410254871473e-05\n",
            "Epoch: 57 Batch: 380 Loss: 5.14984139954322e-06\n",
            "Epoch: 57 Batch: 400 Loss: 0.00044145583524368703\n",
            "Epoch: 57 Batch: 420 Loss: 0.04375886917114258\n",
            "Epoch: 57 Batch: 440 Loss: 9.403228614246473e-05\n",
            "Epoch: 57 Batch: 460 Loss: 3.814697265625e-06\n",
            "Epoch: 57 Batch: 480 Loss: 0.0003108978271484375\n",
            "Epoch: 57 Batch: 500 Loss: 0.0004744529724121094\n",
            "Epoch: 57 Batch: 520 Loss: 0.00011672973778331652\n",
            "Epoch: 57 Batch: 540 Loss: 0.0009013175731524825\n",
            "Epoch: 57 Batch: 560 Loss: 0.2516699731349945\n",
            "Epoch: 57 Batch: 580 Loss: 0.0022712708450853825\n",
            "Epoch: 57 Batch: 600 Loss: 0.006623172666877508\n",
            "Epoch: 57 Batch: 620 Loss: 0.0008100509876385331\n",
            "Epoch: 57 Batch: 640 Loss: 4.272460864740424e-05\n",
            "Epoch: 57 Batch: 660 Loss: 0.0012290955055505037\n",
            "Epoch: 57 Batch: 680 Loss: 8.01086389401462e-06\n",
            "Epoch: 57 Batch: 700 Loss: 0.04091787338256836\n",
            "Epoch: 57 Batch: 720 Loss: 0.0010099411010742188\n",
            "Epoch: 57 Batch: 740 Loss: 0.0018814087379723787\n",
            "Epoch: 57 Batch: 760 Loss: 0.0002629280206747353\n",
            "Epoch: 57 Batch: 780 Loss: 6.790160841774195e-05\n",
            "Epoch: 57 Batch: 800 Loss: 0.005608940031379461\n",
            "Epoch: 57 Batch: 820 Loss: 0.007783412933349609\n",
            "Epoch: 57 Batch: 840 Loss: 3.719329833984375e-05\n",
            "Epoch: 57 Batch: 860 Loss: 0.0688878521323204\n",
            "Epoch: 57 Batch: 880 Loss: 0.0005534171941690147\n",
            "Epoch: 57 Batch: 900 Loss: 0.0002880096435546875\n",
            "Epoch: 57 Batch: 920 Loss: 3.623962356869015e-06\n",
            "Epoch: 57 Batch: 940 Loss: 0.0004008293035440147\n",
            "Epoch: 57 Batch: 960 Loss: 8.230209641624242e-05\n",
            "Epoch: 57 Batch: 980 Loss: 1.5258789289873675e-06\n",
            "Epoch: 58 Batch: 0 Loss: 4.787445141118951e-05\n",
            "Epoch: 58 Batch: 20 Loss: 1.52587890625e-05\n",
            "Epoch: 58 Batch: 40 Loss: 0.3152337670326233\n",
            "Epoch: 58 Batch: 60 Loss: 0.2507377564907074\n",
            "Epoch: 58 Batch: 80 Loss: 0.001897525740787387\n",
            "Epoch: 58 Batch: 100 Loss: 0.00015306472778320312\n",
            "Epoch: 58 Batch: 120 Loss: 0.00010147094872081652\n",
            "Epoch: 58 Batch: 140 Loss: 5.302429053699598e-05\n",
            "Epoch: 58 Batch: 160 Loss: 0.06859598308801651\n",
            "Epoch: 58 Batch: 180 Loss: 0.010518645867705345\n",
            "Epoch: 58 Batch: 200 Loss: 5.722046125811175e-07\n",
            "Epoch: 58 Batch: 220 Loss: 5.34057608092553e-06\n",
            "Epoch: 58 Batch: 240 Loss: 0.0009561538463458419\n",
            "Epoch: 58 Batch: 260 Loss: 0.020075082778930664\n",
            "Epoch: 58 Batch: 280 Loss: 0.014473152346909046\n",
            "Epoch: 58 Batch: 300 Loss: 8.525848534191027e-05\n",
            "Epoch: 58 Batch: 320 Loss: 0.001998329069465399\n",
            "Epoch: 58 Batch: 340 Loss: 0.00035495759220793843\n",
            "Epoch: 58 Batch: 360 Loss: 0.00017795563326217234\n",
            "Epoch: 58 Batch: 380 Loss: 8.20159948489163e-06\n",
            "Epoch: 58 Batch: 400 Loss: 0.515864372253418\n",
            "Epoch: 58 Batch: 420 Loss: 0.08206892013549805\n",
            "Epoch: 58 Batch: 440 Loss: 0.0005218505975790322\n",
            "Epoch: 58 Batch: 460 Loss: 0.0002480506955180317\n",
            "Epoch: 58 Batch: 480 Loss: 0.0004413604619912803\n",
            "Epoch: 58 Batch: 500 Loss: 0.01325225830078125\n",
            "Epoch: 58 Batch: 520 Loss: 8.420944504905492e-05\n",
            "Epoch: 58 Batch: 540 Loss: 0.00013465881056617945\n",
            "Epoch: 58 Batch: 560 Loss: 2.174377368646674e-05\n",
            "Epoch: 58 Batch: 580 Loss: 0.0016172409523278475\n",
            "Epoch: 58 Batch: 600 Loss: 0.014873027801513672\n",
            "Epoch: 58 Batch: 620 Loss: 0.00011978149268543348\n",
            "Epoch: 58 Batch: 640 Loss: 4.1675568354548886e-05\n",
            "Epoch: 58 Batch: 660 Loss: 1.754760705807712e-05\n",
            "Epoch: 58 Batch: 680 Loss: 0.060704898089170456\n",
            "Epoch: 58 Batch: 700 Loss: 0.004079294390976429\n",
            "Epoch: 58 Batch: 720 Loss: 4.57763690064894e-06\n",
            "Epoch: 58 Batch: 740 Loss: 0.0014358520274981856\n",
            "Epoch: 58 Batch: 760 Loss: 0.00013446807861328125\n",
            "Epoch: 58 Batch: 780 Loss: 0.0011137962574139237\n",
            "Epoch: 58 Batch: 800 Loss: 0.0010742187732830644\n",
            "Epoch: 58 Batch: 820 Loss: 0.0002405166596872732\n",
            "Epoch: 58 Batch: 840 Loss: 0.06815347820520401\n",
            "Epoch: 58 Batch: 860 Loss: 0.8919988870620728\n",
            "Epoch: 58 Batch: 880 Loss: 0.00017404556274414062\n",
            "Epoch: 58 Batch: 900 Loss: 0.00197429652325809\n",
            "Epoch: 58 Batch: 920 Loss: 0.0015988349914550781\n",
            "Epoch: 58 Batch: 940 Loss: 0.06392750889062881\n",
            "Epoch: 58 Batch: 960 Loss: 6.10351571594947e-06\n",
            "Epoch: 58 Batch: 980 Loss: 0.0003108978271484375\n",
            "Epoch: 59 Batch: 0 Loss: 0.00026531220646575093\n",
            "Epoch: 59 Batch: 20 Loss: 0.05592222139239311\n",
            "Epoch: 59 Batch: 40 Loss: 9.34600848268019e-06\n",
            "Epoch: 59 Batch: 60 Loss: 0.0020494461059570312\n",
            "Epoch: 59 Batch: 80 Loss: 0.00034036635770462453\n",
            "Epoch: 59 Batch: 100 Loss: 0.13375405967235565\n",
            "Epoch: 59 Batch: 120 Loss: 0.0001905441313283518\n",
            "Epoch: 59 Batch: 140 Loss: 0.014663887210190296\n",
            "Epoch: 59 Batch: 160 Loss: 8.468628220725805e-05\n",
            "Epoch: 59 Batch: 180 Loss: 0.0002846717834472656\n",
            "Epoch: 59 Batch: 200 Loss: 0.00013036727614235133\n",
            "Epoch: 59 Batch: 220 Loss: 0.00011386871483409777\n",
            "Epoch: 59 Batch: 240 Loss: 2.269744800287299e-05\n",
            "Epoch: 59 Batch: 260 Loss: 0.0008335113525390625\n",
            "Epoch: 59 Batch: 280 Loss: 0.025063704699277878\n",
            "Epoch: 59 Batch: 300 Loss: 0.020211314782500267\n",
            "Epoch: 59 Batch: 320 Loss: 3.204345557605848e-05\n",
            "Epoch: 59 Batch: 340 Loss: 0.0005371093866415322\n",
            "Epoch: 59 Batch: 360 Loss: 9.984969801735133e-05\n",
            "Epoch: 59 Batch: 380 Loss: 0.008760357275605202\n",
            "Epoch: 59 Batch: 400 Loss: 0.00018367767916060984\n",
            "Epoch: 59 Batch: 420 Loss: 0.004999828524887562\n",
            "Epoch: 59 Batch: 440 Loss: 0.0008547782781533897\n",
            "Epoch: 59 Batch: 460 Loss: 0.00036602019099518657\n",
            "Epoch: 59 Batch: 480 Loss: 0.0001031875581247732\n",
            "Epoch: 59 Batch: 500 Loss: 0.0013622284168377519\n",
            "Epoch: 59 Batch: 520 Loss: 0.00017108916654251516\n",
            "Epoch: 59 Batch: 540 Loss: 0.00012903213792014867\n",
            "Epoch: 59 Batch: 560 Loss: 0.0013028144603595138\n",
            "Epoch: 59 Batch: 580 Loss: 0.025061797350645065\n",
            "Epoch: 59 Batch: 600 Loss: 3.43322744811303e-06\n",
            "Epoch: 59 Batch: 620 Loss: 0.00039539337740279734\n",
            "Epoch: 59 Batch: 640 Loss: 0.06283502280712128\n",
            "Epoch: 59 Batch: 660 Loss: 7.019042823230848e-05\n",
            "Epoch: 59 Batch: 680 Loss: 0.002608871553093195\n",
            "Epoch: 59 Batch: 700 Loss: 0.022897720336914062\n",
            "Epoch: 59 Batch: 720 Loss: 3.032684253412299e-05\n",
            "Epoch: 59 Batch: 740 Loss: 1.487731969973538e-05\n",
            "Epoch: 59 Batch: 760 Loss: 0.05151405185461044\n",
            "Epoch: 59 Batch: 780 Loss: 0.002857780549675226\n",
            "Epoch: 59 Batch: 800 Loss: 0.030449580401182175\n",
            "Epoch: 59 Batch: 820 Loss: 1.7261505490751006e-05\n",
            "Epoch: 59 Batch: 840 Loss: 0.04012894630432129\n",
            "Epoch: 59 Batch: 860 Loss: 0.01420669537037611\n",
            "Epoch: 59 Batch: 880 Loss: 0.0006863594171591103\n",
            "Epoch: 59 Batch: 900 Loss: 0.12073326110839844\n",
            "Epoch: 59 Batch: 920 Loss: 0.008138465695083141\n",
            "Epoch: 59 Batch: 940 Loss: 0.006860828492790461\n",
            "Epoch: 59 Batch: 960 Loss: 0.0008752822759561241\n",
            "Epoch: 59 Batch: 980 Loss: 0.05013909190893173\n",
            "Epoch: 60 Batch: 0 Loss: 0.0025090693961828947\n",
            "Epoch: 60 Batch: 20 Loss: 0.007441329769790173\n",
            "Epoch: 60 Batch: 40 Loss: 0.0007171630859375\n",
            "Epoch: 60 Batch: 60 Loss: 0.0016608238220214844\n",
            "Epoch: 60 Batch: 80 Loss: 0.024388408288359642\n",
            "Epoch: 60 Batch: 100 Loss: 0.0014408112037926912\n",
            "Epoch: 60 Batch: 120 Loss: 0.013031005859375\n",
            "Epoch: 60 Batch: 140 Loss: 0.0008492469787597656\n",
            "Epoch: 60 Batch: 160 Loss: 0.0010526657570153475\n",
            "Epoch: 60 Batch: 180 Loss: 0.05036945268511772\n",
            "Epoch: 60 Batch: 200 Loss: 0.0007653236389160156\n",
            "Epoch: 60 Batch: 220 Loss: 0.02186555787920952\n",
            "Epoch: 60 Batch: 240 Loss: 0.010377312079071999\n",
            "Epoch: 60 Batch: 260 Loss: 0.0071563720703125\n",
            "Epoch: 60 Batch: 280 Loss: 0.024878596886992455\n",
            "Epoch: 60 Batch: 300 Loss: 0.001573753310367465\n",
            "Epoch: 60 Batch: 320 Loss: 0.009777164086699486\n",
            "Epoch: 60 Batch: 340 Loss: 0.0004968643188476562\n",
            "Epoch: 60 Batch: 360 Loss: 0.0018316268688067794\n",
            "Epoch: 60 Batch: 380 Loss: 0.010696887969970703\n",
            "Epoch: 60 Batch: 400 Loss: 0.06091775745153427\n",
            "Epoch: 60 Batch: 420 Loss: 6.427765038097277e-05\n",
            "Epoch: 60 Batch: 440 Loss: 3.623962356869015e-06\n",
            "Epoch: 60 Batch: 460 Loss: 0.0042511941865086555\n",
            "Epoch: 60 Batch: 480 Loss: 4.100799742445815e-06\n",
            "Epoch: 60 Batch: 500 Loss: 1.0776571035385132\n",
            "Epoch: 60 Batch: 520 Loss: 0.004994106478989124\n",
            "Epoch: 60 Batch: 540 Loss: 0.001001644181087613\n",
            "Epoch: 60 Batch: 560 Loss: 0.0018243789672851562\n",
            "Epoch: 60 Batch: 580 Loss: 0.13585452735424042\n",
            "Epoch: 60 Batch: 600 Loss: 0.0008311271667480469\n",
            "Epoch: 60 Batch: 620 Loss: 0.17737393081188202\n",
            "Epoch: 60 Batch: 640 Loss: 0.01864318922162056\n",
            "Epoch: 60 Batch: 660 Loss: 0.013651465997099876\n",
            "Epoch: 60 Batch: 680 Loss: 0.09741773456335068\n",
            "Epoch: 60 Batch: 700 Loss: 4.711151268566027e-05\n",
            "Epoch: 60 Batch: 720 Loss: 0.0011876106727868319\n",
            "Epoch: 60 Batch: 740 Loss: 0.0005544662708416581\n",
            "Epoch: 60 Batch: 760 Loss: 0.00593643169850111\n",
            "Epoch: 60 Batch: 780 Loss: 4.215240551275201e-05\n",
            "Epoch: 60 Batch: 800 Loss: 0.2690725326538086\n",
            "Epoch: 60 Batch: 820 Loss: 2.975463939947076e-05\n",
            "Epoch: 60 Batch: 840 Loss: 0.006085681729018688\n",
            "Epoch: 60 Batch: 860 Loss: 0.001711082411929965\n",
            "Epoch: 60 Batch: 880 Loss: 0.0006333350902423263\n",
            "Epoch: 60 Batch: 900 Loss: 0.0002518653927836567\n",
            "Epoch: 60 Batch: 920 Loss: 0.20976543426513672\n",
            "Epoch: 60 Batch: 940 Loss: 0.0020520209800451994\n",
            "Epoch: 60 Batch: 960 Loss: 0.00908050499856472\n",
            "Epoch: 60 Batch: 980 Loss: 0.002093028975650668\n",
            "Epoch: 61 Batch: 0 Loss: 0.0030676841270178556\n",
            "Epoch: 61 Batch: 20 Loss: 0.00047788620577193797\n",
            "Epoch: 61 Batch: 40 Loss: 0.00017633437528274953\n",
            "Epoch: 61 Batch: 60 Loss: 0.0037052154075354338\n",
            "Epoch: 61 Batch: 80 Loss: 0.021785354241728783\n",
            "Epoch: 61 Batch: 100 Loss: 0.000244140625\n",
            "Epoch: 61 Batch: 120 Loss: 0.00942382775247097\n",
            "Epoch: 61 Batch: 140 Loss: 0.0010088920826092362\n",
            "Epoch: 61 Batch: 160 Loss: 0.000331878662109375\n",
            "Epoch: 61 Batch: 180 Loss: 5.264282299322076e-05\n",
            "Epoch: 61 Batch: 200 Loss: 0.0002804756222758442\n",
            "Epoch: 61 Batch: 220 Loss: 0.00021524429030250758\n",
            "Epoch: 61 Batch: 240 Loss: 1.144409225162235e-06\n",
            "Epoch: 61 Batch: 260 Loss: 0.0464174747467041\n",
            "Epoch: 61 Batch: 280 Loss: 0.4406704008579254\n",
            "Epoch: 61 Batch: 300 Loss: 0.0006489753723144531\n",
            "Epoch: 61 Batch: 320 Loss: 0.00073919293936342\n",
            "Epoch: 61 Batch: 340 Loss: 0.006449699401855469\n",
            "Epoch: 61 Batch: 360 Loss: 8.506774611305445e-05\n",
            "Epoch: 61 Batch: 380 Loss: 0.00012617111497092992\n",
            "Epoch: 61 Batch: 400 Loss: 0.01251077651977539\n",
            "Epoch: 61 Batch: 420 Loss: 0.0009733199840411544\n",
            "Epoch: 61 Batch: 440 Loss: 2.337832450866699\n",
            "Epoch: 61 Batch: 460 Loss: 7.724762326688506e-06\n",
            "Epoch: 61 Batch: 480 Loss: 1.144409225162235e-06\n",
            "Epoch: 61 Batch: 500 Loss: 8.420944504905492e-05\n",
            "Epoch: 61 Batch: 520 Loss: 0.0005396843189373612\n",
            "Epoch: 61 Batch: 540 Loss: 0.009516621008515358\n",
            "Epoch: 61 Batch: 560 Loss: 0.020796965807676315\n",
            "Epoch: 61 Batch: 580 Loss: 0.06651858985424042\n",
            "Epoch: 61 Batch: 600 Loss: 0.01563706435263157\n",
            "Epoch: 61 Batch: 620 Loss: 0.2100975066423416\n",
            "Epoch: 61 Batch: 640 Loss: 0.0009496688726358116\n",
            "Epoch: 61 Batch: 660 Loss: 0.029442597180604935\n",
            "Epoch: 61 Batch: 680 Loss: 9.918212890625e-05\n",
            "Epoch: 61 Batch: 700 Loss: 0.002461052034050226\n",
            "Epoch: 61 Batch: 720 Loss: 1.4699485301971436\n",
            "Epoch: 61 Batch: 740 Loss: 0.0014405250549316406\n",
            "Epoch: 61 Batch: 760 Loss: 0.7546528577804565\n",
            "Epoch: 61 Batch: 780 Loss: 0.017728518694639206\n",
            "Epoch: 61 Batch: 800 Loss: 0.04379405826330185\n",
            "Epoch: 61 Batch: 820 Loss: 0.006472730543464422\n",
            "Epoch: 61 Batch: 840 Loss: 0.019175242632627487\n",
            "Epoch: 61 Batch: 860 Loss: 0.1254672110080719\n",
            "Epoch: 61 Batch: 880 Loss: 0.342752605676651\n",
            "Epoch: 61 Batch: 900 Loss: 0.13780879974365234\n",
            "Epoch: 61 Batch: 920 Loss: 7.705688767600805e-05\n",
            "Epoch: 61 Batch: 940 Loss: 0.05706815794110298\n",
            "Epoch: 61 Batch: 960 Loss: 0.054251957684755325\n",
            "Epoch: 61 Batch: 980 Loss: 0.00057306292001158\n",
            "Epoch: 62 Batch: 0 Loss: 0.0006421089055947959\n",
            "Epoch: 62 Batch: 20 Loss: 0.0002681732294149697\n",
            "Epoch: 62 Batch: 40 Loss: 0.3108018934726715\n",
            "Epoch: 62 Batch: 60 Loss: 0.003971958067268133\n",
            "Epoch: 62 Batch: 80 Loss: 2.212524486822076e-05\n",
            "Epoch: 62 Batch: 100 Loss: 0.00046329497126862407\n",
            "Epoch: 62 Batch: 120 Loss: 0.00014181136793922633\n",
            "Epoch: 62 Batch: 140 Loss: 0.0005847931024618447\n",
            "Epoch: 62 Batch: 160 Loss: 0.0013336181873455644\n",
            "Epoch: 62 Batch: 180 Loss: 6.69479341013357e-05\n",
            "Epoch: 62 Batch: 200 Loss: 0.0007228851318359375\n",
            "Epoch: 62 Batch: 220 Loss: 4.882812572759576e-05\n",
            "Epoch: 62 Batch: 240 Loss: 0.02047100104391575\n",
            "Epoch: 62 Batch: 260 Loss: 0.0005375862237997353\n",
            "Epoch: 62 Batch: 280 Loss: 0.030385971069335938\n",
            "Epoch: 62 Batch: 300 Loss: 3.814697265625e-06\n",
            "Epoch: 62 Batch: 320 Loss: 1.811981201171875e-05\n",
            "Epoch: 62 Batch: 340 Loss: 0.0003771781921386719\n",
            "Epoch: 62 Batch: 360 Loss: 2.47955322265625e-05\n",
            "Epoch: 62 Batch: 380 Loss: 0.0008873939514160156\n",
            "Epoch: 62 Batch: 400 Loss: 0.052713584154844284\n",
            "Epoch: 62 Batch: 420 Loss: 9.574890282237902e-05\n",
            "Epoch: 62 Batch: 440 Loss: 7.743835158180445e-05\n",
            "Epoch: 62 Batch: 460 Loss: 9.15527380129788e-06\n",
            "Epoch: 62 Batch: 480 Loss: 0.002426528837531805\n",
            "Epoch: 62 Batch: 500 Loss: 1.392364538332913e-05\n",
            "Epoch: 62 Batch: 520 Loss: 0.020054196938872337\n",
            "Epoch: 62 Batch: 540 Loss: 0.03690304607152939\n",
            "Epoch: 62 Batch: 560 Loss: 0.000675201416015625\n",
            "Epoch: 62 Batch: 580 Loss: 7.43865984986769e-06\n",
            "Epoch: 62 Batch: 600 Loss: 0.0004074096796102822\n",
            "Epoch: 62 Batch: 620 Loss: 1.2493133908719756e-05\n",
            "Epoch: 62 Batch: 640 Loss: 2.2602082026423886e-05\n",
            "Epoch: 62 Batch: 660 Loss: 0.0005540847778320312\n",
            "Epoch: 62 Batch: 680 Loss: 0.0010247230529785156\n",
            "Epoch: 62 Batch: 700 Loss: 0.0008445739513263106\n",
            "Epoch: 62 Batch: 720 Loss: 0.0005849838489666581\n",
            "Epoch: 62 Batch: 740 Loss: 0.02247638627886772\n",
            "Epoch: 62 Batch: 760 Loss: 0.0010797500144690275\n",
            "Epoch: 62 Batch: 780 Loss: 0.006434536073356867\n",
            "Epoch: 62 Batch: 800 Loss: 0.16553688049316406\n",
            "Epoch: 62 Batch: 820 Loss: 0.015813827514648438\n",
            "Epoch: 62 Batch: 840 Loss: 0.00038042067899368703\n",
            "Epoch: 62 Batch: 860 Loss: 1.1386029720306396\n",
            "Epoch: 62 Batch: 880 Loss: 0.05132417753338814\n",
            "Epoch: 62 Batch: 900 Loss: 0.19522781670093536\n",
            "Epoch: 62 Batch: 920 Loss: 0.0009593010181561112\n",
            "Epoch: 62 Batch: 940 Loss: 0.062033653259277344\n",
            "Epoch: 62 Batch: 960 Loss: 0.0071204183623194695\n",
            "Epoch: 62 Batch: 980 Loss: 0.0003264427068643272\n",
            "Epoch: 63 Batch: 0 Loss: 4.405975414556451e-05\n",
            "Epoch: 63 Batch: 20 Loss: 0.00012912749662064016\n",
            "Epoch: 63 Batch: 40 Loss: 5.874633643543348e-05\n",
            "Epoch: 63 Batch: 60 Loss: 0.09606580436229706\n",
            "Epoch: 63 Batch: 80 Loss: 0.0010418891906738281\n",
            "Epoch: 63 Batch: 100 Loss: 0.0003684043767862022\n",
            "Epoch: 63 Batch: 120 Loss: 0.00037059784517623484\n",
            "Epoch: 63 Batch: 140 Loss: 0.05197620391845703\n",
            "Epoch: 63 Batch: 160 Loss: 0.0003758430539164692\n",
            "Epoch: 63 Batch: 180 Loss: 7.276535325217992e-05\n",
            "Epoch: 63 Batch: 200 Loss: 0.09643735736608505\n",
            "Epoch: 63 Batch: 220 Loss: 0.0038674355018883944\n",
            "Epoch: 63 Batch: 240 Loss: 0.002182102296501398\n",
            "Epoch: 63 Batch: 260 Loss: 0.11540861427783966\n",
            "Epoch: 63 Batch: 280 Loss: 0.2418980598449707\n",
            "Epoch: 63 Batch: 300 Loss: 0.0047820089384913445\n",
            "Epoch: 63 Batch: 320 Loss: 0.006324577145278454\n",
            "Epoch: 63 Batch: 340 Loss: 0.0404169075191021\n",
            "Epoch: 63 Batch: 360 Loss: 0.0009975433349609375\n",
            "Epoch: 63 Batch: 380 Loss: 0.0028078078757971525\n",
            "Epoch: 63 Batch: 400 Loss: 0.05613555759191513\n",
            "Epoch: 63 Batch: 420 Loss: 0.012505722232162952\n",
            "Epoch: 63 Batch: 440 Loss: 2.2602082026423886e-05\n",
            "Epoch: 63 Batch: 460 Loss: 0.006033706478774548\n",
            "Epoch: 63 Batch: 480 Loss: 0.003784751985222101\n",
            "Epoch: 63 Batch: 500 Loss: 0.0003602028009481728\n",
            "Epoch: 63 Batch: 520 Loss: 0.004524135496467352\n",
            "Epoch: 63 Batch: 540 Loss: 0.00037479400634765625\n",
            "Epoch: 63 Batch: 560 Loss: 8.335113670909777e-05\n",
            "Epoch: 63 Batch: 580 Loss: 0.0008878707885742188\n",
            "Epoch: 63 Batch: 600 Loss: 0.0003266334533691406\n",
            "Epoch: 63 Batch: 620 Loss: 0.09704943001270294\n",
            "Epoch: 63 Batch: 640 Loss: 0.0009056091075763106\n",
            "Epoch: 63 Batch: 660 Loss: 4.00543194700731e-06\n",
            "Epoch: 63 Batch: 680 Loss: 0.00033550261287018657\n",
            "Epoch: 63 Batch: 700 Loss: 0.5168408155441284\n",
            "Epoch: 63 Batch: 720 Loss: 0.002596950624138117\n",
            "Epoch: 63 Batch: 740 Loss: 0.00029458999051712453\n",
            "Epoch: 63 Batch: 760 Loss: 0.00014419555373024195\n",
            "Epoch: 63 Batch: 780 Loss: 0.002899265382438898\n",
            "Epoch: 63 Batch: 800 Loss: 0.030849646776914597\n",
            "Epoch: 63 Batch: 820 Loss: 0.003078937530517578\n",
            "Epoch: 63 Batch: 840 Loss: 0.0015598296886309981\n",
            "Epoch: 63 Batch: 860 Loss: 0.005807781126350164\n",
            "Epoch: 63 Batch: 880 Loss: 0.00031833647517487407\n",
            "Epoch: 63 Batch: 900 Loss: 0.0005303382640704513\n",
            "Epoch: 63 Batch: 920 Loss: 0.008878612890839577\n",
            "Epoch: 63 Batch: 940 Loss: 0.0011692047119140625\n",
            "Epoch: 63 Batch: 960 Loss: 0.10776396095752716\n",
            "Epoch: 63 Batch: 980 Loss: 8.20159948489163e-06\n",
            "Epoch: 64 Batch: 0 Loss: 0.02067537233233452\n",
            "Epoch: 64 Batch: 20 Loss: 0.0001943588285939768\n",
            "Epoch: 64 Batch: 40 Loss: 0.12629003822803497\n",
            "Epoch: 64 Batch: 60 Loss: 0.00045137404231354594\n",
            "Epoch: 64 Batch: 80 Loss: 0.0006885528564453125\n",
            "Epoch: 64 Batch: 100 Loss: 0.0008838653448037803\n",
            "Epoch: 64 Batch: 120 Loss: 0.0010597228538244963\n",
            "Epoch: 64 Batch: 140 Loss: 0.000125885009765625\n",
            "Epoch: 64 Batch: 160 Loss: 0.00021715163893532008\n",
            "Epoch: 64 Batch: 180 Loss: 0.002169990446418524\n",
            "Epoch: 64 Batch: 200 Loss: 9.5367431640625e-07\n",
            "Epoch: 64 Batch: 220 Loss: 0.010757255367934704\n",
            "Epoch: 64 Batch: 240 Loss: 1.296997106692288e-05\n",
            "Epoch: 64 Batch: 260 Loss: 0.0004341125604696572\n",
            "Epoch: 64 Batch: 280 Loss: 0.003711128141731024\n",
            "Epoch: 64 Batch: 300 Loss: 0.0001222610444528982\n",
            "Epoch: 64 Batch: 320 Loss: 0.00019397735013626516\n",
            "Epoch: 64 Batch: 340 Loss: 0.0025293349754065275\n",
            "Epoch: 64 Batch: 360 Loss: 0.0035050392616540194\n",
            "Epoch: 64 Batch: 380 Loss: 2.441406286379788e-05\n",
            "Epoch: 64 Batch: 400 Loss: 0.8339643478393555\n",
            "Epoch: 64 Batch: 420 Loss: 6.742477125953883e-05\n",
            "Epoch: 64 Batch: 440 Loss: 0.00509567279368639\n",
            "Epoch: 64 Batch: 460 Loss: 0.08755016326904297\n",
            "Epoch: 64 Batch: 480 Loss: 0.0006566047668457031\n",
            "Epoch: 64 Batch: 500 Loss: 0.005107402801513672\n",
            "Epoch: 64 Batch: 520 Loss: 0.0033697127364575863\n",
            "Epoch: 64 Batch: 540 Loss: 0.00042285918607376516\n",
            "Epoch: 64 Batch: 560 Loss: 0.023125743493437767\n",
            "Epoch: 64 Batch: 580 Loss: 3.43322744811303e-06\n",
            "Epoch: 64 Batch: 600 Loss: 0.0013979912037029862\n",
            "Epoch: 64 Batch: 620 Loss: 0.300565630197525\n",
            "Epoch: 64 Batch: 640 Loss: 0.007774162106215954\n",
            "Epoch: 64 Batch: 660 Loss: 0.0002231597900390625\n",
            "Epoch: 64 Batch: 680 Loss: 7.362365431617945e-05\n",
            "Epoch: 64 Batch: 700 Loss: 3.299712989246473e-05\n",
            "Epoch: 64 Batch: 720 Loss: 0.01177215576171875\n",
            "Epoch: 64 Batch: 740 Loss: 0.0023304938804358244\n",
            "Epoch: 64 Batch: 760 Loss: 1.869201696536038e-05\n",
            "Epoch: 64 Batch: 780 Loss: 0.0007740974542684853\n",
            "Epoch: 64 Batch: 800 Loss: 0.09416131675243378\n",
            "Epoch: 64 Batch: 820 Loss: 0.17665505409240723\n",
            "Epoch: 64 Batch: 840 Loss: 0.23857536911964417\n",
            "Epoch: 64 Batch: 860 Loss: 0.0011704445350915194\n",
            "Epoch: 64 Batch: 880 Loss: 0.023952197283506393\n",
            "Epoch: 64 Batch: 900 Loss: 0.012996482662856579\n",
            "Epoch: 64 Batch: 920 Loss: 0.020514536648988724\n",
            "Epoch: 64 Batch: 940 Loss: 0.014000368304550648\n",
            "Epoch: 64 Batch: 960 Loss: 7.362365431617945e-05\n",
            "Epoch: 64 Batch: 980 Loss: 0.07743415981531143\n",
            "Epoch: 65 Batch: 0 Loss: 0.0003361701965332031\n",
            "Epoch: 65 Batch: 20 Loss: 5.588531348621473e-05\n",
            "Epoch: 65 Batch: 40 Loss: 4.882812572759576e-05\n",
            "Epoch: 65 Batch: 60 Loss: 0.15125569701194763\n",
            "Epoch: 65 Batch: 80 Loss: 2.8896331059513614e-05\n",
            "Epoch: 65 Batch: 100 Loss: 0.0020672797691076994\n",
            "Epoch: 65 Batch: 120 Loss: 0.0008037567022256553\n",
            "Epoch: 65 Batch: 140 Loss: 7.61985793360509e-05\n",
            "Epoch: 65 Batch: 160 Loss: 0.009287262335419655\n",
            "Epoch: 65 Batch: 180 Loss: 0.00018997192091774195\n",
            "Epoch: 65 Batch: 200 Loss: 0.0030883788131177425\n",
            "Epoch: 65 Batch: 220 Loss: 0.0005613326793536544\n",
            "Epoch: 65 Batch: 240 Loss: 2.937316821771674e-05\n",
            "Epoch: 65 Batch: 260 Loss: 0.0008633613470010459\n",
            "Epoch: 65 Batch: 280 Loss: 4.367828296381049e-05\n",
            "Epoch: 65 Batch: 300 Loss: 0.00018777846707962453\n",
            "Epoch: 65 Batch: 320 Loss: 2.079009937006049e-05\n",
            "Epoch: 65 Batch: 340 Loss: 0.0011012076865881681\n",
            "Epoch: 65 Batch: 360 Loss: 0.00010519028000999242\n",
            "Epoch: 65 Batch: 380 Loss: 0.006297969724982977\n",
            "Epoch: 65 Batch: 400 Loss: 1.144409225162235e-06\n",
            "Epoch: 65 Batch: 420 Loss: 1.640319896978326e-05\n",
            "Epoch: 65 Batch: 440 Loss: 0.0011226653587073088\n",
            "Epoch: 65 Batch: 460 Loss: 0.00012979508028365672\n",
            "Epoch: 65 Batch: 480 Loss: 0.0003204345703125\n",
            "Epoch: 65 Batch: 500 Loss: 0.00017662048048805445\n",
            "Epoch: 65 Batch: 520 Loss: 0.0001642227143747732\n",
            "Epoch: 65 Batch: 540 Loss: 3.948211815441027e-05\n",
            "Epoch: 65 Batch: 560 Loss: 5.14984139954322e-06\n",
            "Epoch: 65 Batch: 580 Loss: 0.000118255615234375\n",
            "Epoch: 65 Batch: 600 Loss: 3.166198803228326e-05\n",
            "Epoch: 65 Batch: 620 Loss: 2.2792815798311494e-05\n",
            "Epoch: 65 Batch: 640 Loss: 0.059168051928281784\n",
            "Epoch: 65 Batch: 660 Loss: 0.006117820739746094\n",
            "Epoch: 65 Batch: 680 Loss: 0.0015582084888592362\n",
            "Epoch: 65 Batch: 700 Loss: 0.0018513679970055819\n",
            "Epoch: 65 Batch: 720 Loss: 0.0004403114435262978\n",
            "Epoch: 65 Batch: 740 Loss: 0.0002898216189350933\n",
            "Epoch: 65 Batch: 760 Loss: 0.0008388519054278731\n",
            "Epoch: 65 Batch: 780 Loss: 1.087188684323337e-05\n",
            "Epoch: 65 Batch: 800 Loss: 0.0034869194496423006\n",
            "Epoch: 65 Batch: 820 Loss: 1.9073486328125e-06\n",
            "Epoch: 65 Batch: 840 Loss: 0.00036144256591796875\n",
            "Epoch: 65 Batch: 860 Loss: 0.0001428604155080393\n",
            "Epoch: 65 Batch: 880 Loss: 0.0013653754722326994\n",
            "Epoch: 65 Batch: 900 Loss: 1.220703143189894e-05\n",
            "Epoch: 65 Batch: 920 Loss: 0.00137920374982059\n",
            "Epoch: 65 Batch: 940 Loss: 7.62939453125e-05\n",
            "Epoch: 65 Batch: 960 Loss: 0.006337547209113836\n",
            "Epoch: 65 Batch: 980 Loss: 0.0005935669178143144\n",
            "Epoch: 66 Batch: 0 Loss: 0.01192240696400404\n",
            "Epoch: 66 Batch: 20 Loss: 0.002795314881950617\n",
            "Epoch: 66 Batch: 40 Loss: 0.0008724212530069053\n",
            "Epoch: 66 Batch: 60 Loss: 8.02993745310232e-05\n",
            "Epoch: 66 Batch: 80 Loss: 0.032118987292051315\n",
            "Epoch: 66 Batch: 100 Loss: 2.0885467165498994e-05\n",
            "Epoch: 66 Batch: 120 Loss: 0.0001050949067575857\n",
            "Epoch: 66 Batch: 140 Loss: 0.00847005844116211\n",
            "Epoch: 66 Batch: 160 Loss: 0.0022710799239575863\n",
            "Epoch: 66 Batch: 180 Loss: 4.57763690064894e-06\n",
            "Epoch: 66 Batch: 200 Loss: 0.025330353528261185\n",
            "Epoch: 66 Batch: 220 Loss: 0.0003520965692587197\n",
            "Epoch: 66 Batch: 240 Loss: 0.0008942604181356728\n",
            "Epoch: 66 Batch: 260 Loss: 0.30943527817726135\n",
            "Epoch: 66 Batch: 280 Loss: 0.0008922576671466231\n",
            "Epoch: 66 Batch: 300 Loss: 0.3427008092403412\n",
            "Epoch: 66 Batch: 320 Loss: 0.020285416394472122\n",
            "Epoch: 66 Batch: 340 Loss: 0.0022302628494799137\n",
            "Epoch: 66 Batch: 360 Loss: 1.068115216185106e-05\n",
            "Epoch: 66 Batch: 380 Loss: 0.0003724098205566406\n",
            "Epoch: 66 Batch: 400 Loss: 0.00034656524076126516\n",
            "Epoch: 66 Batch: 420 Loss: 0.0009813308715820312\n",
            "Epoch: 66 Batch: 440 Loss: 0.07424864917993546\n",
            "Epoch: 66 Batch: 460 Loss: 0.0014927864540368319\n",
            "Epoch: 66 Batch: 480 Loss: 0.006928539369255304\n",
            "Epoch: 66 Batch: 500 Loss: 0.003056478453800082\n",
            "Epoch: 66 Batch: 520 Loss: 0.0012050628429278731\n",
            "Epoch: 66 Batch: 540 Loss: 0.00012483596219681203\n",
            "Epoch: 66 Batch: 560 Loss: 0.021807480603456497\n",
            "Epoch: 66 Batch: 580 Loss: 0.0063575743697583675\n",
            "Epoch: 66 Batch: 600 Loss: 0.00010728836059570312\n",
            "Epoch: 66 Batch: 620 Loss: 0.020084857940673828\n",
            "Epoch: 66 Batch: 640 Loss: 0.00034646986750885844\n",
            "Epoch: 66 Batch: 660 Loss: 0.0035407065879553556\n",
            "Epoch: 66 Batch: 680 Loss: 0.07869338989257812\n",
            "Epoch: 66 Batch: 700 Loss: 0.025914620608091354\n",
            "Epoch: 66 Batch: 720 Loss: 0.0001832962007028982\n",
            "Epoch: 66 Batch: 740 Loss: 0.0008949280017986894\n",
            "Epoch: 66 Batch: 760 Loss: 0.0034258842933923006\n",
            "Epoch: 66 Batch: 780 Loss: 0.00032806396484375\n",
            "Epoch: 66 Batch: 800 Loss: 0.0015298842918127775\n",
            "Epoch: 66 Batch: 820 Loss: 0.0001371383696096018\n",
            "Epoch: 66 Batch: 840 Loss: 0.00022268295288085938\n",
            "Epoch: 66 Batch: 860 Loss: 0.0015129089588299394\n",
            "Epoch: 66 Batch: 880 Loss: 0.0002961158752441406\n",
            "Epoch: 66 Batch: 900 Loss: 0.0029065608978271484\n",
            "Epoch: 66 Batch: 920 Loss: 0.0007915496826171875\n",
            "Epoch: 66 Batch: 940 Loss: 0.0017100333934649825\n",
            "Epoch: 66 Batch: 960 Loss: 0.05517926067113876\n",
            "Epoch: 66 Batch: 980 Loss: 0.020775413140654564\n",
            "Epoch: 67 Batch: 0 Loss: 0.00041875839815475047\n",
            "Epoch: 67 Batch: 20 Loss: 0.03887014463543892\n",
            "Epoch: 67 Batch: 40 Loss: 0.0001926422119140625\n",
            "Epoch: 67 Batch: 60 Loss: 7.05719003235572e-06\n",
            "Epoch: 67 Batch: 80 Loss: 0.035546015948057175\n",
            "Epoch: 67 Batch: 100 Loss: 0.00030460357083939016\n",
            "Epoch: 67 Batch: 120 Loss: 0.05846834182739258\n",
            "Epoch: 67 Batch: 140 Loss: 0.0020224570762366056\n",
            "Epoch: 67 Batch: 160 Loss: 8.287429955089465e-05\n",
            "Epoch: 67 Batch: 180 Loss: 0.0007711410289630294\n",
            "Epoch: 67 Batch: 200 Loss: 0.00034990310086868703\n",
            "Epoch: 67 Batch: 220 Loss: 0.00018424987501930445\n",
            "Epoch: 67 Batch: 240 Loss: 0.0005572318914346397\n",
            "Epoch: 67 Batch: 260 Loss: 0.0008194923284463584\n",
            "Epoch: 67 Batch: 280 Loss: 0.0013322830200195312\n",
            "Epoch: 67 Batch: 300 Loss: 0.009640311822295189\n",
            "Epoch: 67 Batch: 320 Loss: 3.44276413670741e-05\n",
            "Epoch: 67 Batch: 340 Loss: 0.0023101805709302425\n",
            "Epoch: 67 Batch: 360 Loss: 4.38690176451928e-06\n",
            "Epoch: 67 Batch: 380 Loss: 2.670288040462765e-06\n",
            "Epoch: 67 Batch: 400 Loss: 0.006277942564338446\n",
            "Epoch: 67 Batch: 420 Loss: 0.015381288714706898\n",
            "Epoch: 67 Batch: 440 Loss: 2.784729076665826e-05\n",
            "Epoch: 67 Batch: 460 Loss: 0.0006470680236816406\n",
            "Epoch: 67 Batch: 480 Loss: 0.00012149810936534777\n",
            "Epoch: 67 Batch: 500 Loss: 0.0013346672058105469\n",
            "Epoch: 67 Batch: 520 Loss: 0.0001829147367971018\n",
            "Epoch: 67 Batch: 540 Loss: 7.190704491222277e-05\n",
            "Epoch: 67 Batch: 560 Loss: 0.012196731753647327\n",
            "Epoch: 67 Batch: 580 Loss: 8.831024024402723e-05\n",
            "Epoch: 67 Batch: 600 Loss: 0.03670549392700195\n",
            "Epoch: 67 Batch: 620 Loss: 0.001888704253360629\n",
            "Epoch: 67 Batch: 640 Loss: 4.043579247081652e-05\n",
            "Epoch: 67 Batch: 660 Loss: 0.0007558822399005294\n",
            "Epoch: 67 Batch: 680 Loss: 0.50006502866745\n",
            "Epoch: 67 Batch: 700 Loss: 0.28497403860092163\n",
            "Epoch: 67 Batch: 720 Loss: 0.015204668045043945\n",
            "Epoch: 67 Batch: 740 Loss: 0.5906292796134949\n",
            "Epoch: 67 Batch: 760 Loss: 0.0016974449390545487\n",
            "Epoch: 67 Batch: 780 Loss: 0.0027930259238928556\n",
            "Epoch: 67 Batch: 800 Loss: 2.174377368646674e-05\n",
            "Epoch: 67 Batch: 820 Loss: 0.0011819839710369706\n",
            "Epoch: 67 Batch: 840 Loss: 0.0008272171253338456\n",
            "Epoch: 67 Batch: 860 Loss: 0.186039999127388\n",
            "Epoch: 67 Batch: 880 Loss: 2.86102294921875e-06\n",
            "Epoch: 67 Batch: 900 Loss: 0.002553653670474887\n",
            "Epoch: 67 Batch: 920 Loss: 0.004736423492431641\n",
            "Epoch: 67 Batch: 940 Loss: 0.04953040927648544\n",
            "Epoch: 67 Batch: 960 Loss: 0.014613723382353783\n",
            "Epoch: 67 Batch: 980 Loss: 0.00021648406982421875\n",
            "Epoch: 68 Batch: 0 Loss: 0.0007648468017578125\n",
            "Epoch: 68 Batch: 20 Loss: 0.0010420798789709806\n",
            "Epoch: 68 Batch: 40 Loss: 1.71661376953125e-05\n",
            "Epoch: 68 Batch: 60 Loss: 0.0008772850269451737\n",
            "Epoch: 68 Batch: 80 Loss: 0.0063648223876953125\n",
            "Epoch: 68 Batch: 100 Loss: 2.09808349609375e-05\n",
            "Epoch: 68 Batch: 120 Loss: 0.00012273788161110133\n",
            "Epoch: 68 Batch: 140 Loss: 5.53131121705519e-06\n",
            "Epoch: 68 Batch: 160 Loss: 0.0002038955717580393\n",
            "Epoch: 68 Batch: 180 Loss: 0.000102996826171875\n",
            "Epoch: 68 Batch: 200 Loss: 0.71354079246521\n",
            "Epoch: 68 Batch: 220 Loss: 0.00932016409933567\n",
            "Epoch: 68 Batch: 240 Loss: 0.005991840269416571\n",
            "Epoch: 68 Batch: 260 Loss: 0.035669516772031784\n",
            "Epoch: 68 Batch: 280 Loss: 0.00017671585374046117\n",
            "Epoch: 68 Batch: 300 Loss: 0.00025005341740325093\n",
            "Epoch: 68 Batch: 320 Loss: 0.00010185241990257055\n",
            "Epoch: 68 Batch: 340 Loss: 0.00028123855008743703\n",
            "Epoch: 68 Batch: 360 Loss: 9.91821252682712e-06\n",
            "Epoch: 68 Batch: 380 Loss: 0.00013208389282226562\n",
            "Epoch: 68 Batch: 400 Loss: 0.02982611581683159\n",
            "Epoch: 68 Batch: 420 Loss: 0.0003223419189453125\n",
            "Epoch: 68 Batch: 440 Loss: 2.09808349609375e-05\n",
            "Epoch: 68 Batch: 460 Loss: 0.0006509780650958419\n",
            "Epoch: 68 Batch: 480 Loss: 1.0955134630203247\n",
            "Epoch: 68 Batch: 500 Loss: 0.03756828233599663\n",
            "Epoch: 68 Batch: 520 Loss: 7.762909081066027e-05\n",
            "Epoch: 68 Batch: 540 Loss: 2.7370453608455136e-05\n",
            "Epoch: 68 Batch: 560 Loss: 2.632141149661038e-05\n",
            "Epoch: 68 Batch: 580 Loss: 0.031007861718535423\n",
            "Epoch: 68 Batch: 600 Loss: 1.735687328618951e-05\n",
            "Epoch: 68 Batch: 620 Loss: 8.296966552734375e-05\n",
            "Epoch: 68 Batch: 640 Loss: 0.0005517959361895919\n",
            "Epoch: 68 Batch: 660 Loss: 0.0001733779936330393\n",
            "Epoch: 68 Batch: 680 Loss: 0.0008312225108966231\n",
            "Epoch: 68 Batch: 700 Loss: 0.28521662950515747\n",
            "Epoch: 68 Batch: 720 Loss: 0.008780193515121937\n",
            "Epoch: 68 Batch: 740 Loss: 0.3211818337440491\n",
            "Epoch: 68 Batch: 760 Loss: 8.344650268554688e-05\n",
            "Epoch: 68 Batch: 780 Loss: 1.602172778802924e-05\n",
            "Epoch: 68 Batch: 800 Loss: 0.07496164739131927\n",
            "Epoch: 68 Batch: 820 Loss: 0.0022262572310864925\n",
            "Epoch: 68 Batch: 840 Loss: 0.08438167721033096\n",
            "Epoch: 68 Batch: 860 Loss: 2.3746490114717744e-05\n",
            "Epoch: 68 Batch: 880 Loss: 1.468658410885837e-05\n",
            "Epoch: 68 Batch: 900 Loss: 0.0011704445350915194\n",
            "Epoch: 68 Batch: 920 Loss: 0.001970481825992465\n",
            "Epoch: 68 Batch: 940 Loss: 0.023150920867919922\n",
            "Epoch: 68 Batch: 960 Loss: 0.00031766892061568797\n",
            "Epoch: 68 Batch: 980 Loss: 3.6525725590763614e-05\n",
            "Epoch: 69 Batch: 0 Loss: 6.68525681248866e-05\n",
            "Epoch: 69 Batch: 20 Loss: 0.01077423058450222\n",
            "Epoch: 69 Batch: 40 Loss: 0.00035266875056549907\n",
            "Epoch: 69 Batch: 60 Loss: 0.0017099380493164062\n",
            "Epoch: 69 Batch: 80 Loss: 8.993149094749242e-05\n",
            "Epoch: 69 Batch: 100 Loss: 0.00011692046973621473\n",
            "Epoch: 69 Batch: 120 Loss: 3.070831371587701e-05\n",
            "Epoch: 69 Batch: 140 Loss: 0.019077634438872337\n",
            "Epoch: 69 Batch: 160 Loss: 0.03788604587316513\n",
            "Epoch: 69 Batch: 180 Loss: 9.441375732421875e-05\n",
            "Epoch: 69 Batch: 200 Loss: 0.003818988800048828\n",
            "Epoch: 69 Batch: 220 Loss: 0.0005553245428018272\n",
            "Epoch: 69 Batch: 240 Loss: 1.0852360725402832\n",
            "Epoch: 69 Batch: 260 Loss: 0.37866777181625366\n",
            "Epoch: 69 Batch: 280 Loss: 0.00042476653470657766\n",
            "Epoch: 69 Batch: 300 Loss: 0.07477855682373047\n",
            "Epoch: 69 Batch: 320 Loss: 0.00012168884131824598\n",
            "Epoch: 69 Batch: 340 Loss: 0.00021800995455123484\n",
            "Epoch: 69 Batch: 360 Loss: 9.422302537132055e-05\n",
            "Epoch: 69 Batch: 380 Loss: 0.004833507351577282\n",
            "Epoch: 69 Batch: 400 Loss: 0.10749206691980362\n",
            "Epoch: 69 Batch: 420 Loss: 0.0009771346813067794\n",
            "Epoch: 69 Batch: 440 Loss: 0.861223042011261\n",
            "Epoch: 69 Batch: 460 Loss: 0.00119953160174191\n",
            "Epoch: 69 Batch: 480 Loss: 0.01676802709698677\n",
            "Epoch: 69 Batch: 500 Loss: 3.24249267578125e-05\n",
            "Epoch: 69 Batch: 520 Loss: 0.959452748298645\n",
            "Epoch: 69 Batch: 540 Loss: 0.0001279830903513357\n",
            "Epoch: 69 Batch: 560 Loss: 0.0004447936953511089\n",
            "Epoch: 69 Batch: 580 Loss: 0.0011290550464764237\n",
            "Epoch: 69 Batch: 600 Loss: 0.002689457032829523\n",
            "Epoch: 69 Batch: 620 Loss: 0.007031440734863281\n",
            "Epoch: 69 Batch: 640 Loss: 0.00010385513451183215\n",
            "Epoch: 69 Batch: 660 Loss: 0.00060949323233217\n",
            "Epoch: 69 Batch: 680 Loss: 0.005676460452377796\n",
            "Epoch: 69 Batch: 700 Loss: 0.06891556084156036\n",
            "Epoch: 69 Batch: 720 Loss: 2.765655517578125e-05\n",
            "Epoch: 69 Batch: 740 Loss: 0.006297445390373468\n",
            "Epoch: 69 Batch: 760 Loss: 5.91278057981981e-06\n",
            "Epoch: 69 Batch: 780 Loss: 0.00023899078951217234\n",
            "Epoch: 69 Batch: 800 Loss: 0.07375030219554901\n",
            "Epoch: 69 Batch: 820 Loss: 1.9073486612342094e-07\n",
            "Epoch: 69 Batch: 840 Loss: 3.890991138177924e-05\n",
            "Epoch: 69 Batch: 860 Loss: 2.155303991457913e-05\n",
            "Epoch: 69 Batch: 880 Loss: 0.1339341104030609\n",
            "Epoch: 69 Batch: 900 Loss: 7.629394644936838e-07\n",
            "Epoch: 69 Batch: 920 Loss: 0.0025569915305823088\n",
            "Epoch: 69 Batch: 940 Loss: 0.0005252838018350303\n",
            "Epoch: 69 Batch: 960 Loss: 7.629394644936838e-07\n",
            "Epoch: 69 Batch: 980 Loss: 1.564025842526462e-05\n",
            "Epoch: 70 Batch: 0 Loss: 0.022916126996278763\n",
            "Epoch: 70 Batch: 20 Loss: 0.003101921174675226\n",
            "Epoch: 70 Batch: 40 Loss: 0.009206390008330345\n",
            "Epoch: 70 Batch: 60 Loss: 0.5088809132575989\n",
            "Epoch: 70 Batch: 80 Loss: 0.0003470420779194683\n",
            "Epoch: 70 Batch: 100 Loss: 4.863739013671875e-05\n",
            "Epoch: 70 Batch: 120 Loss: 7.200241088867188e-05\n",
            "Epoch: 70 Batch: 140 Loss: 0.000141143798828125\n",
            "Epoch: 70 Batch: 160 Loss: 0.0014383315574377775\n",
            "Epoch: 70 Batch: 180 Loss: 5.626678466796875e-05\n",
            "Epoch: 70 Batch: 200 Loss: 0.0003288268926553428\n",
            "Epoch: 70 Batch: 220 Loss: 0.020142555236816406\n",
            "Epoch: 70 Batch: 240 Loss: 0.0005291939014568925\n",
            "Epoch: 70 Batch: 260 Loss: 2.536773718020413e-05\n",
            "Epoch: 70 Batch: 280 Loss: 1.201629675051663e-05\n",
            "Epoch: 70 Batch: 300 Loss: 1.697540210443549e-05\n",
            "Epoch: 70 Batch: 320 Loss: 0.0006366729503497481\n",
            "Epoch: 70 Batch: 340 Loss: 3.2901763916015625e-05\n",
            "Epoch: 70 Batch: 360 Loss: 0.0015954971313476562\n",
            "Epoch: 70 Batch: 380 Loss: 0.002220058348029852\n",
            "Epoch: 70 Batch: 400 Loss: 0.09484019130468369\n",
            "Epoch: 70 Batch: 420 Loss: 4.367828296381049e-05\n",
            "Epoch: 70 Batch: 440 Loss: 0.000859165214933455\n",
            "Epoch: 70 Batch: 460 Loss: 0.11849403381347656\n",
            "Epoch: 70 Batch: 480 Loss: 0.00015096664719749242\n",
            "Epoch: 70 Batch: 500 Loss: 0.00028972624568268657\n",
            "Epoch: 70 Batch: 520 Loss: 4.19616708313697e-06\n",
            "Epoch: 70 Batch: 540 Loss: 0.0032737732399255037\n",
            "Epoch: 70 Batch: 560 Loss: 3.948211815441027e-05\n",
            "Epoch: 70 Batch: 580 Loss: 1.049041748046875e-05\n",
            "Epoch: 70 Batch: 600 Loss: 0.008677291683852673\n",
            "Epoch: 70 Batch: 620 Loss: 0.00033845900907181203\n",
            "Epoch: 70 Batch: 640 Loss: 0.004420471377670765\n",
            "Epoch: 70 Batch: 660 Loss: 0.19794544577598572\n",
            "Epoch: 70 Batch: 680 Loss: 1.8596649169921875e-05\n",
            "Epoch: 70 Batch: 700 Loss: 5.91278057981981e-06\n",
            "Epoch: 70 Batch: 720 Loss: 0.003475189208984375\n",
            "Epoch: 70 Batch: 740 Loss: 0.08945505321025848\n",
            "Epoch: 70 Batch: 760 Loss: 8.79287690622732e-05\n",
            "Epoch: 70 Batch: 780 Loss: 0.009234333410859108\n",
            "Epoch: 70 Batch: 800 Loss: 0.0009756088256835938\n",
            "Epoch: 70 Batch: 820 Loss: 0.0010122299427166581\n",
            "Epoch: 70 Batch: 840 Loss: 0.0011143684387207031\n",
            "Epoch: 70 Batch: 860 Loss: 8.39233416627394e-06\n",
            "Epoch: 70 Batch: 880 Loss: 0.00020074844360351562\n",
            "Epoch: 70 Batch: 900 Loss: 2.326965295651462e-05\n",
            "Epoch: 70 Batch: 920 Loss: 2.250671423098538e-05\n",
            "Epoch: 70 Batch: 940 Loss: 0.001798343611881137\n",
            "Epoch: 70 Batch: 960 Loss: 7.24792471373803e-06\n",
            "Epoch: 70 Batch: 980 Loss: 0.0011739730834960938\n",
            "Epoch: 71 Batch: 0 Loss: 0.38083600997924805\n",
            "Epoch: 71 Batch: 20 Loss: 0.031171511858701706\n",
            "Epoch: 71 Batch: 40 Loss: 0.008829688653349876\n",
            "Epoch: 71 Batch: 60 Loss: 0.00023384093947242945\n",
            "Epoch: 71 Batch: 80 Loss: 0.0004051208379678428\n",
            "Epoch: 71 Batch: 100 Loss: 1.277923547604587e-05\n",
            "Epoch: 71 Batch: 120 Loss: 0.00844430923461914\n",
            "Epoch: 71 Batch: 140 Loss: 0.0007856368902139366\n",
            "Epoch: 71 Batch: 160 Loss: 4.95910626341356e-06\n",
            "Epoch: 71 Batch: 180 Loss: 4.76837158203125e-06\n",
            "Epoch: 71 Batch: 200 Loss: 4.291534423828125e-05\n",
            "Epoch: 71 Batch: 220 Loss: 0.005098819732666016\n",
            "Epoch: 71 Batch: 240 Loss: 0.20223255455493927\n",
            "Epoch: 71 Batch: 260 Loss: 6.170272536110133e-05\n",
            "Epoch: 71 Batch: 280 Loss: 8.19206252344884e-05\n",
            "Epoch: 71 Batch: 300 Loss: 0.0020036697387695312\n",
            "Epoch: 71 Batch: 320 Loss: 0.009096240624785423\n",
            "Epoch: 71 Batch: 340 Loss: 0.00014972686767578125\n",
            "Epoch: 71 Batch: 360 Loss: 0.00026960374088957906\n",
            "Epoch: 71 Batch: 380 Loss: 0.0018013000953942537\n",
            "Epoch: 71 Batch: 400 Loss: 0.007900762371718884\n",
            "Epoch: 71 Batch: 420 Loss: 0.0026237010024487972\n",
            "Epoch: 71 Batch: 440 Loss: 0.0003203391970600933\n",
            "Epoch: 71 Batch: 460 Loss: 0.0038361549377441406\n",
            "Epoch: 71 Batch: 480 Loss: 0.011220264248549938\n",
            "Epoch: 71 Batch: 500 Loss: 0.00010576248314464465\n",
            "Epoch: 71 Batch: 520 Loss: 3.910064606316155e-06\n",
            "Epoch: 71 Batch: 540 Loss: 0.0007297515985555947\n",
            "Epoch: 71 Batch: 560 Loss: 0.00011911392357433215\n",
            "Epoch: 71 Batch: 580 Loss: 0.0004554748593363911\n",
            "Epoch: 71 Batch: 600 Loss: 7.390975952148438e-05\n",
            "Epoch: 71 Batch: 620 Loss: 0.004926013760268688\n",
            "Epoch: 71 Batch: 640 Loss: 3.0517578125e-05\n",
            "Epoch: 71 Batch: 660 Loss: 0.0003056526184082031\n",
            "Epoch: 71 Batch: 680 Loss: 1.125335711549269e-05\n",
            "Epoch: 71 Batch: 700 Loss: 0.014407062903046608\n",
            "Epoch: 71 Batch: 720 Loss: 0.003578090574592352\n",
            "Epoch: 71 Batch: 740 Loss: 9.5367431640625e-07\n",
            "Epoch: 71 Batch: 760 Loss: 0.011742020025849342\n",
            "Epoch: 71 Batch: 780 Loss: 2.86102294921875e-06\n",
            "Epoch: 71 Batch: 800 Loss: 4.482269105210435e-06\n",
            "Epoch: 71 Batch: 820 Loss: 0.0005717277526855469\n",
            "Epoch: 71 Batch: 840 Loss: 0.006672668270766735\n",
            "Epoch: 71 Batch: 860 Loss: 8.39233416627394e-06\n",
            "Epoch: 71 Batch: 880 Loss: 0.00695724505931139\n",
            "Epoch: 71 Batch: 900 Loss: 0.03403053432703018\n",
            "Epoch: 71 Batch: 920 Loss: 0.03904275968670845\n",
            "Epoch: 71 Batch: 940 Loss: 0.016256237402558327\n",
            "Epoch: 71 Batch: 960 Loss: 1.316070574830519e-05\n",
            "Epoch: 71 Batch: 980 Loss: 2.7179718017578125e-05\n",
            "Epoch: 72 Batch: 0 Loss: 0.012220000848174095\n",
            "Epoch: 72 Batch: 20 Loss: 0.00010223388380836695\n",
            "Epoch: 72 Batch: 40 Loss: 0.0004670143243856728\n",
            "Epoch: 72 Batch: 60 Loss: 0.00011320113844703883\n",
            "Epoch: 72 Batch: 80 Loss: 0.0002501487615518272\n",
            "Epoch: 72 Batch: 100 Loss: 2.098083541568485e-06\n",
            "Epoch: 72 Batch: 120 Loss: 6.446838233387098e-05\n",
            "Epoch: 72 Batch: 140 Loss: 0.004695320036262274\n",
            "Epoch: 72 Batch: 160 Loss: 0.025313114747405052\n",
            "Epoch: 72 Batch: 180 Loss: 0.19773443043231964\n",
            "Epoch: 72 Batch: 200 Loss: 0.0011972427600994706\n",
            "Epoch: 72 Batch: 220 Loss: 0.00027141571626998484\n",
            "Epoch: 72 Batch: 240 Loss: 0.0001029014601954259\n",
            "Epoch: 72 Batch: 260 Loss: 2.28881845032447e-06\n",
            "Epoch: 72 Batch: 280 Loss: 0.011946534737944603\n",
            "Epoch: 72 Batch: 300 Loss: 4.806518700206652e-05\n",
            "Epoch: 72 Batch: 320 Loss: 2.2983551389188506e-05\n",
            "Epoch: 72 Batch: 340 Loss: 0.006162834353744984\n",
            "Epoch: 72 Batch: 360 Loss: 5.931854320806451e-05\n",
            "Epoch: 72 Batch: 380 Loss: 1.754760705807712e-05\n",
            "Epoch: 72 Batch: 400 Loss: 0.00033884047297760844\n",
            "Epoch: 72 Batch: 420 Loss: 0.0010569572914391756\n",
            "Epoch: 72 Batch: 440 Loss: 9.441375732421875e-05\n",
            "Epoch: 72 Batch: 460 Loss: 0.00047245025052689016\n",
            "Epoch: 72 Batch: 480 Loss: 0.0048621175810694695\n",
            "Epoch: 72 Batch: 500 Loss: 0.0\n",
            "Epoch: 72 Batch: 520 Loss: 0.02432422712445259\n",
            "Epoch: 72 Batch: 540 Loss: 0.007499980740249157\n",
            "Epoch: 72 Batch: 560 Loss: 5.397796485340223e-05\n",
            "Epoch: 72 Batch: 580 Loss: 0.0001928329438669607\n",
            "Epoch: 72 Batch: 600 Loss: 0.008372497744858265\n",
            "Epoch: 72 Batch: 620 Loss: 0.08070297539234161\n",
            "Epoch: 72 Batch: 640 Loss: 0.0002115249662892893\n",
            "Epoch: 72 Batch: 660 Loss: 1.0355881452560425\n",
            "Epoch: 72 Batch: 680 Loss: 7.858276512706652e-05\n",
            "Epoch: 72 Batch: 700 Loss: 0.0018409729236736894\n",
            "Epoch: 72 Batch: 720 Loss: 0.0006598472828045487\n",
            "Epoch: 72 Batch: 740 Loss: 0.5177382230758667\n",
            "Epoch: 72 Batch: 760 Loss: 1.5544890629826114e-05\n",
            "Epoch: 72 Batch: 780 Loss: 6.580352874152595e-06\n",
            "Epoch: 72 Batch: 800 Loss: 0.0031584263779222965\n",
            "Epoch: 72 Batch: 820 Loss: 0.0004153251647949219\n",
            "Epoch: 72 Batch: 840 Loss: 0.0017431259620934725\n",
            "Epoch: 72 Batch: 860 Loss: 0.04663896560668945\n",
            "Epoch: 72 Batch: 880 Loss: 0.0010416030418127775\n",
            "Epoch: 72 Batch: 900 Loss: 0.006614208221435547\n",
            "Epoch: 72 Batch: 920 Loss: 0.12065882980823517\n",
            "Epoch: 72 Batch: 940 Loss: 0.7183599472045898\n",
            "Epoch: 72 Batch: 960 Loss: 9.5367431640625e-07\n",
            "Epoch: 72 Batch: 980 Loss: 0.011189651675522327\n",
            "Epoch: 73 Batch: 0 Loss: 0.003882789518684149\n",
            "Epoch: 73 Batch: 20 Loss: 0.0044803619384765625\n",
            "Epoch: 73 Batch: 40 Loss: 0.009991025552153587\n",
            "Epoch: 73 Batch: 60 Loss: 6.65664701955393e-05\n",
            "Epoch: 73 Batch: 80 Loss: 0.0023183822631835938\n",
            "Epoch: 73 Batch: 100 Loss: 6.67572021484375e-06\n",
            "Epoch: 73 Batch: 120 Loss: 2.346038854739163e-05\n",
            "Epoch: 73 Batch: 140 Loss: 0.0003262519894633442\n",
            "Epoch: 73 Batch: 160 Loss: 0.07637543976306915\n",
            "Epoch: 73 Batch: 180 Loss: 0.008784008212387562\n",
            "Epoch: 73 Batch: 200 Loss: 0.002170467283576727\n",
            "Epoch: 73 Batch: 220 Loss: 2.555847095209174e-05\n",
            "Epoch: 73 Batch: 240 Loss: 0.0024459362030029297\n",
            "Epoch: 73 Batch: 260 Loss: 2.47955313170678e-06\n",
            "Epoch: 73 Batch: 280 Loss: 0.0007977485656738281\n",
            "Epoch: 73 Batch: 300 Loss: 0.011928940191864967\n",
            "Epoch: 73 Batch: 320 Loss: 0.00039224623469635844\n",
            "Epoch: 73 Batch: 340 Loss: 0.0067962645553052425\n",
            "Epoch: 73 Batch: 360 Loss: 0.00014381408982444555\n",
            "Epoch: 73 Batch: 380 Loss: 9.794234938453883e-05\n",
            "Epoch: 73 Batch: 400 Loss: 0.00016651154146529734\n",
            "Epoch: 73 Batch: 420 Loss: 0.0011068343883380294\n",
            "Epoch: 73 Batch: 440 Loss: 0.0011874198680743575\n",
            "Epoch: 73 Batch: 460 Loss: 0.0037276267539709806\n",
            "Epoch: 73 Batch: 480 Loss: 0.0006249427678994834\n",
            "Epoch: 73 Batch: 500 Loss: 0.00937805138528347\n",
            "Epoch: 73 Batch: 520 Loss: 0.45055508613586426\n",
            "Epoch: 73 Batch: 540 Loss: 7.629394644936838e-07\n",
            "Epoch: 73 Batch: 560 Loss: 0.006041908171027899\n",
            "Epoch: 73 Batch: 580 Loss: 0.0042705535888671875\n",
            "Epoch: 73 Batch: 600 Loss: 0.03609342500567436\n",
            "Epoch: 73 Batch: 620 Loss: 0.00326194753870368\n",
            "Epoch: 73 Batch: 640 Loss: 0.0002732276916503906\n",
            "Epoch: 73 Batch: 660 Loss: 0.0005230903625488281\n",
            "Epoch: 73 Batch: 680 Loss: 0.00547714252024889\n",
            "Epoch: 73 Batch: 700 Loss: 4.281997826183215e-05\n",
            "Epoch: 73 Batch: 720 Loss: 0.006080531980842352\n",
            "Epoch: 73 Batch: 740 Loss: 0.724025547504425\n",
            "Epoch: 73 Batch: 760 Loss: 0.00234394078142941\n",
            "Epoch: 73 Batch: 780 Loss: 0.00016651154146529734\n",
            "Epoch: 73 Batch: 800 Loss: 0.0013032912975177169\n",
            "Epoch: 73 Batch: 820 Loss: 1.5258789289873675e-06\n",
            "Epoch: 73 Batch: 840 Loss: 1.2111663636460435e-05\n",
            "Epoch: 73 Batch: 860 Loss: 0.0002191543608205393\n",
            "Epoch: 73 Batch: 880 Loss: 0.0020901679527014494\n",
            "Epoch: 73 Batch: 900 Loss: 0.0053765298798680305\n",
            "Epoch: 73 Batch: 920 Loss: 0.11625538021326065\n",
            "Epoch: 73 Batch: 940 Loss: 4.663467552745715e-05\n",
            "Epoch: 73 Batch: 960 Loss: 9.51766996877268e-05\n",
            "Epoch: 73 Batch: 980 Loss: 0.0022440911270678043\n",
            "Epoch: 74 Batch: 0 Loss: 0.0010286330943927169\n",
            "Epoch: 74 Batch: 20 Loss: 0.0001142501860158518\n",
            "Epoch: 74 Batch: 40 Loss: 0.29129037261009216\n",
            "Epoch: 74 Batch: 60 Loss: 0.005888080690056086\n",
            "Epoch: 74 Batch: 80 Loss: 0.0004032134893350303\n",
            "Epoch: 74 Batch: 100 Loss: 0.00013036727614235133\n",
            "Epoch: 74 Batch: 120 Loss: 1.773834264895413e-05\n",
            "Epoch: 74 Batch: 140 Loss: 9.5367431640625e-06\n",
            "Epoch: 74 Batch: 160 Loss: 0.04006228595972061\n",
            "Epoch: 74 Batch: 180 Loss: 0.0007113456958904862\n",
            "Epoch: 74 Batch: 200 Loss: 0.00045137404231354594\n",
            "Epoch: 74 Batch: 220 Loss: 3.871917579090223e-05\n",
            "Epoch: 74 Batch: 240 Loss: 0.0005478858947753906\n",
            "Epoch: 74 Batch: 260 Loss: 0.7581548690795898\n",
            "Epoch: 74 Batch: 280 Loss: 6.11305222264491e-05\n",
            "Epoch: 74 Batch: 300 Loss: 0.0016493797302246094\n",
            "Epoch: 74 Batch: 320 Loss: 0.0011557579273357987\n",
            "Epoch: 74 Batch: 340 Loss: 1.544952465337701e-05\n",
            "Epoch: 74 Batch: 360 Loss: 0.00027151108952239156\n",
            "Epoch: 74 Batch: 380 Loss: 0.00010232925706077367\n",
            "Epoch: 74 Batch: 400 Loss: 0.0010429382091388106\n",
            "Epoch: 74 Batch: 420 Loss: 5.798339770990424e-05\n",
            "Epoch: 74 Batch: 440 Loss: 5.378723290050402e-05\n",
            "Epoch: 74 Batch: 460 Loss: 0.0009654999012127519\n",
            "Epoch: 74 Batch: 480 Loss: 8.869171324477065e-06\n",
            "Epoch: 74 Batch: 500 Loss: 6.341934204101562e-05\n",
            "Epoch: 74 Batch: 520 Loss: 0.00035915375337935984\n",
            "Epoch: 74 Batch: 540 Loss: 0.0038042068481445312\n",
            "Epoch: 74 Batch: 560 Loss: 5.092620995128527e-05\n",
            "Epoch: 74 Batch: 580 Loss: 4.100799742445815e-06\n",
            "Epoch: 74 Batch: 600 Loss: 0.003276824951171875\n",
            "Epoch: 74 Batch: 620 Loss: 0.0005213737604208291\n",
            "Epoch: 74 Batch: 640 Loss: 5.34057608092553e-06\n",
            "Epoch: 74 Batch: 660 Loss: 0.0012498855357989669\n",
            "Epoch: 74 Batch: 680 Loss: 4.96864304295741e-05\n",
            "Epoch: 74 Batch: 700 Loss: 0.0005031585460528731\n",
            "Epoch: 74 Batch: 720 Loss: 0.19336704909801483\n",
            "Epoch: 74 Batch: 740 Loss: 0.00013132095045875758\n",
            "Epoch: 74 Batch: 760 Loss: 0.018084049224853516\n",
            "Epoch: 74 Batch: 780 Loss: 0.0010332107776775956\n",
            "Epoch: 74 Batch: 800 Loss: 0.0001203536958200857\n",
            "Epoch: 74 Batch: 820 Loss: 0.05304746702313423\n",
            "Epoch: 74 Batch: 840 Loss: 0.00022993088350631297\n",
            "Epoch: 74 Batch: 860 Loss: 0.00120630266610533\n",
            "Epoch: 74 Batch: 880 Loss: 0.18616154789924622\n",
            "Epoch: 74 Batch: 900 Loss: 0.00046329497126862407\n",
            "Epoch: 74 Batch: 920 Loss: 2.28881845032447e-06\n",
            "Epoch: 74 Batch: 940 Loss: 0.09947748482227325\n",
            "Epoch: 74 Batch: 960 Loss: 0.08720798790454865\n",
            "Epoch: 74 Batch: 980 Loss: 0.00033311842707917094\n",
            "Epoch: 75 Batch: 0 Loss: 3.43322744811303e-06\n",
            "Epoch: 75 Batch: 20 Loss: 6.752014451194555e-05\n",
            "Epoch: 75 Batch: 40 Loss: 0.0005881309625692666\n",
            "Epoch: 75 Batch: 60 Loss: 0.0028841018211096525\n",
            "Epoch: 75 Batch: 80 Loss: 0.0003762245178222656\n",
            "Epoch: 75 Batch: 100 Loss: 6.84738188283518e-05\n",
            "Epoch: 75 Batch: 120 Loss: 0.028968429192900658\n",
            "Epoch: 75 Batch: 140 Loss: 0.0007431983831338584\n",
            "Epoch: 75 Batch: 160 Loss: 0.002380466554313898\n",
            "Epoch: 75 Batch: 180 Loss: 0.005904006771743298\n",
            "Epoch: 75 Batch: 200 Loss: 0.0003762245178222656\n",
            "Epoch: 75 Batch: 220 Loss: 0.00015840530977584422\n",
            "Epoch: 75 Batch: 240 Loss: 0.0001796722353901714\n",
            "Epoch: 75 Batch: 260 Loss: 0.018178176134824753\n",
            "Epoch: 75 Batch: 280 Loss: 0.02005448378622532\n",
            "Epoch: 75 Batch: 300 Loss: 6.637573096668348e-05\n",
            "Epoch: 75 Batch: 320 Loss: 0.0007871628040447831\n",
            "Epoch: 75 Batch: 340 Loss: 4.95910626341356e-06\n",
            "Epoch: 75 Batch: 360 Loss: 0.002024364424869418\n",
            "Epoch: 75 Batch: 380 Loss: 0.00013418197340797633\n",
            "Epoch: 75 Batch: 400 Loss: 0.0007657051319256425\n",
            "Epoch: 75 Batch: 420 Loss: 4.76837158203125e-07\n",
            "Epoch: 75 Batch: 440 Loss: 1.773834264895413e-05\n",
            "Epoch: 75 Batch: 460 Loss: 0.0014776230091229081\n",
            "Epoch: 75 Batch: 480 Loss: 0.0007406234508380294\n",
            "Epoch: 75 Batch: 500 Loss: 0.018930982798337936\n",
            "Epoch: 75 Batch: 520 Loss: 0.008702278137207031\n",
            "Epoch: 75 Batch: 540 Loss: 0.012698745355010033\n",
            "Epoch: 75 Batch: 560 Loss: 5.7220458984375e-06\n",
            "Epoch: 75 Batch: 580 Loss: 0.17663268744945526\n",
            "Epoch: 75 Batch: 600 Loss: 2.2792815798311494e-05\n",
            "Epoch: 75 Batch: 620 Loss: 7.629394644936838e-07\n",
            "Epoch: 75 Batch: 640 Loss: 2.133371591567993\n",
            "Epoch: 75 Batch: 660 Loss: 0.0002080917329294607\n",
            "Epoch: 75 Batch: 680 Loss: 0.19046470522880554\n",
            "Epoch: 75 Batch: 700 Loss: 0.0009516716236248612\n",
            "Epoch: 75 Batch: 720 Loss: 0.002085399581119418\n",
            "Epoch: 75 Batch: 740 Loss: 0.011640643700957298\n",
            "Epoch: 75 Batch: 760 Loss: 1.373290979245212e-05\n",
            "Epoch: 75 Batch: 780 Loss: 0.00011377334885764867\n",
            "Epoch: 75 Batch: 800 Loss: 0.00013809204392600805\n",
            "Epoch: 75 Batch: 820 Loss: 4.634857032215223e-05\n",
            "Epoch: 75 Batch: 840 Loss: 0.0010406493674963713\n",
            "Epoch: 75 Batch: 860 Loss: 0.0022957802284508944\n",
            "Epoch: 75 Batch: 880 Loss: 0.0003677368222270161\n",
            "Epoch: 75 Batch: 900 Loss: 0.00013294219388626516\n",
            "Epoch: 75 Batch: 920 Loss: 0.2976749539375305\n",
            "Epoch: 75 Batch: 940 Loss: 1.8596649169921875e-05\n",
            "Epoch: 75 Batch: 960 Loss: 0.6626302003860474\n",
            "Epoch: 75 Batch: 980 Loss: 0.032601021230220795\n",
            "Epoch: 76 Batch: 0 Loss: 0.0005523681757040322\n",
            "Epoch: 76 Batch: 20 Loss: 0.00463523855432868\n",
            "Epoch: 76 Batch: 40 Loss: 0.0012900352012366056\n",
            "Epoch: 76 Batch: 60 Loss: 0.002771091414615512\n",
            "Epoch: 76 Batch: 80 Loss: 1.144409225162235e-06\n",
            "Epoch: 76 Batch: 100 Loss: 0.07704553753137589\n",
            "Epoch: 76 Batch: 120 Loss: 1.2353646755218506\n",
            "Epoch: 76 Batch: 140 Loss: 1.583099401614163e-05\n",
            "Epoch: 76 Batch: 160 Loss: 0.00011768341209972277\n",
            "Epoch: 76 Batch: 180 Loss: 0.0003298759402241558\n",
            "Epoch: 76 Batch: 200 Loss: 0.0003654480096884072\n",
            "Epoch: 76 Batch: 220 Loss: 9.17434663278982e-05\n",
            "Epoch: 76 Batch: 240 Loss: 6.103515625e-05\n",
            "Epoch: 76 Batch: 260 Loss: 0.00801010150462389\n",
            "Epoch: 76 Batch: 280 Loss: 0.0005249023670330644\n",
            "Epoch: 76 Batch: 300 Loss: 0.00013952255540061742\n",
            "Epoch: 76 Batch: 320 Loss: 0.0038806437514722347\n",
            "Epoch: 76 Batch: 340 Loss: 0.0012507438659667969\n",
            "Epoch: 76 Batch: 360 Loss: 0.0010433197021484375\n",
            "Epoch: 76 Batch: 380 Loss: 0.0006960869068279862\n",
            "Epoch: 76 Batch: 400 Loss: 0.0014719009632244706\n",
            "Epoch: 76 Batch: 420 Loss: 0.3047889769077301\n",
            "Epoch: 76 Batch: 440 Loss: 0.03519477695226669\n",
            "Epoch: 76 Batch: 460 Loss: 0.42609405517578125\n",
            "Epoch: 76 Batch: 480 Loss: 0.23580750823020935\n",
            "Epoch: 76 Batch: 500 Loss: 0.0006948470836505294\n",
            "Epoch: 76 Batch: 520 Loss: 0.0020351409912109375\n",
            "Epoch: 76 Batch: 540 Loss: 0.023349666967988014\n",
            "Epoch: 76 Batch: 560 Loss: 0.0017858505016192794\n",
            "Epoch: 76 Batch: 580 Loss: 0.0002502441348042339\n",
            "Epoch: 76 Batch: 600 Loss: 5.130767749506049e-05\n",
            "Epoch: 76 Batch: 620 Loss: 0.01256856881082058\n",
            "Epoch: 76 Batch: 640 Loss: 0.0011775970924645662\n",
            "Epoch: 76 Batch: 660 Loss: 9.078979201149195e-05\n",
            "Epoch: 76 Batch: 680 Loss: 2.841949390131049e-05\n",
            "Epoch: 76 Batch: 700 Loss: 0.0004561424138955772\n",
            "Epoch: 76 Batch: 720 Loss: 0.0007568359142169356\n",
            "Epoch: 76 Batch: 740 Loss: 0.00012283325486350805\n",
            "Epoch: 76 Batch: 760 Loss: 0.007258367724716663\n",
            "Epoch: 76 Batch: 780 Loss: 3.452301098150201e-05\n",
            "Epoch: 76 Batch: 800 Loss: 4.997253563487902e-05\n",
            "Epoch: 76 Batch: 820 Loss: 0.0020592689979821444\n",
            "Epoch: 76 Batch: 840 Loss: 0.07110395282506943\n",
            "Epoch: 76 Batch: 860 Loss: 0.0013282776344567537\n",
            "Epoch: 76 Batch: 880 Loss: 0.008097553625702858\n",
            "Epoch: 76 Batch: 900 Loss: 1.144409225162235e-06\n",
            "Epoch: 76 Batch: 920 Loss: 4.348754737293348e-05\n",
            "Epoch: 76 Batch: 940 Loss: 3.128051685052924e-05\n",
            "Epoch: 76 Batch: 960 Loss: 9.5367431640625e-07\n",
            "Epoch: 76 Batch: 980 Loss: 0.01186666451394558\n",
            "Epoch: 77 Batch: 0 Loss: 1.8277629613876343\n",
            "Epoch: 77 Batch: 20 Loss: 0.13676118850708008\n",
            "Epoch: 77 Batch: 40 Loss: 0.005591535475105047\n",
            "Epoch: 77 Batch: 60 Loss: 0.003565883729606867\n",
            "Epoch: 77 Batch: 80 Loss: 2.708435022213962e-05\n",
            "Epoch: 77 Batch: 100 Loss: 0.00013828277587890625\n",
            "Epoch: 77 Batch: 120 Loss: 0.005290889646857977\n",
            "Epoch: 77 Batch: 140 Loss: 0.00040378569974564016\n",
            "Epoch: 77 Batch: 160 Loss: 0.004043865017592907\n",
            "Epoch: 77 Batch: 180 Loss: 0.0009027480846270919\n",
            "Epoch: 77 Batch: 200 Loss: 1.144409225162235e-06\n",
            "Epoch: 77 Batch: 220 Loss: 0.002303695771843195\n",
            "Epoch: 77 Batch: 240 Loss: 0.00010776519775390625\n",
            "Epoch: 77 Batch: 260 Loss: 0.0084228515625\n",
            "Epoch: 77 Batch: 280 Loss: 9.384155418956652e-05\n",
            "Epoch: 77 Batch: 300 Loss: 0.0002830505254678428\n",
            "Epoch: 77 Batch: 320 Loss: 0.0004249572812113911\n",
            "Epoch: 77 Batch: 340 Loss: 2.2983551389188506e-05\n",
            "Epoch: 77 Batch: 360 Loss: 0.4297734200954437\n",
            "Epoch: 77 Batch: 380 Loss: 0.0637543648481369\n",
            "Epoch: 77 Batch: 400 Loss: 5.817413239128655e-06\n",
            "Epoch: 77 Batch: 420 Loss: 0.02172880247235298\n",
            "Epoch: 77 Batch: 440 Loss: 0.006740093231201172\n",
            "Epoch: 77 Batch: 460 Loss: 0.015239810571074486\n",
            "Epoch: 77 Batch: 480 Loss: 0.0029409886337816715\n",
            "Epoch: 77 Batch: 500 Loss: 0.0021098137367516756\n",
            "Epoch: 77 Batch: 520 Loss: 0.0081634521484375\n",
            "Epoch: 77 Batch: 540 Loss: 1.6117095583467744e-05\n",
            "Epoch: 77 Batch: 560 Loss: 0.00020685195340774953\n",
            "Epoch: 77 Batch: 580 Loss: 0.04428849369287491\n",
            "Epoch: 77 Batch: 600 Loss: 1.5258789289873675e-06\n",
            "Epoch: 77 Batch: 620 Loss: 0.00030641554621979594\n",
            "Epoch: 77 Batch: 640 Loss: 0.002574920654296875\n",
            "Epoch: 77 Batch: 660 Loss: 0.15411987900733948\n",
            "Epoch: 77 Batch: 680 Loss: 0.00042743684025481343\n",
            "Epoch: 77 Batch: 700 Loss: 1.201629675051663e-05\n",
            "Epoch: 77 Batch: 720 Loss: 6.437301635742188e-05\n",
            "Epoch: 77 Batch: 740 Loss: 0.706329882144928\n",
            "Epoch: 77 Batch: 760 Loss: 0.000614166259765625\n",
            "Epoch: 77 Batch: 780 Loss: 0.12565211951732635\n",
            "Epoch: 77 Batch: 800 Loss: 5.53131103515625e-05\n",
            "Epoch: 77 Batch: 820 Loss: 1.9073486612342094e-07\n",
            "Epoch: 77 Batch: 840 Loss: 0.26611509919166565\n",
            "Epoch: 77 Batch: 860 Loss: 0.29256659746170044\n",
            "Epoch: 77 Batch: 880 Loss: 0.0029846192337572575\n",
            "Epoch: 77 Batch: 900 Loss: 0.03995790332555771\n",
            "Epoch: 77 Batch: 920 Loss: 0.0014410018920898438\n",
            "Epoch: 77 Batch: 940 Loss: 0.0001203536958200857\n",
            "Epoch: 77 Batch: 960 Loss: 0.0268267635256052\n",
            "Epoch: 77 Batch: 980 Loss: 1.068115216185106e-05\n",
            "Epoch: 78 Batch: 0 Loss: 0.0008870124584063888\n",
            "Epoch: 78 Batch: 20 Loss: 0.00013093948655296117\n",
            "Epoch: 78 Batch: 40 Loss: 9.088516526389867e-05\n",
            "Epoch: 78 Batch: 60 Loss: 4.76837158203125e-06\n",
            "Epoch: 78 Batch: 80 Loss: 2.136230432370212e-05\n",
            "Epoch: 78 Batch: 100 Loss: 1.7642974853515625e-05\n",
            "Epoch: 78 Batch: 120 Loss: 8.106231689453125e-06\n",
            "Epoch: 78 Batch: 140 Loss: 1.3351440202313825e-06\n",
            "Epoch: 78 Batch: 160 Loss: 7.667541649425402e-05\n",
            "Epoch: 78 Batch: 180 Loss: 0.00044307709322310984\n",
            "Epoch: 78 Batch: 200 Loss: 0.08490119129419327\n",
            "Epoch: 78 Batch: 220 Loss: 2.021789623540826e-05\n",
            "Epoch: 78 Batch: 240 Loss: 0.00152759556658566\n",
            "Epoch: 78 Batch: 260 Loss: 0.0014180183643475175\n",
            "Epoch: 78 Batch: 280 Loss: 1.9073486612342094e-07\n",
            "Epoch: 78 Batch: 300 Loss: 0.19328513741493225\n",
            "Epoch: 78 Batch: 320 Loss: 0.016474151983857155\n",
            "Epoch: 78 Batch: 340 Loss: 0.004164600279182196\n",
            "Epoch: 78 Batch: 360 Loss: 0.0006126404041424394\n",
            "Epoch: 78 Batch: 380 Loss: 1.296997106692288e-05\n",
            "Epoch: 78 Batch: 400 Loss: 0.006198406219482422\n",
            "Epoch: 78 Batch: 420 Loss: 0.0006266593700274825\n",
            "Epoch: 78 Batch: 440 Loss: 0.022365570068359375\n",
            "Epoch: 78 Batch: 460 Loss: 0.0010324477916583419\n",
            "Epoch: 78 Batch: 480 Loss: 0.0003440856817178428\n",
            "Epoch: 78 Batch: 500 Loss: 0.001004886580631137\n",
            "Epoch: 78 Batch: 520 Loss: 0.011371994391083717\n",
            "Epoch: 78 Batch: 540 Loss: 0.0020352364517748356\n",
            "Epoch: 78 Batch: 560 Loss: 8.611679368186742e-05\n",
            "Epoch: 78 Batch: 580 Loss: 0.0052620889618992805\n",
            "Epoch: 78 Batch: 600 Loss: 0.00023050307936500758\n",
            "Epoch: 78 Batch: 620 Loss: 0.0005747795221395791\n",
            "Epoch: 78 Batch: 640 Loss: 0.0013438224559649825\n",
            "Epoch: 78 Batch: 660 Loss: 8.239746239269152e-05\n",
            "Epoch: 78 Batch: 680 Loss: 8.37326078908518e-05\n",
            "Epoch: 78 Batch: 700 Loss: 0.00021505355834960938\n",
            "Epoch: 78 Batch: 720 Loss: 0.04616060107946396\n",
            "Epoch: 78 Batch: 740 Loss: 6.456374831032008e-05\n",
            "Epoch: 78 Batch: 760 Loss: 0.03524589538574219\n",
            "Epoch: 78 Batch: 780 Loss: 0.7823833227157593\n",
            "Epoch: 78 Batch: 800 Loss: 0.0017593384254723787\n",
            "Epoch: 78 Batch: 820 Loss: 0.00020275116548873484\n",
            "Epoch: 78 Batch: 840 Loss: 3.06129441014491e-05\n",
            "Epoch: 78 Batch: 860 Loss: 0.5596396327018738\n",
            "Epoch: 78 Batch: 880 Loss: 0.00020799637422896922\n",
            "Epoch: 78 Batch: 900 Loss: 0.1808798760175705\n",
            "Epoch: 78 Batch: 920 Loss: 0.0009809493785724044\n",
            "Epoch: 78 Batch: 940 Loss: 0.12504439055919647\n",
            "Epoch: 78 Batch: 960 Loss: 0.00011978149268543348\n",
            "Epoch: 78 Batch: 980 Loss: 0.013767051510512829\n",
            "Epoch: 79 Batch: 0 Loss: 0.0020529746543616056\n",
            "Epoch: 79 Batch: 20 Loss: 8.659363084007055e-05\n",
            "Epoch: 79 Batch: 40 Loss: 0.00220146169885993\n",
            "Epoch: 79 Batch: 60 Loss: 1.754760705807712e-05\n",
            "Epoch: 79 Batch: 80 Loss: 0.0004268646298442036\n",
            "Epoch: 79 Batch: 100 Loss: 0.004418659023940563\n",
            "Epoch: 79 Batch: 120 Loss: 9.841918654274195e-05\n",
            "Epoch: 79 Batch: 140 Loss: 0.0002349853457417339\n",
            "Epoch: 79 Batch: 160 Loss: 0.0019011497497558594\n",
            "Epoch: 79 Batch: 180 Loss: 0.0008634567493572831\n",
            "Epoch: 79 Batch: 200 Loss: 0.20140095055103302\n",
            "Epoch: 79 Batch: 220 Loss: 0.00012502670870162547\n",
            "Epoch: 79 Batch: 240 Loss: 0.013557052239775658\n",
            "Epoch: 79 Batch: 260 Loss: 0.0001049041748046875\n",
            "Epoch: 79 Batch: 280 Loss: 0.004891395568847656\n",
            "Epoch: 79 Batch: 300 Loss: 5.54084763280116e-05\n",
            "Epoch: 79 Batch: 320 Loss: 0.0003907203790731728\n",
            "Epoch: 79 Batch: 340 Loss: 1.3351440202313825e-06\n",
            "Epoch: 79 Batch: 360 Loss: 0.00013494491577148438\n",
            "Epoch: 79 Batch: 380 Loss: 8.726119995117188e-05\n",
            "Epoch: 79 Batch: 400 Loss: 0.0016122817760333419\n",
            "Epoch: 79 Batch: 420 Loss: 0.04127960279583931\n",
            "Epoch: 79 Batch: 440 Loss: 1.010894811770413e-05\n",
            "Epoch: 79 Batch: 460 Loss: 1.716613724056515e-06\n",
            "Epoch: 79 Batch: 480 Loss: 0.004710674285888672\n",
            "Epoch: 79 Batch: 500 Loss: 0.0022145272232592106\n",
            "Epoch: 79 Batch: 520 Loss: 0.004079055972397327\n",
            "Epoch: 79 Batch: 540 Loss: 6.12258882028982e-05\n",
            "Epoch: 79 Batch: 560 Loss: 7.24792471373803e-06\n",
            "Epoch: 79 Batch: 580 Loss: 0.0010856628650799394\n",
            "Epoch: 79 Batch: 600 Loss: 0.0002037048398051411\n",
            "Epoch: 79 Batch: 620 Loss: 0.00020694732666015625\n",
            "Epoch: 79 Batch: 640 Loss: 1.602172778802924e-05\n",
            "Epoch: 79 Batch: 660 Loss: 0.0005681991460733116\n",
            "Epoch: 79 Batch: 680 Loss: 0.015531348995864391\n",
            "Epoch: 79 Batch: 700 Loss: 0.000579833984375\n",
            "Epoch: 79 Batch: 720 Loss: 0.0003800392150878906\n",
            "Epoch: 79 Batch: 740 Loss: 0.002498722169548273\n",
            "Epoch: 79 Batch: 760 Loss: 0.0003813743533100933\n",
            "Epoch: 79 Batch: 780 Loss: 0.0004138946533203125\n",
            "Epoch: 79 Batch: 800 Loss: 0.0031117438338696957\n",
            "Epoch: 79 Batch: 820 Loss: 0.03637895733118057\n",
            "Epoch: 79 Batch: 840 Loss: 0.5595799684524536\n",
            "Epoch: 79 Batch: 860 Loss: 0.003324270248413086\n",
            "Epoch: 79 Batch: 880 Loss: 0.18078136444091797\n",
            "Epoch: 79 Batch: 900 Loss: 0.0009327888255938888\n",
            "Epoch: 79 Batch: 920 Loss: 0.00024108887009788305\n",
            "Epoch: 79 Batch: 940 Loss: 4.00543212890625e-05\n",
            "Epoch: 79 Batch: 960 Loss: 0.00014638900756835938\n",
            "Epoch: 79 Batch: 980 Loss: 0.453971803188324\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkjGfhMlSPb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA3dxy-_LxKZ",
        "outputId": "2ca635eb-6a55-4781-ef76-5ead94386a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "print(\"Predicting..\")\n",
        "test = [];\n",
        "for i in range(len(X_test)):\n",
        "    test.append(X_test[i].reshape(1,1,161,101));\n",
        "y_pred=clf.predict(test)\n",
        "\n",
        "print(np.unique(y_pred,return_counts=True))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "classes = ('ANG', 'HAP',\n",
        "            'NEU', 'SAD')\n",
        "\n",
        "target_names = classes\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(array([0, 1, 2, 3]), array([607, 709, 611, 537]))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ANG       0.71      0.73      0.72       596\n",
            "         HAP       0.57      0.64      0.60       628\n",
            "         NEU       0.63      0.66      0.64       589\n",
            "         SAD       0.77      0.64      0.70       651\n",
            "\n",
            "    accuracy                           0.66      2464\n",
            "   macro avg       0.67      0.67      0.67      2464\n",
            "weighted avg       0.67      0.66      0.67      2464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFyX1xA0LzOy",
        "outputId": "20266459-1da7-4b79-bd24-86fb6012c580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 66.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWZzv8esT8C1",
        "outputId": "24ceb3c2-4440-4d73-b90b-37a3d7b8b464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "results = confusion_matrix(y_test, torch.FloatTensor(y_pred)) \n",
        "df_cm = pd.DataFrame(results, columns=np.unique(y_test), index = np.unique(y_test))\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.set(font_scale=1.4)#for label size\n",
        "sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print ('Accuracy Score :',accuracy_score(y_test, y_pred)) \n",
        "\n",
        "print ('Report : ')\n",
        "\n",
        "print (classification_report(y_test, y_pred)) \n",
        "\n",
        "print('Confusion Matrix :'  )  \n",
        "\n",
        "print(results) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score : 0.6643668831168831\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.73      0.72       596\n",
            "           1       0.57      0.64      0.60       628\n",
            "           2       0.63      0.66      0.64       589\n",
            "           3       0.77      0.64      0.70       651\n",
            "\n",
            "    accuracy                           0.66      2464\n",
            "   macro avg       0.67      0.67      0.67      2464\n",
            "weighted avg       0.67      0.66      0.67      2464\n",
            "\n",
            "Confusion Matrix :\n",
            "[[433 123  21  19]\n",
            " [118 402  74  34]\n",
            " [ 34 101 386  68]\n",
            " [ 22  83 130 416]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxB1ikV8X1zN",
        "outputId": "5a8cd48b-c61e-4142-f300-47cb2dcf3db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(clf, (1, 161, 101))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 161, 101]             640\n",
            "       BatchNorm2d-2         [-1, 64, 161, 101]             128\n",
            "              ReLU-3         [-1, 64, 161, 101]               0\n",
            "         MaxPool2d-4           [-1, 64, 80, 50]               0\n",
            "            Conv2d-5          [-1, 128, 80, 50]          73,856\n",
            "       BatchNorm2d-6          [-1, 128, 80, 50]             256\n",
            "              ReLU-7          [-1, 128, 80, 50]               0\n",
            "         MaxPool2d-8          [-1, 128, 40, 25]               0\n",
            "            Conv2d-9          [-1, 256, 40, 25]         295,168\n",
            "      BatchNorm2d-10          [-1, 256, 40, 25]             512\n",
            "             ReLU-11          [-1, 256, 40, 25]               0\n",
            "           Conv2d-12          [-1, 256, 40, 25]         590,080\n",
            "      BatchNorm2d-13          [-1, 256, 40, 25]             512\n",
            "             ReLU-14          [-1, 256, 40, 25]               0\n",
            "           Conv2d-15          [-1, 256, 40, 25]         590,080\n",
            "      BatchNorm2d-16          [-1, 256, 40, 25]             512\n",
            "             ReLU-17          [-1, 256, 40, 25]               0\n",
            "        MaxPool2d-18          [-1, 256, 20, 12]               0\n",
            "           Conv2d-19          [-1, 512, 20, 12]       1,180,160\n",
            "      BatchNorm2d-20          [-1, 512, 20, 12]           1,024\n",
            "             ReLU-21          [-1, 512, 20, 12]               0\n",
            "           Conv2d-22          [-1, 512, 20, 12]       2,359,808\n",
            "      BatchNorm2d-23          [-1, 512, 20, 12]           1,024\n",
            "             ReLU-24          [-1, 512, 20, 12]               0\n",
            "        MaxPool2d-25           [-1, 512, 10, 6]               0\n",
            "           Conv2d-26           [-1, 512, 10, 6]       2,359,808\n",
            "      BatchNorm2d-27           [-1, 512, 10, 6]           1,024\n",
            "             ReLU-28           [-1, 512, 10, 6]               0\n",
            "           Conv2d-29           [-1, 512, 10, 6]       2,359,808\n",
            "      BatchNorm2d-30           [-1, 512, 10, 6]           1,024\n",
            "             ReLU-31           [-1, 512, 10, 6]               0\n",
            "        MaxPool2d-32            [-1, 512, 5, 3]               0\n",
            "           Conv2d-33            [-1, 512, 5, 3]       2,359,808\n",
            "      BatchNorm2d-34            [-1, 512, 5, 3]           1,024\n",
            "             ReLU-35            [-1, 512, 5, 3]               0\n",
            "        Dropout2d-36            [-1, 512, 5, 3]               0\n",
            "        AvgPool2d-37            [-1, 512, 5, 3]               0\n",
            "           Linear-38                  [-1, 512]       3,932,672\n",
            "             ReLU-39                  [-1, 512]               0\n",
            "           Linear-40                    [-1, 4]           2,052\n",
            "       LogSoftmax-41                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 16,110,980\n",
            "Trainable params: 16,110,980\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 64.14\n",
            "Params size (MB): 61.46\n",
            "Estimated Total Size (MB): 125.66\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwls2DpaY0Pi"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}